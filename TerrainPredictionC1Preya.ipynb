{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "governing-oxford",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "continent-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_path = 'TrainingData/' # raw training data\n",
    "raw_test_path = 'TestData/' # raw testing data\n",
    "\n",
    "pp_tr_path = 'PreppedTrainingData/' # preprocessed training data folder\n",
    "pp_tr_session = 'tr_session_' # prefix for preprocessed training data for each session\n",
    "pp_tr_final = 'pp_tr.csv' # prefix for final training data\n",
    "\n",
    "pp_te_path = 'PreppedTestData/' # preprocessed test data folder\n",
    "pp_te_session = 'session_' # prefix for preprocessed test data for each session\n",
    "pp_te_final = 'pp_te.csv' # prefix for final test data\n",
    "\n",
    "tr_features_folder = \"TrainingFeatures/\" # Here training data with features calculated are stored\n",
    "\n",
    "acc_gyro_freq = 40\n",
    "label_freq = 10\n",
    "\n",
    "sample_p_label = acc_gyro_freq//label_freq # sample per label\n",
    "\n",
    "time_window = 1 # time frame we will look at - unit (seconds)\n",
    "\n",
    "row_p_window = time_window*acc_gyro_freq # how many samples fit in the time window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "speaking-diagram",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_session_ids(path):\n",
    "    files = os.listdir(path)\n",
    "#     print(\"File Count: \" + str(len(files)))\n",
    "    session_ids = []\n",
    "    for f in files:\n",
    "#         print(f)\n",
    "        temp = f.split('subject_')[1].split('__')[0]\n",
    "        if temp not in session_ids:\n",
    "            session_ids.append(temp)\n",
    "#     print(\"Session Count: \" + str(len(session_ids)))\n",
    "#     print(session_ids)\n",
    "    return session_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "olympic-commission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training or testing - for each session there are 4 required files\n",
    "def prepData(raw_data_folder, prepped_data_folder, session_prefix, final_prefix):\n",
    "    session_ids = extract_session_ids(raw_data_folder)\n",
    "    final_data = pd.DataFrame()\n",
    "    for i in range(0,len(session_ids)):\n",
    "        sid = session_ids[i]\n",
    "        x_time_path = raw_data_folder + \"subject_\" + sid + \"__x_time.csv\"\n",
    "        x_path =  raw_data_folder + \"subject_\" + sid + \"__x.csv\"\n",
    "        y_time_path = raw_data_folder + \"subject_\" + sid + \"__y_time.csv\"\n",
    "        y_path = raw_data_folder + \"subject_\" + sid + \"__y.csv\"\n",
    "\n",
    "        # read the files \n",
    "        x = pd.read_csv(x_path, header = None, names=[\"xa\",\"ya\",\"za\",\"xg\",\"yg\",\"zg\"])\n",
    "        x[\"x_time\"] = pd.read_csv(x_time_path, header = None, names=[\"x_time\"])\n",
    "        y = pd.read_csv(y_time_path, header = None, names = [\"y_time\"])\n",
    "        y[\"label\"] = pd.read_csv(y_path, header = None, names = [\"label\"])\n",
    "        \n",
    "#         print(len(x)/len(y))\n",
    "#         print(x.head())\n",
    "#         print(y.head())\n",
    "        \n",
    "        # merge labels with accelarometer and gyroscope observation - for each sample_p_label rows of features 1 label is used\n",
    "        y_time = []\n",
    "        y_label = []\n",
    "        session = [i]*len(x)           \n",
    "        ind = 0\n",
    "        counter = 1\n",
    "        for row in x.itertuples():\n",
    "            y_row = y.iloc[ind]\n",
    "            y_time.append(y_row.y_time)\n",
    "            y_label.append(y_row.label)\n",
    "            \n",
    "            if counter < sample_p_label:\n",
    "                counter = counter + 1\n",
    "            else:\n",
    "                if ind + 1 < len(y):\n",
    "                    ind = ind + 1\n",
    "                counter = 1\n",
    "        x[\"y_time\"] = y_time\n",
    "        x[\"label\"] = y_label\n",
    "        x[\"label\"] = x[\"label\"].astype('category')\n",
    "        x[\"session\"] = session\n",
    "        final_data = pd.concat([final_data,x])\n",
    "        # create files for preprocessed data for each session\n",
    "        file_path = prepped_data_folder + session_prefix + sid + '.csv'\n",
    "        print(file_path)\n",
    "        x.to_csv(file_path,index=False)\n",
    "    final_data.to_csv(prepped_data_folder + final_prefix, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "opposite-alias",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreppedTrainingData/tr_session_006_01.csv\n",
      "PreppedTrainingData/tr_session_001_08.csv\n",
      "PreppedTrainingData/tr_session_001_07.csv\n",
      "PreppedTrainingData/tr_session_006_03.csv\n",
      "PreppedTrainingData/tr_session_007_04.csv\n",
      "PreppedTrainingData/tr_session_004_02.csv\n",
      "PreppedTrainingData/tr_session_001_01.csv\n",
      "PreppedTrainingData/tr_session_003_02.csv\n",
      "PreppedTrainingData/tr_session_001_04.csv\n",
      "PreppedTrainingData/tr_session_008_01.csv\n",
      "PreppedTrainingData/tr_session_002_02.csv\n",
      "PreppedTrainingData/tr_session_001_05.csv\n",
      "PreppedTrainingData/tr_session_005_02.csv\n",
      "PreppedTrainingData/tr_session_007_01.csv\n",
      "PreppedTrainingData/tr_session_007_02.csv\n",
      "PreppedTrainingData/tr_session_007_03.csv\n",
      "PreppedTrainingData/tr_session_005_01.csv\n",
      "PreppedTrainingData/tr_session_002_01.csv\n",
      "PreppedTrainingData/tr_session_003_01.csv\n",
      "PreppedTrainingData/tr_session_002_04.csv\n",
      "PreppedTrainingData/tr_session_001_03.csv\n",
      "PreppedTrainingData/tr_session_001_02.csv\n",
      "PreppedTrainingData/tr_session_004_01.csv\n",
      "PreppedTrainingData/tr_session_006_02.csv\n",
      "PreppedTrainingData/tr_session_005_03.csv\n",
      "PreppedTrainingData/tr_session_001_06.csv\n",
      "PreppedTrainingData/tr_session_002_03.csv\n",
      "PreppedTrainingData/tr_session_003_03.csv\n",
      "PreppedTrainingData/tr_session_002_05.csv\n"
     ]
    }
   ],
   "source": [
    "# prepare train data + visualize\n",
    "prepData(raw_train_path, pp_tr_path, pp_tr_session,pp_tr_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "mexican-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to create features\n",
    "def dist_vector(x1,y1,z1,x2,y2,z2): # distance between two vectors\n",
    "    return math.sqrt((x1-x2)*(x1-x2)+(y1-y2)*(y1-y2)+(z1-z2)*(z1-z2))\n",
    "def len_vec(x,y,z): # length of vector\n",
    "    return math.sqrt(x*x+y*y+z*z)\n",
    "def angle_vector(x1,y1,z1,x2,y2,z2): # cosine of angle\n",
    "    len1 = len_vec(x1,y1,z1)\n",
    "    len2 = len_vec(x2,y2,z2)\n",
    "    if len1 == 0 or len2 == 0:\n",
    "        return 0 # need to decide on this later\n",
    "    return (x1*x2+y1*y2+z1*z2)/(len1*len2) # what will happen if the denominator is zero\n",
    "def create_features(data_folder, dest_folder, all_session_file, use_prev_label=False, file_type=\"Train\"):\n",
    "    files = os.listdir(data_folder)\n",
    "    all_sessions_feat = pd.DataFrame() # features for all sessions\n",
    "    ts_feat_count = (row_p_window-1)*4 # time sensitive feature count - 4 features for each consecutive row pairs\n",
    "\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        df = pd.read_csv(data_folder + f)\n",
    "        if file_type == \"Test\":\n",
    "            df = pd.read_csv(data_folder + f, names=[\"xa\",\"ya\",\"za\",\"xg\",\"yg\",\"zg\"]) \n",
    "        \n",
    "        # each row contains session number + features + label for previous row + target label\n",
    "        prev_row = None\n",
    "        prev_features = None\n",
    "        for row in df.itertuples():\n",
    "            features = []\n",
    "            if file_type == \"Train\":\n",
    "                features.append(row.session)\n",
    "            # time sensitive features---------------------------------\n",
    "            # step 1: initialize the time sensitive feature list\n",
    "            ts_features = [0]*ts_feat_count\n",
    "            if prev_features is not None:\n",
    "                ts_features = prev_features\n",
    "            # step 2: remove the first four features\n",
    "            ts_features = ts_features[4:]\n",
    "            # step 3: add the new four features at the end\n",
    "            if prev_row is None:\n",
    "                # 0,0,0 is the reference point - don't know if it's the right thing to do\n",
    "                dacc = dist_vector(0,0,0,row.xa,row.ya,row.za) # distance between accelarator vectors\n",
    "                aacc = angle_vector(0,0,0,row.xa,row.ya,row.za) # angle between accelerator vectors\n",
    "                dgyro = dist_vector(0,0,0,row.xg,row.yg,row.zg) # distance between gyroscope vectors\n",
    "                agyro = angle_vector(0,0,0,row.xg,row.yg,row.zg) # angle between gyroscope vectors\n",
    "                ts_features.extend([dacc, aacc, dgyro, agyro])\n",
    "            else:\n",
    "                dacc = dist_vector(prev_row.xa,prev_row.ya,prev_row.za,row.xa,row.ya,row.za) # distance between accelarator vectors\n",
    "                aacc = angle_vector(prev_row.xa,prev_row.ya,prev_row.za,row.xa,row.ya,row.za) # angle between accelerator vectors\n",
    "                dgyro = dist_vector(prev_row.xg,prev_row.yg,prev_row.zg,row.xg,row.yg,row.zg) # distance between gyroscope vectors\n",
    "                agyro = angle_vector(prev_row.xg,prev_row.yg,prev_row.zg,row.xg,row.yg,row.zg) # angle between gyroscope vectors\n",
    "                ts_features.extend([dacc, aacc, dgyro, agyro])\n",
    "            # step 4: add all the time sensitive features\n",
    "            features.extend(ts_features)\n",
    "            #---------------------------------------------------------\n",
    "            \n",
    "            if use_prev_label: # using previous row label as a feature could be problematic for test data\n",
    "                if prev_row is None:\n",
    "                    features.append(row.label) # if it is the first row use the label of this row\n",
    "                else:\n",
    "                    features.append(prev_row.label)\n",
    "                    \n",
    "            # Finally add the label\n",
    "            if file_type == \"Train\":\n",
    "                features.append(row.label)\n",
    "            # add row of features to the features file for a session\n",
    "            features_df = pd.DataFrame([features])\n",
    "            features_df.to_csv(dest_folder + f,mode='a',header=None,index=False) # features for a session\n",
    "            \n",
    "            # add row of features to the features file for all sessions\n",
    "            if file_type == \"Train\":\n",
    "                features_df.to_csv(dest_folder + all_session_file,mode='a',header=None,index=False)\n",
    "            \n",
    "            prev_row = row\n",
    "            prev_features = ts_features        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "removed-hometown",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_session_002_01.csv\n",
      "tr_session_002_03.csv\n",
      "tr_session_002_02.csv\n",
      ".DS_Store\n",
      "tr_session_002_05.csv\n",
      "tr_session_002_04.csv\n",
      "tr_session_004_02.csv\n",
      "tr_session_004_01.csv\n",
      "tr_session_008_01.csv\n",
      "tr_session_006_01.csv\n",
      "tr_session_001_08.csv\n",
      "tr_session_006_02.csv\n",
      "tr_session_006_03.csv\n",
      "tr_session_003_02.csv\n",
      "tr_session_001_07.csv\n",
      "tr_session_001_06.csv\n",
      "tr_session_003_03.csv\n",
      "tr_session_003_01.csv\n",
      "tr_session_001_04.csv\n",
      "tr_session_001_05.csv\n",
      "tr_session_001_01.csv\n",
      "tr_session_001_02.csv\n",
      "tr_session_001_03.csv\n",
      "tr_session_005_01.csv\n",
      "tr_session_007_04.csv\n",
      "tr_session_005_03.csv\n",
      "tr_session_005_02.csv\n",
      "tr_session_007_03.csv\n",
      "tr_session_007_02.csv\n",
      "tr_session_007_01.csv\n"
     ]
    }
   ],
   "source": [
    "# create features\n",
    "create_features(pp_tr_path, tr_features_folder, pp_tr_final, False, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afraid-syntax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling\n",
      "Counter({0.0: 1006926, 3.0: 206436, 2.0: 73068, 1.0: 55216})\n",
      "1341646\n",
      "1341646\n",
      "After oversampling\n",
      "4027704\n",
      "4027704\n",
      "Counter({0.0: 1006926, 1.0: 1006926, 2.0: 1006926, 3.0: 1006926})\n"
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "train_data_w_feature = pd.read_csv(tr_features_folder + pp_tr_final, header = None)\n",
    "columns = len(train_data_w_feature.columns)\n",
    "X_train = train_data_w_feature.iloc[:,1:columns-1]\n",
    "y_train = train_data_w_feature.iloc[:,columns-1]\n",
    "counter = Counter(y_train)\n",
    "print(\"Before oversampling\")\n",
    "print(counter)\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "oversample = SMOTE()\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "counter = Counter(y_train)\n",
    "print(\"After oversampling\")\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "broke-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = columns-2\n",
    "output_dim = 4\n",
    "lr=0.001\n",
    "def simpleNN(num_hidden_layers, num_neurons):\n",
    "    # input layer -> hidden layer -> hidden layer -> output layer\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, input_dim=input_dim, activation='relu')) # input layer\n",
    "    for i in range(0,num_hidden_layers):\n",
    "        model.add(Dense(num_neurons, activation='relu')) # hidden layers\n",
    "        \n",
    "    model.add(Dense(output_dim, activation='softmax')) # output layer\n",
    "    # compile the keras model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "understanding-mechanics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for plotting training and validation learning curves\n",
    "def plot_history(history):\n",
    "    # plot loss\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='red', label='test')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "\n",
    "    # plot accuracy\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_accuracy'], color='red', label='test')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "radical-charter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 84s 31us/step - loss: 0.1294 - accuracy: 0.9546\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0763 - accuracy: 0.9736\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.0643 - accuracy: 0.9780\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0582 - accuracy: 0.9801\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0538 - accuracy: 0.9818\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0511 - accuracy: 0.9828\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.0487 - accuracy: 0.9836\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0474 - accuracy: 0.9842\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.0461 - accuracy: 0.98471s - loss:\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.0450 - accuracy: 0.9851\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.0441 - accuracy: 0.9854\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0435 - accuracy: 0.9857\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.0430 - accuracy: 0.9860\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0423 - accuracy: 0.9863\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0421 - accuracy: 0.9864\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 78s 29us/step - loss: 0.0416 - accuracy: 0.9867\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0414 - accuracy: 0.9867\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0413 - accuracy: 0.9868\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 77s 29us/step - loss: 0.0415 - accuracy: 0.9869\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 77s 29us/step - loss: 0.0411 - accuracy: 0.9870\n",
      "1342568/1342568 [==============================] - 14s 11us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.4811 - accuracy: 0.7966\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.4053 - accuracy: 0.8357\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3839 - accuracy: 0.8463\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 77s 29us/step - loss: 0.3710 - accuracy: 0.8526\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3623 - accuracy: 0.8570\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.3555 - accuracy: 0.8601\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.3506 - accuracy: 0.8626\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.3468 - accuracy: 0.8646\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 74s 27us/step - loss: 0.3432 - accuracy: 0.8661\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 73s 27us/step - loss: 0.3403 - accuracy: 0.8677\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3374 - accuracy: 0.8691\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3352 - accuracy: 0.8703\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3336 - accuracy: 0.8710\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3318 - accuracy: 0.8720\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.3301 - accuracy: 0.8727\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.3285 - accuracy: 0.8735\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.3271 - accuracy: 0.8740\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3262 - accuracy: 0.8744\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3252 - accuracy: 0.8750\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.3244 - accuracy: 0.8754\n",
      "1342568/1342568 [==============================] - 12s 9us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3544 - accuracy: 0.8699\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.2884 - accuracy: 0.8942\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2723 - accuracy: 0.9000\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 72s 27us/step - loss: 0.2632 - accuracy: 0.9036\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.2572 - accuracy: 0.9060\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 77s 29us/step - loss: 0.2525 - accuracy: 0.9077\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2488 - accuracy: 0.9091\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2458 - accuracy: 0.9102\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.2436 - accuracy: 0.9111\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2417 - accuracy: 0.9120\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 73s 27us/step - loss: 0.2398 - accuracy: 0.9125\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 73s 27us/step - loss: 0.2384 - accuracy: 0.9132\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 72s 27us/step - loss: 0.2374 - accuracy: 0.9138\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2365 - accuracy: 0.9141\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.2351 - accuracy: 0.9146\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.2343 - accuracy: 0.9149\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2340 - accuracy: 0.9150\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 71s 27us/step - loss: 0.2333 - accuracy: 0.9152\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 72s 27us/step - loss: 0.2324 - accuracy: 0.9156\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 73s 27us/step - loss: 0.2320 - accuracy: 0.9157\n",
      "1342568/1342568 [==============================] - 20s 15us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 93s 35us/step - loss: 0.1206 - accuracy: 0.9579\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 95s 35us/step - loss: 0.0707 - accuracy: 0.9760\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.0601 - accuracy: 0.9800\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 90s 33us/step - loss: 0.0557 - accuracy: 0.9817\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.0534 - accuracy: 0.9829\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 90s 33us/step - loss: 0.0524 - accuracy: 0.9834\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.0514 - accuracy: 0.9840\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.0512 - accuracy: 0.9841\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 85s 32us/step - loss: 0.0510 - accuracy: 0.9844\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 85s 32us/step - loss: 0.0508 - accuracy: 0.9846\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 86s 32us/step - loss: 0.0505 - accuracy: 0.9847\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 86s 32us/step - loss: 0.0511 - accuracy: 0.9847\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 86s 32us/step - loss: 0.0515 - accuracy: 0.9846\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 85s 32us/step - loss: 0.0513 - accuracy: 0.9845\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 84s 31us/step - loss: 0.0524 - accuracy: 0.9845\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 84s 31us/step - loss: 0.0522 - accuracy: 0.9846\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.0532 - accuracy: 0.9845\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.0532 - accuracy: 0.9846\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 86s 32us/step - loss: 0.0535 - accuracy: 0.9844\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.0543 - accuracy: 0.9842\n",
      "1342568/1342568 [==============================] - 14s 11us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.4631 - accuracy: 0.8063\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.3852 - accuracy: 0.8463\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.3628 - accuracy: 0.8576\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.3515 - accuracy: 0.8633\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.3445 - accuracy: 0.8667\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.3414 - accuracy: 0.8683\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 87s 33us/step - loss: 0.3379 - accuracy: 0.8706\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.3360 - accuracy: 0.8717\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 93s 35us/step - loss: 0.3339 - accuracy: 0.8726\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 93s 35us/step - loss: 0.3326 - accuracy: 0.8736\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.3316 - accuracy: 0.8744\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.3306 - accuracy: 0.8748\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.3298 - accuracy: 0.8752\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.3289 - accuracy: 0.8758\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 92s 34us/step - loss: 0.3299 - accuracy: 0.8755\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.3306 - accuracy: 0.8756\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.3297 - accuracy: 0.8758\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.3301 - accuracy: 0.8760\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.3322 - accuracy: 0.8753\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.3314 - accuracy: 0.8755\n",
      "1342568/1342568 [==============================] - 21s 15us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.3389 - accuracy: 0.8762\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2748 - accuracy: 0.8994\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2585 - accuracy: 0.9058\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2497 - accuracy: 0.9094\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2444 - accuracy: 0.9113\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 90s 33us/step - loss: 0.2412 - accuracy: 0.9127\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2387 - accuracy: 0.9138\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 90s 34us/step - loss: 0.2371 - accuracy: 0.9146\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2359 - accuracy: 0.9153\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2348 - accuracy: 0.9157\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2344 - accuracy: 0.9160\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2344 - accuracy: 0.9162\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 90s 34us/step - loss: 0.2356 - accuracy: 0.9159\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 90s 34us/step - loss: 0.2361 - accuracy: 0.9158\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 90s 34us/step - loss: 0.2358 - accuracy: 0.9162\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2357 - accuracy: 0.9162\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2361 - accuracy: 0.9163\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2369 - accuracy: 0.9162\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2367 - accuracy: 0.9162\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2377 - accuracy: 0.9160\n",
      "1342568/1342568 [==============================] - 15s 11us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 223s 83us/step - loss: 0.1187 - accuracy: 0.9588\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 222s 82us/step - loss: 0.0716 - accuracy: 0.9761\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 220s 82us/step - loss: 0.0621 - accuracy: 0.9797\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 218s 81us/step - loss: 0.0580 - accuracy: 0.9813\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 219s 82us/step - loss: 0.0561 - accuracy: 0.9821\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 226s 84us/step - loss: 0.0547 - accuracy: 0.9829\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 220s 82us/step - loss: 0.0533 - accuracy: 0.9835\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 222s 83us/step - loss: 0.0534 - accuracy: 0.9836\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 228s 85us/step - loss: 0.0528 - accuracy: 0.9838\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 222s 83us/step - loss: 0.0531 - accuracy: 0.9839\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 221s 82us/step - loss: 0.0533 - accuracy: 0.9839\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 220s 82us/step - loss: 0.0538 - accuracy: 0.9841\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 224s 83us/step - loss: 0.0537 - accuracy: 0.9841\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 224s 83us/step - loss: 0.0540 - accuracy: 0.9841\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 223s 83us/step - loss: 0.0546 - accuracy: 0.9840\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 223s 83us/step - loss: 0.0551 - accuracy: 0.9842\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 222s 83us/step - loss: 0.0548 - accuracy: 0.9841\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 222s 83us/step - loss: 0.0565 - accuracy: 0.9837\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 226s 84us/step - loss: 0.0562 - accuracy: 0.9837\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 223s 83us/step - loss: 0.0562 - accuracy: 0.9837\n",
      "1342568/1342568 [==============================] - 20s 15us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 222s 83us/step - loss: 0.4591 - accuracy: 0.8085\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 227s 84us/step - loss: 0.3840 - accuracy: 0.8479\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 230s 86us/step - loss: 0.3643 - accuracy: 0.8578\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 212s 79us/step - loss: 0.3540 - accuracy: 0.8630\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 187s 70us/step - loss: 0.3479 - accuracy: 0.8661\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 183s 68us/step - loss: 0.3424 - accuracy: 0.8688\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.3390 - accuracy: 0.8706\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.3365 - accuracy: 0.8720\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.3338 - accuracy: 0.8733\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.3323 - accuracy: 0.8744\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.3313 - accuracy: 0.8749\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 182s 68us/step - loss: 0.3299 - accuracy: 0.8755\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 181s 68us/step - loss: 0.3294 - accuracy: 0.8759\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.3277 - accuracy: 0.8765\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.3280 - accuracy: 0.8768\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 182s 68us/step - loss: 0.3275 - accuracy: 0.8769\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.3274 - accuracy: 0.8771\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 181s 68us/step - loss: 0.3269 - accuracy: 0.8774\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 181s 68us/step - loss: 0.3267 - accuracy: 0.8774\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.3255 - accuracy: 0.8781\n",
      "1342568/1342568 [==============================] - 19s 14us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.3357 - accuracy: 0.8772\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 178s 66us/step - loss: 0.2742 - accuracy: 0.8999\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2596 - accuracy: 0.9057\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2527 - accuracy: 0.9085\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2481 - accuracy: 0.9101\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2439 - accuracy: 0.9120\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2420 - accuracy: 0.9130\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2404 - accuracy: 0.9137s - loss: 0.2404 - accuracy: 0.91\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2392 - accuracy: 0.9143\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2383 - accuracy: 0.9149\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2380 - accuracy: 0.9148\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2379 - accuracy: 0.9152\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.2378 - accuracy: 0.9152\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2371 - accuracy: 0.9155\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2361 - accuracy: 0.9160\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2361 - accuracy: 0.9160\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2355 - accuracy: 0.9161\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2357 - accuracy: 0.9164\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2361 - accuracy: 0.9162\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.2361 - accuracy: 0.9160\n",
      "1342568/1342568 [==============================] - 19s 14us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.1267 - accuracy: 0.9554: 1:24 - loss: - ETA: 1:22 - loss: 0.3331 - accuracy: 0.87 - ETA: 1:22 - loss: 0.3319 - accura - E - ETA: 1:18 - los - ETA: 1:15 - loss: 0.2645 - ETA - ETA - ETA: 55s - loss: 0.18 - ETA: 54s - loss: 0.1802 - ETA: 46s - loss: 0.1664 - accuracy: 0.9 - ETA: 46s - loss - ET - ETA: 31s - loss: 0.1486 -   - ETA: 24s - lo - ETA: 22s - loss: 0.1407 - accu - ETA: 22s - loss: 0.1403 - accu - ETA: 2 - ETA: 19s - loss: 0.1385 - accuracy:  - ETA: 19s - loss: 0.1383 - accur - ETA: 18s - loss: 0.1379 - accurac - ETA: 18s - loss: 0.1376 - accuracy: 0.951 - ETA: 18s - loss: 0.137 - ETA: 17s - loss: 0.1368 - a - ETA: 16s - loss: 0.1363  - ETA: 13s - loss: 0.1342 - a - ETA: 12s - loss: 0.1338 - accuracy: 0.9 - ETA: 12s - ETA: 4s - l - ETA: 3s - los - ETA: 3s - ETA: 2s - loss: - ETA: 1s - ETA: 0s - loss: 0.1272 - accuracy - ETA: 0s - loss: 0.1271 - accuracy - ETA: 0s - loss: 0.1269 \n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.0760 - accuracy: 0.9739 - ETA: 1:00 - loss: 0.081 - ETA: 1:00 - loss - ETA: 38s - loss: 0.0793 - accu - ETA: 37s - loss: 0.0792 - accuracy: 0. - ETA: 32s - loss: 0.0787 - ac - ETA: 32s - loss: 0.0787 - accurac - ETA: 3 - ETA: 27s -\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 84s 31us/step - loss: 0.0663 - accuracy: 0.9774: 58s - loss: 0 - ETA: 57s - loss: 0.0674 - accuracy: 0 - ETA: 57s - loss: 0.0674 - accuracy: 0.97 - ETA: 57s - loss: 0.0674 - accuracy:  - ETA: 57s - loss: 0.0674 - accuracy: 0.97 - ETA: 56s - loss: 0.0674 - accu - ETA - ETA: 3s - loss: 0.0663 - accura\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 85s 32us/step - loss: 0.0621 - accuracy: 0.9791: 58s - loss: 0.0627 - a - ETA:  - ETA: 56s - loss: 0.0625 - accuracy: 0.97 - ETA: 55s - loss - ETA: 54s - loss: 0.0623 - ETA: 53s - loss: 0.0623 - accuracy: 0. - ETA: 53s - loss: 0.0623 - accuracy: 0.979 - ETA: 53s - loss: 0.0623 -  - ETA: 52s - loss: 0 - ETA: 51s - loss: 0.0624 - accuracy: 0.978 - ETA: 51s -  - ETA: 49s - loss: 0.0625 - accuracy: 0. - ETA: 49s - lo  - ETA: 43s - loss: 0.0 - ETA: 42s - loss: 0.0624 - accur - ETA: 41s - loss - ETA: 40s - loss: 0.0624 - accu - ETA: 39s - loss: 0.0624 - accur - ETA: 13s - loss: 0.0621 - accuracy: 0.97 - ETA: 13s - loss: 0.0621 - accuracy:  - ETA: 13s - los - ETA: 11s - - E\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.0591 - accuracy: 0.9802: 59s - loss: 0 - ETA: 58s - loss: 0.059 - ETA: 55s - loss: 0.0593 - accuracy: 0.9 - ETA: 55s - loss: - ETA: 53s - loss: - ETA: - ETA: 50s - loss: 0.0592 - acc - ETA: 49s - loss: 0.0592  - ETA: 48s - loss: 0.0591 - accuracy:  - ETA: 48s - loss: 0.0592 - accuracy:  - ETA: 48s - loss: 0.0591 - accur - ETA: 47s - loss: 0.0592 - ac - ETA: 46s - loss: 0.0592 - accuracy: 0.9 - ETA: 46s - loss: 0.0592 - accuracy: 0. - ETA: 46s - loss: 0.0592 - accuracy: 0. - ETA: 46s - loss: 0.05 - ETA: 45s - loss: 0.0593 - accurac - ETA: 42s - loss: 0.0594 - accura - ETA: 39s - loss: 0.0593 - accuracy - ETA: 39s - loss: 0.0593  -  - ETA - ETA: 29s - loss: 0.0593 - accuracy: 0.98 - ETA: 29s - ETA: 27s - loss: 0.0592 - accuracy: 0 - ETA: 27s - loss: 0.0 - ETA: 25s - loss: 0.0592 - - ETA: 25s - loss: 0.0591 - accur - ETA: 22s - loss: 0.05 - ETA: 21s - loss: 0.0593 - - ETA: 20s - loss: 0.0592 - accuracy:  - ETA: 19s - loss: 0.0592 - accura - ETA: 19s - loss: 0.0593 - accura - ETA: 18s - loss: 0.0593 - ac - ETA: 18s - loss: 0.0593 - accurac - ETA: 17s -  - ETA: 16s - loss: 0. - ETA: 1 - ETA: 2s - l - ETA: 0s - loss: 0.0\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0573 - accuracy: 0.9809: 1:31 - loss: 0.056 - ETA: 1:23 - loss: 0.0 - ETA: 1:22 - loss: 0.0576 - accuracy: 0. - ETA: 1:22 - loss: - ETA: 1:17 - loss: 0.0575 - accuracy: 0.98 - ETA: 1:17 - l - ETA - ETA: 1:15 - loss: 0.057 - ETA: 1:14 - loss: 0 - ETA: 1:12 - ETA: 1:11 - loss: 0.0583  - ETA: 1:11 - loss: 0.0581 - accuracy:  - ETA: 1:11 - loss: 0.0579 - accuracy: 0. - ETA: 1:11 - loss: 0.0580 - accuracy: 0.98 - ETA: 1:11 - loss: 0.0580 - accuracy:  - ETA: - ETA: 58s - loss: 0.0581 - accura - ETA: 57s - loss: 0.0581 - accuracy: 0.980 - ETA: 57s - loss: 0.0582 - accurac - ETA: 57s - loss: 0.0581 - accur - ETA: 56s - loss: 0.0582 - accuracy: - ETA: 56s - loss: 0.0581 - accur - ETA: 55s - loss: 0.0583  - ETA: 54s - loss: 0.0582 - a - ETA: 54s - loss: 0.0582 - accurac - ET - ETA: 51s - loss: 0.0581 - ac - ETA: 50s - loss: 0.0580 - accuracy: 0.98 - ETA: 50s - loss: 0.0580 - a - ETA: 50s - loss: 0.0579 - accuracy - ETA: 49s - loss:  - ETA: 48s - loss: 0.0579 - accuracy - ETA: 47s - loss: 0.0578 - ac - ETA: 47s - los - ETA: 45s - loss: 0.0579 - ETA: 42s - loss: 0.0577 - accuracy:  - ETA: 39s - loss: 0.0578 - ETA: 39s - loss: 0.0578 - accuracy - ETA: 38s - loss: 0.0578 - accu - ETA: 37s - lo - ET - ETA: 34s - loss: 0.0578 - - ETA: 33s - loss: 0.05 - ETA: 32s - loss: 0.0578 - ac - ETA: 31s - los - ETA: 30s - loss: 0.0576 - accuracy: - ETA: 30s - loss: 0.0 - ETA: 28s - loss: 0.05 - ETA: 27s - loss: 0.0575  - ETA: 2s - los - ETA: 1s - -\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0559 - accuracy: 0.9815: 1:12 - loss: 0.0561  - ETA: 1:11 - loss: 0.0560 - accuracy - ETA: 1:11 - loss: 0.0559 - - ETA: 59s - loss: 0.0551 - accuracy: 0 - ETA: 59s - loss: 0.0550 - accuracy:  -  - ETA: 54s - loss - ETA: 53s - loss: 0.0551 - accur - - ETA: 50s - l - ETA: 49s - loss: 0.0551 - accuracy: 0 - ETA: 49s - loss: 0.0552 - ETA: 48s - loss - ETA: 46s - loss: 0.0551 - accuracy: 0.9 - ETA: 46s - loss: 0.0552 - accuracy: 0.9 - ETA: 46s - loss: 0.0551 -  - ETA: 43s - loss: 0.0553 - accuracy: 0.98 - ETA: 43s - los - ETA: 41s - loss: 0.0552 - accuracy: 0. - ETA: 41s - loss: 0.0553 - acc  - ETA: 38s - loss: 0.0552 - accuracy: 0.98 - ETA: 38 - ETA: 36s - loss: 0.0553 - accuracy - ETA: 36s - loss: 0.0555 - accuracy: 0.98 - ETA: 36s - l - ETA: 30s - loss: 0.0556 - accuracy: 0.98 - ETA: 30s  - ETA: 26s - loss: 0.0559 -  - ETA: 25s - loss: 0.0559 - accuracy: 0 - ETA: 25s - loss: 0.055 - ETA: 24s - loss: 0.0560 -  - ETA: 23s - loss: 0.0558 - accurac - ETA: 22s - loss: 0.0558 - accu - ETA: 22s - loss: 0.0558 - ETA: 21s - loss:  - ETA: 20s - loss: 0.0558 - accuracy: - ETA: 19s - lo - ETA: 18s - loss: 0.0559 - ac - ETA: 17s - loss: 0.0560  - E - ETA: 1 - ETA: 4s - loss: 0.0560  - ETA - ETA: \n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.0540 - accuracy: 0.9822TA: 59s - loss: 0.0542 - a - ETA: 58s - loss: 0.0543 - accuracy:  - ETA: 57s - ETA: 56s  - ETA: 54s - loss: 0 - ETA: 53s - loss: 0.0540 - accuracy: - ETA: 53s - loss: 0.0542 - accuracy - ETA: 52s - ETA: 50s - loss: 0.0540 - accuracy: 0 - ETA: 50s - loss: 0.0540 - accuracy - ETA: 50s - loss: 0.05 - ETA: 44s - loss: 0.053 - ETA: 43s - los - ETA: 42s - loss: 0.0538 - accura - ETA: 39s - loss: 0.0539 - - - ETA: 36s - lo - ETA: 34s - loss: 0.0 - ETA: 33s - loss: 0.05 - ETA: 32s - loss: 0.0539 - accuracy: 0.9 - ETA: 32s - loss: 0.0539 - accur - ETA: 31s - loss: 0 - ETA: 30s - loss: 0. - ETA: 27s - loss: 0.0541 - accuracy: 0 - ETA: 26s - loss: 0.0541 - accuracy: 0.982 - ETA: 26s - loss: 0.0541 - accurac - ETA: 26s - loss: 0.0541 - accuracy:  - ETA: 26s - loss: 0.0541 - accuracy:  - ETA: 23s - loss: 0.0541 - accuracy: - ETA: 23s - loss:  - ETA: 21s - loss: 0.0541 - accuracy: 0 - ETA: 21s - loss: 0.0540 - accuracy:  - ETA: 21s - loss: 0.0540 - a - ETA: 16s - loss: 0.0541  - ETA: 15s - loss: 0.0542 - accurac - ETA: 12s -  - ETA: 0s - loss: 0.0540 - ac\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0535 - accuracy: 0.9824: 59s - loss: 0.0534 - accuracy: 0.98 - ETA: 59s - loss: 0.0534 - ETA: 58s - loss: - ETA: 57s - loss: 0.0536 - accuracy: 0.982 - ETA: 57s - loss: 0.0535 - accuracy: 0.9 - ETA: 56s - loss: 0.0535 - accura - ETA: - ETA: 54s  - ETA: 52s - loss: 0.0533 - - ETA: 52s - loss: 0.0 - ETA: 50s - loss: 0.0535 - accuracy: 0.98 - ETA: 50s - los - ETA: 42s - loss: 0.0538 - accura - ETA: 42s - loss: 0.0537 - accuracy: 0.98 - ETA - ETA: 39s  - ETA - ETA: 36s - loss: 0.0538 -  - ET - ETA: 33s - loss: 0.0537 - accuracy: 0 - ETA: 33 - ETA: 31s - loss: 0.0538 - accuracy: 0.982 - ETA: 31s - loss: 0.0538 - acc - ETA: 30s - loss: 0.0537 - ETA: 29s - loss: 0.0537 - accuracy: 0. - ETA: 29s - loss: 0.0536 - acc - ETA: 28s - loss: 0.0536 - accu - ETA: 28s - loss: 0.0536 - accur - ETA: 27s - loss: 0.0536 - accuracy: 0. - ETA: 27s - loss: 0.0537 - accur - ETA: 27s - loss: 0.0536  - ETA: 26  - ETA: 5s - loss: 0.0534  - ETA\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0522 - accuracy: 0.9829: 1:21 - loss: 0.0536 - accuracy - ETA: 1:20 - loss: 0.0548 - accuracy: 0. - ETA - ETA: 1:10 - loss: 0.0529 - accuracy:  - ETA:  - ETA: 1:08 - loss: 0.0530 - accu - ETA: 1:08 - l - ETA: 1:03 - loss: 0.0527 - ac - ETA: 1:02 - loss: 0.0526 - accura - ETA: 1:02 - loss: 0.0527 - accu - ETA: 1:02 - loss: 0.0526 - accuracy: 0. - ETA: 1:01 - loss: 0.0527  - E - ETA: 1:0 - ETA: 59s - loss: 0.0526 - accuracy: 0.98 - ETA: 58s  - ETA: 57s - loss: 0.0526 - acc - ETA: 56s - loss: 0.05 - ETA: 55s - loss: 0.0525 - - ETA: 54s - l - ETA: 53s - loss: 0.0525 - accuracy: 0. - ETA: 52s - loss: 0.0525 - accuracy: - ETA: 52s - loss: 0.0525 - accuracy: 0.982 - ETA: 52s - loss: 0.0525 - accur - ETA: 51s - loss: 0.0524 - accuracy: 0.982 - ETA: 51s - loss: 0.052 - ETA: 50s - los - ETA: 49s - - ETA: 47s - loss: 0.0523 - accuracy: - ETA: 47s - loss: 0.0524 - accur - ETA: 46s - loss: 0.0524 - accuracy: - ETA: 46s - loss: 0.0524 - accuracy: 0.982 - ETA: 46s - loss: 0.0523 - accuracy: 0.982 - ETA: 46s - loss: 0.052 - ETA: 45s - loss: 0.0524 - a - ETA: 42s - loss: 0.0526 - accuracy:  - ETA: 42s - loss: 0.0526 - accuracy: 0 - ETA: 41s - loss: 0.0526 - acc - ETA: 41s - loss: 0.0526 - accuracy: 0 - ETA: 40s - ETA: 39s - loss: 0.0525  - ETA: 38s - loss: 0.0526 - accur - ETA: 3 - ETA - ETA: 33s - loss: 0.0523 - ETA: 33 - ETA: 31s - loss: 0.0524 - ETA: 27s - loss: 0.0524 - accuracy: 0 - ETA: 27s - loss: 0.0524 - ac - ETA: - - ETA: 13s - l -\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0515 - accuracy: 0.9832ETA: 27s - loss: 0.0519 - accura - ETA: 27s - loss: - ETA: 25s - loss: 0 - ETA: 24s - loss - ETA: 23s - loss:  - ETA: 22s - loss: 0.0516 - accuracy: 0.9 - ETA: 21s - loss: 0.051 - ETA: 20s - los - ETA: 19s - loss: 0.0515 - - ETA: 16s - loss: 0.0514 - accuracy:  - ETA: 16s - loss: 0.0514 - a - ETA: 15s - loss: 0.0515  - ETA: 12s - loss: 0.0516 - accuracy: 0 - ETA: 11s - loss: 0.0518 - accuracy: - ETA: 7s - loss: - ETA: 3s - loss: 0.0515 - accu - ETA:  - ETA: 2s - loss: 0.0515 - ac - ETA: 1s - ETA\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0503 - accuracy: 0.9838: 1:02 - - ETA: 54s - loss: 0.0503 - accura - ETA: 53s - loss: 0.0503 - ETA: 50s - loss: 0.0501 - accuracy: 0. - ETA: 50s - loss: - ETA: 49s - loss: 0.0502 - accuracy: 0.983 - ETA: 49s - loss: 0.0502 - accuracy: 0. - ETA: 48s - ETA: 44s - loss: 0.0502 - accuracy: - ETA: 44s - - ETA: 4 - ETA: 41s - loss: 0.0505 - accuracy: - ETA: 40s - loss: 0.0 - ETA: 39s - loss: 0.05 - ETA: 36s - loss: 0.0502 - acc - ETA: 35s - loss: 0.0503 - accuracy: 0.983 - ETA: 35s - loss: 0.0503 - accur - ETA: 35s - loss: 0.0502 - - ETA: 34s - loss: 0.0504 - accurac - ETA: 33s - loss - ETA: 16s -  - ETA: 14s - loss: 0.0504 - accuracy: 0.983 - ETA: 14s - loss: 0.0504 - accuracy: - ETA: 14s - loss: 0.0504 - accurac - E - ETA: 3s - loss: 0.0500  - ETA: 2s - los - ETA: 1s - loss: 0.0501 - accu - ETA: 1s - - ETA: 0s\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0495 - accuracy: 0.9843:  - ETA: 1:21 - E - ETA: 1:13 - l - ETA: 1:12 - loss: 0.0487 - accura - ETA: 1:12 - loss: 0.0487 - accuracy:  - ETA: 1:12 - lo - ETA: 47s - loss: 0.0488 - accuracy: 0. - ETA: 46s - loss: 0.0489 - accur - ETA: 46s - loss: 0.0488 - accura - ETA: 45s - loss: 0.0488 - accuracy: 0.98 - ETA: 45s - loss: 0.0488 - acc - ETA: 44s - loss: 0.0489 - accuracy: 0 - ETA: 44s - loss: 0.0489 - accuracy: 0.98 - ETA: 44 - ETA: 40s - loss: 0.0488 - accuracy:  - ETA: 40s - loss: 0.0488 - accura - ETA: 39s - loss: 0.0487 - acc - ETA: 39 - ETA: 37 - ETA: 35s - loss - ETA: 27s - loss: 0.0490 -  - ETA: 26s - loss: 0.0490 - accuracy: 0. - ETA: 26s - loss: 0.0491  - ETA: 25s - loss: 0.0490 - accuracy - ETA: 25s - loss: 0.0490 - accuracy: - ETA: 24s - loss: 0.0491 - accuracy - ETA: 24s - loss: 0.0490 - accuracy: 0 - ETA: - ETA: 22s - loss:  - ETA: 20s - loss: 0.0495 -  - ETA: 20s - loss: 0. - ETA: 1 - ETA: 10s - loss: 0.0 - ETA: 9s - loss: 0.0494 - accuracy - - ETA: 7s - loss: 0.0495 - accuracy:  - ETA: 7s - loss: 0.0495  - ETA: 6s - loss: 0.0 - ETA: 6s - loss: - ETA: 4s - loss: 0.0494  - ETA - ETA: 1s - loss: - ETA: 1s - los - ETA: 0s - loss: 0.0495 - \n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.0477 - accuracy: 0.9849: 1:05 - loss: 0.0483 - accu - E - ETA: 1 - ETA: 50s - loss: 0.0474 - accuracy - ETA: 49s - loss: 0.0475 -  - ETA: 48s - loss: 0.0476 - accuracy: 0 - ETA: 48s - loss: 0.0476 - accuracy: 0 - ETA: 48s  - ETA: 44s - loss: 0.0475 - accuracy: 0.9 - E - ETA: 42s - loss: 0.0472 - accurac - ETA: 41s - loss: 0.0475 - accu - ETA: 41s - loss - ETA: 39s - loss: 0.0473 - accuracy: - ET - ETA: 37s - loss: 0. -  - ETA: 31s - loss: 0 - ET - ETA: 23s - loss: 0.0474 - ac - ETA: 23s - loss: 0.0474 - accuracy: 0.985 - ETA: 23s - loss: 0.0474 - accurac - E - ETA: 18s - loss: 0.0472 - accuracy: 0 - ETA: - ETA: 0s - loss:\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0474 - accuracy: 0.9853TA: 1:18 - loss: 0.0452  - ETA: 1:08 - loss: 0.046 - ETA: 1:08 - loss: 0.046 - ETA: 1:04 - ETA: 1:03 - loss: 0.0474 - accuracy:  - ETA: 1:03 - loss: 0.0 - ETA: 1:02 - loss: - ETA: 1:02 - loss: 0.0 - ETA - ETA: 1:00 - loss: 0.0486 - accuracy: 0.98 - ETA: 1:00 - loss: 0.048 - ETA: - ETA: 55s - loss: 0.0481 - a - ETA: 54 - ETA: 46s - loss: 0.0483 -  - ETA: 45s - loss: 0.0483 - accuracy: 0.985 - ETA: 45s - loss - ET - ETA: 41s - loss - ETA: 40s - loss: 0.0479  - ETA: 39s - lo - ETA: 35s - loss: - ETA: 32s - loss: 0.0478 - accur - ETA: 31s - loss: 0.0478 -  - ETA: 28s - loss: 0.0477 - accurac - ETA: 28s - loss: 0.0477 - accuracy: 0 - ETA: 27s - loss: 0.0477 - acc - ETA: 27s - loss: 0.0476 - ETA: 26s - loss: 0. - ETA: 25s - loss: 0.0477 - a - ETA: 24s -  - ETA: 22s - loss: 0.0477 - accuracy: 0.98 - ETA: 22s - loss: 0.0477 - accuracy: 0.985 - ETA: 22s - loss: 0.0477 - accuracy: 0 - ETA: 22s - loss: 0.0478 - accuracy: - ETA: 22s - loss: 0.0478 - ETA: 21s - loss: 0.0477 - accuracy: 0 - ETA: 20s - loss:  - E - ETA: 17s - loss: 0.0479 - accuracy: 0.98 - ETA: 17s - loss: 0.0478 - accurac - ETA: 16s - loss: 0.0478 - accuracy: 0.9 - ETA: 16s - loss: 0.0478 - - ETA: 15s - loss: 0.0478 - accuracy: 0 - ETA: 15s - loss: 0.0478 - accurac - ETA: 15s - loss: 0.0478 - accuracy: 0.98 - ETA: 15s - loss:  - ETA: 11s - loss: 0.0476 - accura - ETA: 11s - loss: 0.0476 - accuracy: 0.98 - ETA: 11s - loss: 0.0476 - acc - ETA: 10s - loss: 0.0477 - accuracy: 0.985 - ETA: 10s - loss: 0.0477 - - ETA: 9s - loss: 0 - ETA:  - ETA: 0s - loss: 0.0474 \n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0461 - accuracy: 0.9856: 1:31 - l - ETA: 1:22 - - ETA: 58 - ETA: 54s - loss: 0.0467 - accurac - ETA - ETA: 52s - loss: 0.0467 - accuracy: 0 - ETA: 52s - loss:  - ETA: 50 - ETA: 48s - loss: 0.0465 - accurac - ETA: 48s - loss: 0.0464 - ETA: 47s - loss: 0.0462 - accuracy - ETA: 47s - l - ETA: 43s - loss: 0.0462 - accuracy:  - ETA: 43s - loss: 0.0462 - accuracy: - ETA: 42s - loss: 0.0462 - ET - ETA: 28s - loss:  - E - ETA: 24s - loss: 0.0462 - accur - ETA: 24s - loss: 0.0462  - ETA: 23s - \n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.0464 - accuracy: 0.9857: 58s - loss: 0.0451 - accuracy: 0.98 - ETA: 58s - loss: - ETA: 57s - loss: 0.0450 -  - ETA: 56s - loss:  - ETA: 55s - ETA: 49s - loss: 0. - ETA: 47s - loss: 0.0453 - accu - ETA: 47s - loss: 0.0453  - ETA: 46s - loss: - ETA: 42s - l - ET  - ETA: 36s - loss: 0.0463 - accuracy - ETA: 36s - loss: 0.0463 \n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0465 - accuracy: 0.9860: 1:23 - los - ETA: 1:21 - loss: 0.041 - ETA: 1:21 - loss: 0.0416 - accu - ETA - ETA: 1:19 - loss: 0.043 - ETA: 1: - ETA: 1:08 - loss: 0.0442 - accura - ETA: 1:08 - loss: 0.0 - ETA: 1:07 - loss: 0.0448 - accuracy: 0.98 - ETA: 1:07 - loss: 0.0450 - ac - ETA: 1:07 - loss: 0.0451  - E - ETA: 1:04 - loss: 0.0 - ETA: 1:03 - loss: 0 -  - ET - ETA: 50s - loss: 0 - ETA: 47s - loss: 0.0462 - ac - ETA: 46s - loss: 0.0463 - ETA: 45s - loss:  - ETA: 44s - loss: 0.0464 - accur - ETA: 43s - loss: 0.0465 - accuracy: 0. - ETA: 43s - loss: 0.0465 - ac - ETA: 40s - loss: 0 - ETA: 39s - loss: 0.0464 - accu - ETA: 38s - ETA: 37s - loss: 0.0463 - acc - ETA: 36  - ETA: 16s - l - ETA: 14s - loss: 0.0464 - acc - ETA: 11s - loss: 0.046 - ETA: 10s - loss: 0.0464 - accuracy: 0.9 - ETA: 10s - loss: 0.0464 - ac\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0466 - accuracy: 0.9861: 1:16 - loss: 0.0426 - accuracy - ETA - ETA: 1:15 - loss: 0.0425 - accuracy - ETA: 1:14 - - ETA:  - ETA: 1:13 - loss: - ETA: 1:10 - loss: 0.0446 - accuracy: 0.98 - ETA: 1:10 - loss: 0.0447 - accu - ETA: 1:09 - loss: 0.0448 - accu - ETA: 1:04 - loss: 0.045 - E -  - ETA: 50s - loss: 0.0457 - acc - ETA: 49 - ETA: 47s - loss: 0.0455 - accuracy: 0.9 - ETA: 47s - loss: 0.0456 - accuracy: 0.9 - ETA: 47s - loss: 0.0456 - accuracy: - ETA: 47s - loss: 0.0456 -  - ETA: 44s - loss: 0.0456 - accuracy: 0 - ETA: 43s - loss:  - ETA: 42s - loss: 0.0 - ETA: 41s - loss: 0.0456 - acc - ETA: 40s - loss: 0.0456 - accurac - ETA:  - ETA: 3 - ETA: 34s - loss: 0.0456 - accuracy: 0. - ETA: 34 - ET - ETA: 19s - loss: 0.0461 - a - ETA:  - ETA: 3s - loss: 0.0466 - accuracy: 0. - ETA: 2s - loss: 0.0467 - accura - ETA: 2s - los - ETA: 1s - - ETA: 1s - loss: 0.0467 -  - ETA: 0s - l\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0481 - accuracy: 0.9864 ETA: 1:18 - loss: 0.0470 - ac - ETA: 1:13 - ETA: 1:04 - los - ETA: 1:03 - loss: 0. - ETA: 54s - loss: 0.0479 - accuracy - ETA: 53s - loss: 0.0479 - accuracy: 0.986 - ETA: 53s - loss: 0.0479 - accuracy: - ETA: 53s - loss: 0.0478 - accura - ETA: 52s - loss: 0.0477 - accuracy:  - ETA: 52s - ETA: 44s - loss: 0.0491 - accuracy: 0.98 - ETA: 43s - loss: 0.0491 - accuracy:  - ETA: 43s - loss: 0.0490 - ac - ETA: 42s -  - ETA: 41s - l - ETA: 35s  - ETA: 33s - los - ETA: 32s - loss: 0.0486 - ETA: 31s - loss: 0 - ETA: 29s - loss: 0.0483 - accuracy: 0.98 - ETA: 29s - loss: 0.0486 - accuracy: - ETA: 29s - loss: 0.0487 - accuracy: 0 - ETA: 29s - loss: 0.0487 - accu - ETA: 28s - loss: 0.0486 - ac - ETA: 27s - loss: 0.048 - ETA: 26s - loss: 0.0483 - accura - ETA: 26s - loss: 0.0483 - accuracy: 0.986 - ETA: 26s - loss: 0.0483 - a - ETA: 25s - loss: 0.0482 - accuracy:  - ETA: 25s - lo  - ETA: 19s - loss: 0.0490 - accuracy:  - ETA: 16s - loss: 0.0488 - accuracy: 0.9 - ETA: 1\n",
      "1342568/1342568 [==============================] - 11s 8us/step\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.4786 - accuracy: 0.7970: 51s - loss: 0.5426 - - ETA: 48s - loss: 0.5355 - acc - ETA: 47s - loss: 0. - ETA: 46s - loss: 0.531 - ETA: 45s - loss: 0.5299 - accu - ETA: 44s - loss: 0.5287 - ac - ETA: 43s - loss: 0.5272 - accuracy:  - ETA: 43s - loss: 0.5267 - accuracy: - ETA: 43s - loss - ETA: 41s - loss: - ETA: 40s - loss: 0.5214 - accuracy - ETA: 39s - loss: 0.5207 -  - ETA: 39s - loss: 0.5195 - accuracy: 0.775 - ETA: 38s - lo - E - ETA: 35s  - ETA: 33s - loss: 0.5118 - accuracy: - ETA: 33s - loss: 0.5113 - a - ETA: 32s - loss:  - ETA: 24s - loss: 0.5001 - ac - ETA: 18s - loss: 0.4944 -  - ETA: 18s  - ETA: 16s - l - ETA: 14s - loss: 0.4904  - ETA - ETA: 0s - l\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.4022 - accuracy: 0.8371: 1:06 - - ETA: 1: - ETA  - ETA: 46s - loss: 0.4105 - ac - ETA: 46s - loss: 0.4104 - accuracy: - ETA: 45s - loss: 0.4102 - accuracy: 0.833 - ETA: 45s - loss: 0.4102 - accu - ETA: 45s - loss: 0.4101 - ac - ETA: 44s - loss: 0.4 - ETA: 40s - loss: 0.4093 - ac - ETA: 40s - loss: 0.4091  - ETA: 37s - loss: 0.4083 - - ETA: 36s - ETA: 34s - loss: 0.4079 - acc - ETA: 33s - lo - ETA: 32s - loss: 0.4075 - ETA: 13s - loss: 0.4041 - accuracy: 0.8 - ETA: 12s - loss: 0.4 - ETA: 11s - loss: 0.4039 - accuracy: 0 - ETA: 11s - loss: 0.4038 -  - - - ETA: 0s - loss: 0.4023 \n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3814 - accuracy: 0.8475: 59s - loss: 0.3854 - accuracy: - ETA: 58s - ETA: 56s - loss: 0.3854 - - ETA: 56s - loss: 0.3853 - accu - ETA: - ETA: 53s - loss: 0.3853 - accuracy: 0.845 - ETA: 53s - loss: 0.3853 - accuracy: 0. - ETA: 53s - loss: 0.3852 - accuracy: 0.8 - ETA: 53s -  - ETA: 49s - loss: 0.3849 - accuracy: 0.8 - ETA: 49s - loss: 0.3849 - accuracy: 0.845 - ETA: 49s - loss: 0.3 - ETA: 47s - loss: 0.3848 - accuracy: 0.84 - ETA: 47s - loss: 0.3848 - ac - ETA: 47s - loss: 0.3849 - - ETA: 46s - loss: 0.3848 - accuracy: 0.84 - ETA: 46s - loss: 0.3848 - accuracy: 0.845 - ETA: 46s - loss: 0 - ETA: 42s - loss - ETA: 41s - loss: 0.3 - ETA - ETA: 38s - loss: 0.3839 - acc - ETA: 37s - loss:  - ETA: 36s - loss: 0.3839 - accuracy: 0. - ETA: 35s - loss: 0.383 - ETA: 34s - loss: 0.3838 - accurac - ET - ETA: 32s - loss: 0.3836 - accuracy: - ETA: 32s - loss: 0.3836 - acc - ETA: 31s - loss: 0.3836 - accuracy: 0.84 - ETA: 31s - loss: 0.3 - ETA: 30s - loss: 0.3834 - accuracy: 0. - - ETA: 25s - loss: 0 - ETA: 24s - loss:  - ETA: 23s - loss: 0.3830 - accur - ETA: 22s - loss: 0 - ETA: 21s - loss: 0.3828 - accuracy: 0.84 - ET\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3713 - accuracy: 0.8525: 28s - loss: 0.3727 - accuracy: 0 - ETA: 28s - loss: 0.3726 - accuracy: 0. - ETA: 28s - loss: 0 - ETA: 26s - loss: 0.3725 - acc - ETA: 26s - loss: 0.3725  - ETA: 25s - loss: 0.3726 - accu - ETA: 24s - loss: 0.3726 - ETA: 21s - loss: 0.3723 -  - ETA: 20s - loss: 0.37 - E - E - E - ETA: 1s - l - ETA: 0s\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3644 - accuracy: 0.8563: 49s - loss: 0.3653 - ac - ETA: 49s -  - - ETA: 36s - loss: 0.3649 - accuracy: 0 - ETA: 36s - loss: 0.3649 - accuracy:  - ETA: 35s - loss: 0.3649 -  - ETA: 34s - loss: 0.3649 - accuracy:  - ETA: 34s - loss: 0.3652 - a - ETA: 33s - loss: 0.3652 - ac - ETA: 33s - loss: 0.3651 - accur - ETA: 32s -  - ETA: 31s - loss: 0.3653 - accuracy: 0.8 - ETA: 30s - loss: 0.3653 - accuracy: 0.856 - ETA: 30s - loss: 0.3653 - accuracy: 0. - ETA: 30s - loss: 0.3653 - accuracy - ETA: 2 -  - ETA: 17s - loss: 0.3648 -  - ETA: 16 - ETA: 14s - loss: 0.3 - ETA: 13 - ETA: 0s - loss: 0.3645 - ac - ETA: 0s - loss: 0.3644 - accuracy: 0. - ETA: 0s - loss: 0.3644 - ac\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3591 - accuracy: 0.8592:  - ETA: 1:02 - loss: 0.3595 - accuracy - ETA: 1: - ETA - ETA: 59s - loss: 0.3593 - accuracy - ETA: 45s - loss: 0.3612 - accuracy:  - ETA: 45s - loss: 0.3611 - accuracy: 0 - ETA: 45s - loss: 0.3611 - accuracy: 0.858 - ETA: 45s - loss: 0.3 - ETA: 4\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3534 - accuracy: 0.8618: 57s - loss: 0.3541 - ac - ET - ETA: 55s - loss: 0.3540 - accuracy: 0. - ETA: 55s - loss: 0.3540 - accuracy: 0.861 - ETA: 54s - loss: 0.3540 - accuracy: 0. - ETA: 54s - ETA: 53s - loss: 0.3540 - accuracy: 0.861 - ETA - ETA: 51s - loss: 0.3540 - a - ETA: 50s - loss: 0.3538 - a - ETA: 49s - loss: 0.3542 - accu - ETA: 48s - loss: 0.3542 -  - ETA: 48s - loss: 0.3540 - - ETA: 47s - loss: 0. - ETA: 39s - loss: 0.3541 - ac - ETA: 38s - loss: 0.3540 - accuracy: 0.8 - ETA: 38s - loss: 0.3539 - accuracy:   - ETA: 35s - ETA: 34s - loss: 0.3545 - accuracy:  - ETA: - ETA: 20s - loss: 0.  - ETA: 12s - loss: 0.3535  - - ETA: 9s - loss: 0.3534 - accuracy:  - - ETA: 8s - loss: 0.3 - ETA: 4s - l - ETA: 0s - loss: 0\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3491 - accuracy: 0.8639: 1:20 - loss: - E - ETA: 1:17 - loss - ETA: 58s - loss: 0.3 - ETA: 57s - loss: 0.3496 - accuracy: 0.8 - ETA: 57s - loss: 0.3497  - ETA: 56s -  - ETA: 54s - loss: 0.3495 - accuracy: - ETA: 54s - loss: 0.3495 - accuracy: 0. - ETA: 54s - loss: 0.3494  - ETA: 53s - loss: 0.3495 - a - ETA: 5 - E - ETA: 48s - loss: 0.3495 - accurac - ETA: 47s  - ETA: 39s - loss: 0.3498 - accuracy: 0.863 - ETA: 39s - loss: 0.3498 - accuracy: 0.86 - ETA: 39s - loss: 0.3497 - accuracy: 0.863 - ETA: - ETA: 37s - loss: 0. - ETA: 36s - loss: 0.3498 - a - ETA: 35s - loss: 0.3496 - acc - ETA: 30s - loss: 0.3493  - ETA: 0s - loss: 0.3491 - accura - ETA: 0s - loss: 0.3491 - accuracy: 0.\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3455 - accuracy: 0.8660: 58s - loss: 0.3464 - accuracy: 0.86 - ETA: 58s - loss: 0.3465  - ETA: 57s - loss: 0.3467 - accura - ETA: 57s - loss:  - ETA: 55s - loss: 0.34 - ETA: 45s - loss: 0.3468 - ac - ETA: 44s - loss: - ETA: 43s - loss: 0.3468 - accuracy: 0. - ETA: 43s -  - ETA: 41s - loss: 0.3467 - accurac - ETA: 41s - loss: 0.3468 - accura - ETA: 40s - loss: 0.3469 - accurac - ETA: 40s - - ETA: 38s - loss: 0.3466 - accuracy: 0.865 - ETA: - ETA: 36s - loss: 0.3467 - acc - ETA: 36s - loss: 0.3468 - accuracy: - ETA: 35s - loss: 0.3466 -  - ETA: 34s - loss: 0.3465 - ETA: 34s - loss: 0.346 - ETA: 32s - loss: - ETA: 31s - loss: 0.3463 - ac - ETA: 30s -  - ETA: 27s - loss: 0.3461 - accur - ETA: 26s - loss: 0.3461 - accuracy: 0.865 - ETA: 26s - loss: 0.3461 -  - ETA: 25s - loss: 0.3461  - ETA: 24s - loss: 0.3461 - accura - ETA: 24s - loss: 0.3461 - accuracy - ETA: 23s - loss: 0.3461 - accurac - ETA: 16s - loss: 0.3460 - accuracy:  - ETA: 16s - lo - ETA: 14s - loss: 0.3459 -  - ETA: 13s - loss: 0.3458 - accuracy: 0 - ETA: 13s - loss: 0.3458 - accuracy - ET - ETA: 11s - loss: 0.3457 - acc - ETA: 10s - loss: 0.3457 - accuracy: 0.86 - ETA: 10s - lo - ETA: 8s - loss:\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3431 - accuracy: 0.8673: 1:11 - loss: 0.342 - ETA: 55s - los - ETA: 54s - loss: 0.34 - ETA: 53s - loss - ETA: 49 - ETA: - ETA: 46s - loss: 0.3446 - accur - ETA: 45s - loss: 0.34 - ETA: 42s -  - ETA: 40s - loss: 0.3 - ETA: 39s - loss: 0.3441 - acc - ETA: 38s - loss: 0.3442 - accuracy: 0. - ETA: 38s - loss: 0.3442 - a - ETA: 37s  - ETA: 36s - loss - ETA: 30s - loss: 0.3443 - accuracy:  - ETA: 30s - loss: 0.3443 - accuracy: 0.86 - ETA: 29s - loss: 0.3443 - a - ETA: 29s  - ETA: 27s - loss:  - ETA: 26s - loss: 0 - ETA: - ETA: 0s - l\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3421 - accuracy: 0.8689: 1: - ETA: 1:09 - loss: 0.3466 - accuracy: 0.86  - ETA: 55s - loss:  - ETA: 53s - loss: 0. - ETA: 43s - loss: 0.3413 - accuracy:  - ETA: 43s - loss: 0.3412 - accuracy: 0. - ETA: 42s - loss: 0.3413 - - ETA: 42s - loss: 0.3411 - accuracy: 0.868 - E  - ETA:  - ETA - ETA: 22s - loss: 0.3435 - accuracy: 0.86 - ETA: 22s - loss: 0.3435  - ETA: 21s - loss: 0.3 - ETA: 2 - ETA: 6s - loss: 0.3426 - accuracy: 0.86 - ETA: 6s - loss: 0.3426 - accuracy: 0. - ETA: 6s - loss: 0.3 - ETA - ETA: 3s - loss: 0.3424  - ETA: \n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3377 - accuracy: 0.8696: 1:21 - loss: 0.3377  - ETA: 1:20 - loss: 0.3356  - ETA: 1:19 - los - ETA: 1:11 - - ETA: 59s - loss: 0.3363 - acc - ETA: 59s - los - ETA: 57s - loss: 0.3391 - a - ETA: 56s - loss - ETA: 55s - loss: 0.3390 -  - ETA: 54s - loss: 0.3388 - accuracy: 0. - ETA: 54s - loss: 0.3387 - a - ETA:  - ETA: 37s - loss: 0.3 - ETA: 36s - loss: 0.3381 - accuracy: 0 - ETA: 36s - loss: 0.3381 -  - ETA: 35s - lo - ETA: 34s - loss: 0.33 - ETA: 33s - loss: 0.3380 - accuracy: 0. - ETA: 33s - loss: 0.3380 - accuracy: 0. - ETA: 32s - loss: 0.3380 - accuracy:  - ETA: 32s - loss: 0.3380 - accuracy - ETA: 32s - loss: 0.3380 - accuracy: - ETA: 31s - loss: 0.3379 - a - ETA: 31s - loss: 0.3378 -  - ETA: 30s - loss: 0.3 - ETA: 29s - loss: 0.3375 - acc - ETA: 28s - loss: 0.3375 - accuracy: 0.869 - ETA: 28s - loss: 0.3375 - accura - ETA: 27s - loss: 0.3377 - ETA: 26s - loss: 0.3375 - accuracy: 0.8 - ETA: 26s - loss: 0.337 - ETA: 25s - loss: 0.3375 - accuracy: 0.86 - ETA: 25s - loss: 0.3376 - accura - ETA: 25s - loss: 0 - ETA: 23s - loss: 0.3374 -  - ETA: - ETA: 21s - loss: 0.3376 - ETA: 20s - loss: 0.3376 - acc - ETA:  - ETA: 17s - loss: 0.3377 - acc - ETA: 17s - loss: 0.3376 - accuracy: 0. - ETA: 16s - loss: 0.3375 - accuracy: 0.8 - ETA: 16s - loss: 0.3375 - accuracy: - ETA: 16s - loss: 0.3376 - ETA: 15s - loss: 0.3374 - accuracy: 0.869 - ETA: 15s - loss: 0.3374 - accuracy:  - ETA: 15s - loss: 0. - ETA: 13s - - ETA: 10s - lo - ETA: 3s - loss: 0 - ETA: 3s - loss: 0.3375 - ac - E - ETA: 0s - loss: 0\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3362 - accuracy: 0.8710: 54s - loss: 0.3334 - accuracy: 0 - ETA: 54s - loss: 0.3334 - accuracy: 0.8 - ETA: 53s - loss: 0.3334 - accuracy: 0.871 - ETA: 53s - loss: 0.3335 - ac - ETA: 53s - loss: 0.3335 - ac - ETA: 52s - loss: 0.3336 - accu - ETA: 51s - loss: 0.3337 - accurac - ET - ETA: 49s - loss: 0. - ETA: 48s - loss: 0.3336 - accuracy: 0. - ETA: 47s - loss: 0.3337 - accuracy: 0.871 - ETA: 47s - l - ETA: 46s - loss: 0 - ETA: 45s - loss: 0.33 - ETA: 44s - loss: 0.3338 - accuracy: 0 - ETA: 43s - loss: 0.3337 - accuracy: - ETA: 43s - loss: 0.3338 - accuracy: 0.8 - ETA: 43s - loss: 0.3338 - accuracy: - ETA: 42s - loss:  - ETA: 41s - loss - ETA: 40s - loss: 0 - ETA: 39s - l - ETA: 37s - loss: 0.3341 - accuracy:  - ETA: 37s - loss: 0.3342 - accuracy: 0.87 - ETA: 37s - loss: 0.3342 - accuracy: 0.871 - ETA: 37s - loss: 0.3342 - accuracy - ETA: - ETA: 34s - lo - ETA: 33s - loss: 0.3363 - - ETA - ETA: - ETA: 28s - loss:  - ETA: 27s - loss: 0.3359 - acc - ETA: 26s - loss: 0.3359 - accuracy: 0.8 - ETA: 26s  - ETA: 2 - ETA: 23s - loss: 0.3360 - acc - ETA: 22s - loss: 0.3359 - accuracy - ETA: 21s - loss: 0.3359 - accuracy: 0.8 - ETA: 21s - loss: 0.3360 - accuracy - ETA: 21s - loss: - ETA: 20s - loss: 0.3362 -\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3336 - accuracy: 0.8718: 1:23 - los - ETA: 1:21 - l - ETA: 1:20 - loss: 0.3359 - ac - ETA: 1:16 - loss: 0.3346 - accu - ETA: 1: - ETA: 1:14 - loss: - - E - ETA: 1:11 - loss: 0.3338 - accuracy: 0.87 - ETA: 1: - ETA: 1:07 - - ETA: 1:07 - loss: 0.3 - ETA: 1:06 - loss: 0.3339 - ac - ETA: 1:00 - loss: 0.3342 - a - ETA: 59s - loss: 0.3339 - accuracy: 0.87 - ETA: 59s - lo - ETA: 57s - loss: 0.3337 - accuracy: 0. - ETA: 57s - lo - ETA: 56s - loss: 0.33 - ETA: 55s - loss: 0.3340 - accuracy - ETA: 54s - los - ETA: 48s - loss: 0.3340 - ETA: 47s - loss: 0.3339 - accura - ETA: 47s - loss - ETA: 45s - loss: 0.3337 - acc - ET - ETA: 29s - loss:  - ETA: 5s - loss: 0.3337 -  - - ETA: 3s - loss: 0.3336 - accuracy:  - ETA: 3s - loss: 0.3336  - ETA: 0s - loss: 0.3336 - accuracy: 0.87\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.3313 - accuracy: 0.8731: 1: - ETA: 1:00 - loss:  - ETA: 58s - loss: 0.3305 - accurac - ETA: 55s - loss: 0 - ETA: 54s - loss: 0.3308 - accuracy: 0.8 - ETA: 53s - loss: - ETA: 52s - loss: 0.3305 - accuracy:  - ETA: 45s - loss: 0.3309 - accurac - ET - ETA: 38s - loss: 0.3315 -  - ETA: 35s - loss: 0.3313 - accur - ETA: 34s - loss: 0.3314 - ETA: 33s - loss: - ETA: 25s - - ETA: 23s - loss: 0.33 - ETA: 22s - loss: - ETA: 21s - loss: 0.3314 - a - ETA: 20s - loss: 0.3313 - accuracy: - ETA: 18s  - ETA: 14s - loss: 0.3317 - ETA: 13s - loss: 0.3317 - accuracy: 0.873 - ETA: 13s - loss: 0.3317 - accuracy - ETA: 12s - loss: 0.3317 - accuracy: 0.873 - ETA: 12s - loss: 0.3317 - - ETA: 11s - loss: 0.3316 - accuracy: 0.8 - - ETA: 9s - loss: 0.3318 - accura - ETA: 8s - loss: 0.3318 - accuracy: 0. - ETA: 8s - los - ETA: 5s - loss: 0.3317 -  - ETA: 2s - - ETA:  - ETA: 1s - loss: 0.3314 - accuracy: 0. - ETA: 0s - ETA: 0s - loss: 0.3313 - accuracy: 0.\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3331 - accuracy: 0.8741: 1:10 - loss: - ETA: 1:09 - loss: 0.332 - ETA: 1:08 - - ETA: 1:06 - l - ETA: 1:02 - loss: 0.3300 - accuracy:  - ETA: 1 - - ETA: 55s - ETA: 54s - loss: 0.3297 - a - ETA: 53s - loss: 0.3399 - a - ETA: 52s - loss: 0.3398 - a - ETA: 49s - loss: 0.3382 - - ETA: 48s - loss: 0.3378 -  - ETA: 47s - loss: 0.3372 - a - ETA: 47s - loss: 0.3372 - accuracy: 0.87 - ETA: 46s - loss: 0.3372 - acc - - ETA: 37s - loss: - ETA: 33s - loss: 0.3 - ETA: 32s - loss: 0.3362 - ac - ETA: 31s - loss: 0.3360 - accura - ETA - ETA: 27s - loss: 0.3355 - accuracy: 0.87 - ETA: 24s - loss: 0.3352 - ETA: 23s - loss: 0. - ETA: 22s - loss: 0 - ETA: 21s - loss: 0.3346 - accuracy - ETA: 20s - loss: 0.3345 -  - ETA: 20s - loss: 0.3343 -\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3338 - accuracy: 0.8747: 59s - los - ETA: 58s - loss: 0. - ETA: 57s - loss: 0.3268 - - ETA: 56s - loss: 0.3270 - acc - ETA: 55s - loss: 0.3271 - accurac - ETA: 55s - loss: 0.3270 - ac - - ETA: 38s - loss: 0.3353 - accuracy: 0.874 - ETA: 38s - loss: 0.33 - - ETA: 30s - loss: 0.3355  - ETA: 29s - loss: 0.3353 - accuracy: 0.8 - ETA: 29s - ETA: 27s - loss: 0.3350 - accurac - ETA: 27s - los - ETA: 26s - loss: 0.3346 - accuracy: - ETA: 25s - los - ETA: 24s - loss: 0. - ETA: 23s - l - ETA: 8s - loss: 0.333 - ETA: 8s - loss: 0.3330 - accu - ETA: 8s - loss: 0.3330 - accuracy: 0.87 - ETA: 8s - loss: 0.3330 - accuracy: 0.87 - ETA: 8s - loss: - ETA: 0s - loss: 0.3339 -  - ETA: 0s - loss: 0.3338 - \n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3268 - accuracy: 0.8755\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.3259 - accuracy: 0.8764\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.3243 - accuracy: 0.8770\n",
      "1342568/1342568 [==============================] - 12s 9us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3490 - accuracy: 0.8712\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2843 - accuracy: 0.8950\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2697 - accuracy: 0.9004\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 82s 30us/step - loss: 0.2624 - accuracy: 0.9033\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2577 - accuracy: 0.9054\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 82s 30us/step - loss: 0.2541 - accuracy: 0.9070\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2506 - accuracy: 0.90872s - l - ETA: 0s - loss:\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2485 - accuracy: 0.9093\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2455 - accuracy: 0.9104\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2494 - accuracy: 0.91130s - loss: 0.2495 - accuracy\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2437 - accuracy: 0.9116\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2415 - accuracy: 0.9123\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2402 - accuracy: 0.9129\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 82s 30us/step - loss: 0.2402 - accuracy: 0.9136: 22s - loss: 0.2399 - accuracy: 0 - ETA: 22 - ETA: 20s - loss: 0.2399 - accu - ETA: 19s - loss: 0.\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2384 - accuracy: 0.9142:  - ETA: 1:28 - loss: 0.2371  - ETA: 1:26 - - ETA: 1:11 - loss: 0 - ETA: 1:05 - loss: 0.2390  - - ETA: 59s - lo - ETA: 58s - loss: 0.2382 - accuracy: 0. - ETA: 58s - loss: 0. - ETA: 56s - loss: 0.2384 - accuracy: 0. - ETA: 56s - loss: 0.2 - ETA - ETA: 48s - loss: 0.2382 - accuracy: 0.9 - ETA: 48s - loss:   - ETA: - ETA: 34s - loss: 0.2380 - - ETA: 33s - loss: 0.2381 - accuracy:  - ETA: 32s - lo - ETA: 22s - loss: 0.23 - ETA: 21s - loss: 0.2382 - accur - ETA: 20s - loss: 0.2 - ETA: 19s - loss: 0.2382 - accuracy: 0 - ETA: 19s - loss: 0.2381 - accura - ETA: 18s - loss: 0.238 - ETA: - E - ETA - ETA: 0s - l\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2365 - accuracy: 0.9150: 1:07 - ETA: 1:05 - loss: 0.2383 - accuracy - ETA: 1:05 - loss: 0.2380 - accuracy:  - ETA: 1:04 - loss: 0.2379 - accuracy - ETA:  - ETA: 1:01 - l - - ETA: 56s - loss: 0.2374 - accuracy: 0.91 - ETA: 56s - los - ETA: 55s - loss: 0.2370 - accuracy - ETA: 54s - loss: 0.2371 - a - ETA: 53s - loss: 0.2369 - accur - ETA: 53s - loss: 0.2367 - E - ETA: 43s - loss: 0.2367 - accurac - ETA: 43s - los - ETA: 41s - loss: 0.2366 - accuracy: 0.915 - ETA: - ETA: 32s - ETA: 1s - loss: 0.236 - ETA: 0s - l\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2353 - accuracy: 0.9156:  - ETA: 1:09 - loss: - ETA: 1:08 - ETA: 46s - loss: 0.2332 -  - ETA: 46s - loss: 0.2331 - accuracy: 0 - ETA: 45s - loss: 0.2331 - ETA: 44s - l - ETA: 43s - los - ETA: 39 - ETA: 37s - loss: 0.2339 - accura - ETA: 37s - loss: 0.23 - ETA: 36s - loss: 0.2338  - ETA: 3 - ETA: 31s - loss: 0.2351  - ETA: 30s - loss: 0.2350 - accur - ETA: 29s - loss: 0.2350 - accuracy:  - ETA: 29s - loss: 0.2350 -  - ETA: 28s - loss - ET - ETA: 22s - loss: 0.2355 - accuracy: 0.91 - ETA: 22s - loss: 0.2 - ETA: 21s - loss: 0.2355 - accuracy: 0 - ETA: 21s - loss: 0.2355 - a - ETA: 20s - loss: 0.2 - ETA: 19s - loss - ETA: 18s - loss: 0.2354 - accuracy - ETA: 17 - ETA: 16s - loss: 0.235 - ETA: 15s - loss: 0.2353 - accuracy: 0 - ETA: 14s - loss: 0.2353 - accuracy: - ETA: 14s - loss: 0.2354 - ac - ETA: 13s - loss: 0.2353 - ETA: 12s - loss: 0.2 - ETA: 11s - loss: 0.2353 - accuracy: 0.91 - ETA: 11s - loss: 0.2353 - ac - ETA: 10s - loss: 0.2352 - accuracy: 0. - ETA: 1s - loss: 0.2353 - ac - ETA: 0s - loss: - ETA: 0s - loss: 0.2353 - ac\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2366 - accuracy: 0.9155TA: 56s -  - ETA: 54s - loss: 0.2363 - accura - ETA: 53s - l - ETA: 52s - loss: 0.2370 - acc - ETA: 51s - - ETA: 50s - loss: 0.2370 - accuracy: 0.915 - ETA: 49s - loss: 0.2370 - accuracy: 0 - ETA: 49s - los - ETA: 48s - loss: 0.2366 - accuracy - ETA: 47s - loss: 0.2365 - acc - ETA: 47s - loss: 0.2365 - accu - ETA: 46s  - - ETA: 42s -  - ETA: 41s - loss: 0.2 - ETA: 37s - loss: 0.2360 - accuracy: 0. - ETA: 37s - loss: 0.2360 - accuracy: 0.9 - ETA: 37s - - ETA: 35s - loss: 0.2 - ETA: 34s - loss: 0.2356 - accur - ETA: 34s - loss: 0.2356 - accuracy:  - ETA: 3 - ETA: 29s - loss: 0.2365 - accuracy: 0.91 - ETA: 29s - loss: 0.2365 - - ETA: 28s - loss: 0.2368  - ETA: 27s - loss: 0.2368 - accuracy: 0.91 - ETA: 27s - loss: 0.2368 - - ETA: 15 - ETA: 0s - los\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2374 - accuracy: 0.9161: 57s  - ETA: 55s - loss: 0.2349 - accuracy: 0.91 - ETA: 55s - loss: 0.2350 - - ETA: 49s - loss: 0.2350 - accuracy: 0.916 - ETA: 49s - loss: 0.2351 - accurac - ETA: 49s - loss: 0.2352 - accuracy: 0.9 - ETA: 49s - loss: 0.2353 - accuracy: 0. - ETA: 46s - loss: 0.2356 - accuracy: 0.9 - ETA: 46s - loss: 0.2355 - accuracy: 0. - ETA: 46s - loss: 0 - ETA: 45s - loss: 0. - ETA: 41s - loss: 0.2355 - accur - ETA: 41s - loss: 0.2356 - accuracy: 0. - ETA: 40s - loss: 0.2356 - accuracy: 0. - ETA: 40 - ETA: 38s - loss: 0.2357 - accuracy: 0.91 - ETA: 38s - loss: 0.2 - ETA: 35s - loss: 0.2354 - - ETA: 27s - loss: 0.23 - ETA: 26s - ETA: 25s - - ETA: 23s - loss: 0.2 - ETA: 22s - loss: 0.2350 - accuracy: 0.9 - ETA: 2 - ETA: 20s - los - ETA: 16s - loss: 0.2365 - accurac - ETA: 16s - loss: 0.2364 - accuracy: 0.916 - ETA: 16s - loss:  - ET - ETA: 3s - loss: 0.2375 - accura\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2336 - accuracy: 0.9169: 59s - loss: 0.2320 - accuracy: 0. - ETA: 58s - loss: 0.2320 - accura - ETA: 58s - loss: 0.2321 - accuracy: 0. - ETA: 58s - loss: 0.2322 - accuracy: 0. - ETA: 58s -  - ETA: 54s - loss: 0.2323  - ETA: 0s - loss: 0.2336 - accuracy: 0.91 - ETA: 0s - loss: 0.2336 - accuracy - ETA: 0s - loss:\n",
      "1342568/1342568 [==============================] - 11s 8us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.1185 - accuracy: 0.9586\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.0703 - accuracy: 0.9761\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0591 - accuracy: 0.9802\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0525 - accuracy: 0.9826\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0490 - accuracy: 0.9839\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0463 - accuracy: 0.9853\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0429 - accuracy: 0.9864\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0406 - accuracy: 0.9872\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0401 - accuracy: 0.9878\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 101s 37us/step - loss: 0.0389 - accuracy: 0.9883\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 101s 37us/step - loss: 0.0380 - accuracy: 0.9886\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0390 - accuracy: 0.9886\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0369 - accuracy: 0.9891\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0379 - accuracy: 0.9892\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0376 - accuracy: 0.9894\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0364 - accuracy: 0.9896\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0382 - accuracy: 0.9896\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0391 - accuracy: 0.9896\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0372 - accuracy: 0.9896\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0391 - accuracy: 0.9898\n",
      "1342568/1342568 [==============================] - 16s 12us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.4577 - accuracy: 0.8085\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 104s 39us/step - loss: 0.3724 - accuracy: 0.8518\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.3458 - accuracy: 0.8648\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.3314 - accuracy: 0.8722\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 104s 39us/step - loss: 0.3207 - accuracy: 0.8767\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.3125 - accuracy: 0.8812\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.3051 - accuracy: 0.8843\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 104s 39us/step - loss: 0.3002 - accuracy: 0.8867\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.2960 - accuracy: 0.8886\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.2930 - accuracy: 0.8905\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.2886 - accuracy: 0.8921\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.2869 - accuracy: 0.8933\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.2851 - accuracy: 0.8939\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.2823 - accuracy: 0.8958\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.2790 - accuracy: 0.8968\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 104s 39us/step - loss: 0.2774 - accuracy: 0.8977\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.2760 - accuracy: 0.8985\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 104s 39us/step - loss: 0.2756 - accuracy: 0.8989\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.2724 - accuracy: 0.8999\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.2718 - accuracy: 0.9005\n",
      "1342568/1342568 [==============================] - 16s 12us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.3378 - accuracy: 0.8759\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2702 - accuracy: 0.9005\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2512 - accuracy: 0.9080\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2405 - accuracy: 0.9121\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2342 - accuracy: 0.9151\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2289 - accuracy: 0.9171\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2235 - accuracy: 0.9191\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2220 - accuracy: 0.9202\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2203 - accuracy: 0.9214\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2180 - accuracy: 0.9226\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2175 - accuracy: 0.9229\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2155 - accuracy: 0.9241\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2230 - accuracy: 0.9247\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2116 - accuracy: 0.9255\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2110 - accuracy: 0.9261\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2110 - accuracy: 0.9263\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2085 - accuracy: 0.9268\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2102 - accuracy: 0.9275\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2049 - accuracy: 0.9280\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2056 - accuracy: 0.9286\n",
      "1342568/1342568 [==============================] - 16s 12us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 345s 129us/step - loss: 0.1176 - accuracy: 0.9591\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 349s 130us/step - loss: 0.0684 - accuracy: 0.9769\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 349s 130us/step - loss: 0.0563 - accuracy: 0.9812\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 348s 129us/step - loss: 0.0501 - accuracy: 0.9836\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 349s 130us/step - loss: 0.0452 - accuracy: 0.9853\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0425 - accuracy: 0.9866\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0414 - accuracy: 0.9874\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 353s 131us/step - loss: 0.0401 - accuracy: 0.9881 - loss: 0.0401 - accuracy: 0.98 - ETA: 1s - loss: 0.0401 \n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0384 - accuracy: 0.9888\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 353s 131us/step - loss: 0.0371 - accuracy: 0.9894\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0362 - accuracy: 0.9896\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 353s 131us/step - loss: 0.0358 - accuracy: 0.9899\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0365 - accuracy: 0.9901\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0366 - accuracy: 0.9902\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 353s 131us/step - loss: 0.0365 - accuracy: 0.9904\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0360 - accuracy: 0.9906\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 26187s 10ms/step - loss: 0.0384 - accuracy: 0.9906\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 393s 146us/step - loss: 0.0366 - accuracy: 0.9911\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 393s 146us/step - loss: 0.0367 - accuracy: 0.9915\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 393s 146us/step - loss: 0.0399 - accuracy: 0.9915\n",
      "1342568/1342568 [==============================] - 23s 17us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 363s 135us/step - loss: 0.4522 - accuracy: 0.8113\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 356s 132us/step - loss: 0.3655 - accuracy: 0.8553\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 596s 222us/step - loss: 0.3377 - accuracy: 0.8685\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 417s 155us/step - loss: 0.3206 - accuracy: 0.8761\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 443s 165us/step - loss: 0.3101 - accuracy: 0.8809\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 384s 143us/step - loss: 0.3024 - accuracy: 0.8847\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 377s 140us/step - loss: 0.2949 - accuracy: 0.8883\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 366s 136us/step - loss: 0.2906 - accuracy: 0.8907\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 379s 141us/step - loss: 0.2843 - accuracy: 0.8934\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 371s 138us/step - loss: 0.2811 - accuracy: 0.8952\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 371s 138us/step - loss: 0.2792 - accuracy: 0.8962\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 377s 141us/step - loss: 0.2740 - accuracy: 0.8982\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 382s 142us/step - loss: 0.2727 - accuracy: 0.8991\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 365s 136us/step - loss: 0.2709 - accuracy: 0.9005\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 372s 139us/step - loss: 0.2665 - accuracy: 0.9018\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 371s 138us/step - loss: 0.2653 - accuracy: 0.9026\n",
      "Epoch 17/20\n",
      "2460064/2685136 [==========================>...] - ETA: 32s - loss: 0.2618 - accuracy: 0.9039"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-94bfd8b5f534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# fit the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhot_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgrid_search_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhot_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# best params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Score \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"with \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    807\u001b[0m                                    (split_idx, (train, test)) in product(\n\u001b[1;32m    808\u001b[0m                                    \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                                    enumerate(cv.split(X, y, groups))))\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "tuned_model = KerasClassifier(build_fn = simpleNN, epochs=20, verbose = 1)\n",
    "# define hyperparameter options\n",
    "num_hidden_layers = [1, 2, 3, 4]\n",
    "num_neurons = [100, 250, 500]\n",
    "param_grid = dict(num_hidden_layers = num_hidden_layers, num_neurons = num_neurons)\n",
    "# prepare the grid\n",
    "grid = GridSearchCV(estimator = tuned_model, param_grid = param_grid, cv=3)\n",
    "# fit the grid\n",
    "hot_y_train = np_utils.to_categorical(y_train)\n",
    "grid_search_result = grid.fit(X_train, hot_y_train)\n",
    "# best params\n",
    "print(\"Best Score \" + str(grid_search_result.best_score_) + \"with \" + str(grid_search_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "intense-mixture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4027704\n",
      "4027704\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "partial-establishment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3020778 samples, validate on 1006926 samples\n",
      "Epoch 1/20\n",
      "3020778/3020778 [==============================] - 107s 35us/step - loss: 0.4084 - accuracy: 0.8340 - val_loss: 0.3540 - val_accuracy: 0.8596\n",
      "Epoch 2/20\n",
      "3020778/3020778 [==============================] - 106s 35us/step - loss: 0.3329 - accuracy: 0.8684 - val_loss: 0.3161 - val_accuracy: 0.8772\n",
      "Epoch 3/20\n",
      "3020778/3020778 [==============================] - 105s 35us/step - loss: 0.3140 - accuracy: 0.8771 - val_loss: 0.3177 - val_accuracy: 0.8779\n",
      "Epoch 4/20\n",
      "3020778/3020778 [==============================] - 109s 36us/step - loss: 0.3053 - accuracy: 0.8814 - val_loss: 0.2986 - val_accuracy: 0.8839\n",
      "Epoch 5/20\n",
      "3020778/3020778 [==============================] - 105s 35us/step - loss: 0.3020 - accuracy: 0.8832 - val_loss: 0.3074 - val_accuracy: 0.8834\n",
      "Epoch 6/20\n",
      "3020778/3020778 [==============================] - 106s 35us/step - loss: 0.2978 - accuracy: 0.8854 - val_loss: 0.2994 - val_accuracy: 0.8853\n",
      "Epoch 7/20\n",
      "3020778/3020778 [==============================] - 107s 35us/step - loss: 0.2959 - accuracy: 0.8869 - val_loss: 0.3092 - val_accuracy: 0.8819\n",
      "Epoch 8/20\n",
      "3020778/3020778 [==============================] - 108s 36us/step - loss: 0.2930 - accuracy: 0.8885 - val_loss: 0.3087 - val_accuracy: 0.8835\n",
      "Epoch 9/20\n",
      "3020778/3020778 [==============================] - 106s 35us/step - loss: 0.2882 - accuracy: 0.8909 - val_loss: 0.2901 - val_accuracy: 0.8916\n",
      "Epoch 10/20\n",
      "3020778/3020778 [==============================] - 113s 37us/step - loss: 0.2865 - accuracy: 0.8918 - val_loss: 0.2817 - val_accuracy: 0.8954\n",
      "Epoch 11/20\n",
      "3020778/3020778 [==============================] - 102s 34us/step - loss: 0.2829 - accuracy: 0.8936 - val_loss: 0.3264 - val_accuracy: 0.8783\n",
      "Epoch 12/20\n",
      "3020778/3020778 [==============================] - 105s 35us/step - loss: 0.2801 - accuracy: 0.8948 - val_loss: 0.2843 - val_accuracy: 0.8919\n",
      "Epoch 13/20\n",
      "3020778/3020778 [==============================] - 106s 35us/step - loss: 0.2784 - accuracy: 0.8961 - val_loss: 0.2873 - val_accuracy: 0.8961\n",
      "Epoch 14/20\n",
      "3020778/3020778 [==============================] - 107s 36us/step - loss: 0.2756 - accuracy: 0.8970 - val_loss: 0.2805 - val_accuracy: 0.8974\n",
      "Epoch 15/20\n",
      "3020778/3020778 [==============================] - 111s 37us/step - loss: 0.2735 - accuracy: 0.8982 - val_loss: 0.2682 - val_accuracy: 0.8996\n",
      "Epoch 16/20\n",
      "3020778/3020778 [==============================] - 107s 35us/step - loss: 0.2722 - accuracy: 0.8988 - val_loss: 0.2723 - val_accuracy: 0.8991\n",
      "Epoch 17/20\n",
      "3020778/3020778 [==============================] - 107s 35us/step - loss: 0.2709 - accuracy: 0.8992 - val_loss: 0.2849 - val_accuracy: 0.8917\n",
      "Epoch 18/20\n",
      "3020778/3020778 [==============================] - 108s 36us/step - loss: 0.2694 - accuracy: 0.8998 - val_loss: 0.3018 - val_accuracy: 0.8846\n",
      "Epoch 19/20\n",
      "3020778/3020778 [==============================] - 107s 35us/step - loss: 0.2689 - accuracy: 0.9004 - val_loss: 0.2729 - val_accuracy: 0.9015\n",
      "Epoch 20/20\n",
      "3020778/3020778 [==============================] - 105s 35us/step - loss: 0.2712 - accuracy: 0.9010 - val_loss: 0.2658 - val_accuracy: 0.8999\n"
     ]
    }
   ],
   "source": [
    "# Training and Testing after Oversampling\n",
    "# split into train and validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.25)\n",
    "hot_y_train = np_utils.to_categorical(y_tr)\n",
    "hot_y_val = np_utils.to_categorical(y_val)\n",
    "# create the model\n",
    "# base_model = simpleNN(grid_search_result.best_params_['num_hidden_layers'],grid_search_result.best_params_['num_neurons']) # columns-2 features and 4 possible classes\n",
    "base_model = simpleNN(2, 100)\n",
    "# fit the keras model on the dataset\n",
    "hist = base_model.fit(X_tr, hot_y_train, batch_size=32, epochs=20, validation_data=(X_val, hot_y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "former-aging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3dd3hUZfbA8e8h9CZVUUABhSBINcDaARVBJVh3wQayyopiXXsXdXdFVl1XdK1r+SnIuquioriCBXUtQSkiIEVUUHqXnpzfH+cODGGSTJK5M5PkfJ5nnszc+957TyblzFvu+4qq4pxzzuVXKdUBOOecS0+eIJxzzsXkCcI551xMniCcc87F5AnCOedcTJ4gnHPOxeQJwjnnXEyeIJwrARFZLCInpDoO58LkCcI551xMniCcSxARqSYiD4nIz8HjIRGpFuxrJCJvisg6EVkjIlNFpFKw7wYRWSoiG0Vknogcn9rvxDlTOdUBOFeO3AL8BugMKPA6cCtwG/BHYAnQOCj7G0BFJBMYAXRT1Z9FpAWQkdywnYvNaxDOJc65wEhVXaGqK4G7gPODfTuA/YGDVHWHqk5VmwgtF6gGtBORKqq6WFUXpiR65/LxBOFc4hwA/BD1+odgG8D9wALgXRFZJCI3AqjqAuAq4E5ghYiME5EDcC4NeIJwLnF+Bg6Ken1gsA1V3aiqf1TVVkA2cE2kr0FVX1LVo4NjFbgvuWE7F5snCOdKroqIVI88gLHArSLSWEQaAbcD/wcgIqeKyCEiIsB6rGkpT0QyRaR30Jm9FdgC5KXm23FuT54gnCu5idg/9MijOpADzARmAV8B9wRlWwPvAZuA/wGPqur7WP/DX4BVwDJgX+Cm5H0LzhVMfMEg55xzsXgNwjnnXEyeIJxzzsXkCcI551xMniCcc87FVG6m2mjUqJG2aNEi1WE451yZMm3atFWq2jjWvnKTIFq0aEFOTk6qw3DOuTJFRH4oaJ83MTnnnIvJE4RzzrmYPEE455yLqdz0QTjnypcdO3awZMkStm7dmupQyoXq1avTrFkzqlSpEvcxniCcc2lpyZIl1KlThxYtWmBzHLqSUlVWr17NkiVLaNmyZdzHeROTcy4tbd26lYYNG3pySAARoWHDhsWujXmCcM6lLU8OiVOS97LCJ4i1a2HkSPBbKJxzbk8VPkFkZMAdd8DkyamOxDmXTlavXk3nzp3p3LkzTZo0oWnTprteb9++vdBjc3JyuOKKK5IUaXgqfCd13brQpAnMnZvqSJxz6aRhw4ZMnz4dgDvvvJPatWtz7bXX7tq/c+dOKleO/S80KyuLrKyspMQZpgpfgwDIzIR581IdhXMu3Q0ZMoRLLrmEHj16cP311/PFF19wxBFH0KVLF4488kjmBf9IPvjgA0499VTAksvQoUPp2bMnrVq14uGHH07lt1AsFb4GAZYgXnkl1VE45wpy1VUQfJhPmM6d4aGHin/ckiVL+PTTT8nIyGDDhg1MnTqVypUr895773HzzTfz73//e69j5s6dy/vvv8/GjRvJzMxk+PDhxbofIVU8QWAJYs0aWLUKGjVKdTTOuXR29tlnk5GRAcD69esZPHgw8+fPR0TYsWNHzGNOOeUUqlWrRrVq1dh3331Zvnw5zZo1S2bYJeIJAmjb1r7Om+cJwrl0VJJP+mGpVavWrue33XYbvXr14tVXX2Xx4sX07Nkz5jHVqlXb9TwjI4OdO3eGHWZCeB8EVoMA74dwzhXP+vXradq0KQDPPvtsaoMJgScIoEULqFrVRzI554rn+uuv56abbqJLly5lplZQHKKqqY4hIbKysrQ0Cwa1bw+HHAKvv57AoJxzJTZnzhwOPfTQVIdRrsR6T0VkmqrGHJMbag1CRPqKyDwRWSAiNxZS7kwRURHJitp2U3DcPBE5Kcw4wYe6OudcfqElCBHJAMYA/YB2wCARaRejXB3gSuDzqG3tgIFAe6Av8GhwvtC0bQsLF0IBgxCcc67CCbMG0R1YoKqLVHU7MA4YEKPc3cB9QPQ0gwOAcaq6TVW/BxYE5wtNZibs3Anffx/mVZxzruwIM0E0BX6Ker0k2LaLiHQFmqvqW8U9Njh+mIjkiEjOypUrSxVsZCSTd1Q755xJ2SgmEakEPAD8saTnUNUnVDVLVbMaN25cqnh8qKtzzu0pzBvllgLNo143C7ZF1AEOAz4I5ilvAkwQkew4jk24+vWhcWNPEM45FxFmDeJLoLWItBSRqlin84TITlVdr6qNVLWFqrYAPgOyVTUnKDdQRKqJSEugNfBFiLECPpLJObdbr169mDRp0h7bHnroIYYPHx6zfM+ePYkMtT/55JNZt27dXmXuvPNORo8eXeh1X3vtNb799ttdr2+//Xbee++94oafEKElCFXdCYwAJgFzgPGqOltERga1hMKOnQ2MB74F3gEuU9XcsGKNaNvWE4RzzgwaNIhx48btsW3cuHEMGjSoyGMnTpxIvXr1SnTd/Ali5MiRnHDCCSU6V2mF2gehqhNVtY2qHqyq9wbbblfVCTHK9gxqD5HX9wbHZarq22HGGZGZCStX2sR9zrmK7ayzzuKtt97atTjQ4sWL+fnnnxk7dixZWVm0b9+eO+64I+axLVq0YNWqVQDce++9tGnThqOPPnrXdOAATz75JN26daNTp06ceeaZbN68mU8//ZQJEyZw3XXX0blzZxYuXMiQIUN4JZhuevLkyXTp0oUOHTowdOhQtm3btut6d9xxB127dqVDhw7MTdBoG5+sL0p0R/URR6Q2FudclBTM992gQQO6d+/O22+/zYABAxg3bhy//e1vufnmm2nQoAG5ubkcf/zxzJw5k44dO8Y8x7Rp0xg3bhzTp09n586ddO3alcMPPxyAM844g4svvhiAW2+9laeffprLL7+c7OxsTj31VM4666w9zrV161aGDBnC5MmTadOmDRdccAGPPfYYV111FQCNGjXiq6++4tFHH2X06NE89dRTpX6LfC6mKD6SyTkXLbqZKdK8NH78eLp27UqXLl2YPXv2Hs1B+U2dOpXTTz+dmjVrUrduXbKzd7euf/PNNxxzzDF06NCBF198kdmzZxcay7x582jZsiVt2rQBYPDgwXz00Ue79p9xxhkAHH744SxevLik3/IevAYRpWVLqFzZE4RzaSdF830PGDCAq6++mq+++orNmzfToEEDRo8ezZdffkn9+vUZMmQIW7duLfpEMQwZMoTXXnuNTp068eyzz/LBBx+UKtbIlOKJnE7caxBRqlSxCfs8QTjnAGrXrk2vXr0YOnQogwYNYsOGDdSqVYt99tmH5cuX8/bbhXePHnvssbz22mts2bKFjRs38sYbb+zat3HjRvbff3927NjBiy++uGt7nTp12Lhx417nyszMZPHixSxYsACAF154geOOOy5B32lsniDy8aGuzrlogwYNYsaMGQwaNIhOnTrRpUsX2rZtyznnnMNRRx1V6LFdu3bld7/7HZ06daJfv35069Zt1767776bHj16cNRRR9E2smoZMHDgQO6//366dOnCwoULd22vXr06//znPzn77LPp0KEDlSpV4pJLLkn8NxzFp/vO54Yb4MEHYfNma25yzqWGT/edeGk13XdZlJlpM7omqI/HOefKLE8Q+fhIJuecM54g8vEE4Vz6KC9N4OmgJO+lJ4h8GjWChg09QTiXatWrV2f16tWeJBJAVVm9ejXVq1cv1nHeDRuDj2RyLvWaNWvGkiVLKO1aL85Ur16dZs2aFesYTxAxZGbCxImpjsK5iq1KlSq0bNky1WFUaN7EFENmJixfDuvXpzoS55xLHU8QMXhHtXPOeYKIKXJToycI51xF5gkihlatICPDE4RzrmLzBBFD1aqWJBK05oZzzpVJniAK4ENdnXMVXagJQkT6isg8EVkgIjfG2H+JiMwSkeki8rGItAu2VxGR54J9c0TkpjDjjCUzE+bPh9zQV8J2zrn0FFqCEJEMYAzQD2gHDIokgCgvqWoHVe0MjAIeCLafDVRT1Q7A4cAfRKRFWLHG0rYtbNsGP/6YzKs651z6CLMG0R1YoKqLVHU7MA4YEF1AVTdEvawFRO6pV6CWiFQGagDbgeiyofOhrs65ii7MBNEU+Cnq9ZJg2x5E5DIRWYjVIK4INr8C/Ar8AvwIjFbVNSHGupdIgvCOaudcRZXyTmpVHaOqBwM3ALcGm7sDucABQEvgjyLSKv+xIjJMRHJEJCfR87U0bgz16nkNwjlXcYWZIJYCzaNeNwu2FWQccFrw/BzgHVXdoaorgE+AvVY8UtUnVDVLVbMaN26coLCNiI9kcs5VbGEmiC+B1iLSUkSqAgOBCdEFRKR11MtTgPnB8x+B3kGZWsBvgKQ39niCcM5VZKElCFXdCYwAJgFzgPGqOltERopIdlBshIjMFpHpwDXA4GD7GKC2iMzGEs0/VXVmWLEWpG1b+Pln2Lgx2Vd2zrnUC3W6b1WdCEzMt+32qOdXFnDcJmyoa0pFOqq/+w4OPzy1sTjnXLKlvJM6nflIJudcReYJohCHHAKVKnk/hHOuYvIEUYhq1aBFC08QzrmKyRNEEdq29QThnKuYPEEUITPTOqnz8lIdiXPOJZcniCJkZsKWLfDTT0WXdc658sQTRBF80j7nXEXlCaIIniCccxWVJ4giNGkCdep4gnDOVTyeIIog4iOZnHMVkyeIOPikfc65isgTRBwyM20U06+/pjoS55xLHk8QcYietM855yoKTxBx8JFMzrmKyBNEHFq3ts5qTxDOuYrEE0QcatSAgw7yBOGcq1g8QcQpM9PXhXDOVSyeIOIUmbRPNdWROOdccniCiFNmpg1zXbo01ZE451xyhJogRKSviMwTkQUicmOM/ZeIyCwRmS4iH4tIu6h9HUXkfyIyOyhTPcxYi9K2rX31fgjnXEURWoIQkQxgDNAPaAcMik4AgZdUtYOqdgZGAQ8Ex1YG/g+4RFXbAz2BHWHFGg8f6uqcq2jCrEF0Bxao6iJV3Q6MAwZEF1DVDVEvawGRFv4+wExVnRGUW62quSHGWqQDDoDatT1BOOcqjjATRFMgepmdJcG2PYjIZSKyEKtBXBFsbgOoiEwSka9E5PpYFxCRYSKSIyI5K1euTHD4+a8Fbdr4SCbnXMWR8k5qVR2jqgcDNwC3BpsrA0cD5wZfTxeR42Mc+4SqZqlqVuPGjUOP1Sftc85VJGEmiKVA86jXzYJtBRkHnBY8XwJ8pKqrVHUzMBHoGkqUxZCZCT/+aEuQOudceRdmgvgSaC0iLUWkKjAQmBBdQERaR708BZgfPJ8EdBCRmkGH9XHAtyHGGpe2be0+iPnziy7rnHNlXeWwTqyqO0VkBPbPPgN4RlVni8hIIEdVJwAjROQEbITSWmBwcOxaEXkASzIKTFTVt8KKNV7RI5k6dkxtLM45F7bQEgSAqk7Emoeit90e9fzKQo79P2yoa9poHdR3vKPaOVcRpLyTuiypVQuaN/eOaudcxeAJAuD772HNmriK+kgm51xF4Qli0SJo1QrGjo2reNu2liB80j7nXHnnCaJVK7sDbsKEostiNYiNG2HZspDjcs65FPMEAZCdDe+/Dxs2FFnU52RyzlUUniDAEsSOHTBpUpFFIwnCRzI558o7TxAARxwBDRvG1czUrJktQeo1COdceecJAqByZTjlFHjrLdi5s9CilSpZl4UnCOdceecJIiI7G9auhU8+KbJoZCSTc86VZ54gIvr0gapV42pmysyExYth69bww3LOuVTxBBFRpw707g2vv17kTQ6ZmZCXBwsWJCk255xLAU8Q0bKzYeFCmDOn0GI+1NU5VxF4gojWv799LaKZqU0b++oJwjlXnnmCiNasGXTtWmSCqFMHmjb1BOGcK988QeQ3YAB89hksX15oMZ+0zzlX3nmCyC872zqp3yp8faJIgvBJ+5xz5ZUniPw6dbJFH4poZsrMhHXrYMWKJMXlnHNJ5gkiPxGrRbz7LmzZUmAxH8nknCvv4koQIlJLRCoFz9uISLaIVAk3tBTKzrbkMHlygUU8QTjnyrt4axAfAdVFpCnwLnA+8GxRB4lIXxGZJyILROTGGPsvEZFZIjJdRD4WkXb59h8oIptE5No440yM446zoUqFNDMdeCBUr+4JwjlXfsWbIERVNwNnAI+q6tlA+0IPEMkAxgD9gHbAoPwJAHhJVTuoamdgFPBAvv0PAG/HGWPiVKsGffvCG2/YLdMxZGRA69aeIJxz5VfcCUJEjgDOBSLDezKKOKY7sEBVF6nqdmAcMCC6gKpGr9BTC9g1JkhETgO+B2bHGWNiZWfbsnE5OQUWycz0dSGcc+VXvAniKuAm4FVVnS0irYD3izimKfBT1OslwbY9iMhlIrIQq0FcEWyrDdwA3FXYBURkmIjkiEjOypUr4/xW4nTyyVZNKKSZKTMTvv8etm9P7KWdcy4dxJUgVPVDVc1W1fuCzupVqnpFIgJQ1TGqejCWEG4NNt8JPKiqm4o49glVzVLVrMaNGycinN0aNICjjy4yQeTm2vRNzjlX3sQ7iuklEakrIrWAb4BvReS6Ig5bCjSPet0s2FaQccBpwfMewCgRWYzVXm4WkRHxxJpQ2dkwa5ZVE2Jo29a+ej+Ec648ireJqV3QX3Aa1mncEhvJVJgvgdYi0lJEqgIDgT0+jotI66iXpwDzAVT1GFVtoaotgIeAP6nqI3HGmjiRyfveeCPmbh/q6pwrz+JNEFWC+x5OAyao6g6iOpRjUdWdwAhgEjAHGB/0X4wUkeyg2AgRmS0i04FrgMEl+i7C0ro1HHpogc1MdetCkyaeIJxz5VPlOMs9DiwGZgAfichBwIZCjwBUdSIwMd+226OeXxnHOe6MM8ZwZGfDX/9q82rUq7fXbh/J5Jwrr+LtpH5YVZuq6slqfgB6hRxbesjOhp074Z13Yu72WV2dc+VVvJ3U+4jIA5EhpSLyV+y+hfKvRw9o3LjAZqa2bWHNGli1KslxOedcyOLtg3gG2Aj8NnhsAP4ZVlBpJSMDTj0VJk6EHTv22u0d1c658ireBHGwqt4R3BW9SFXvAlqFGVhayc6G9eth6tS9dkUSxIwZSY7JOedCFm+C2CIiR0deiMhRQMFzYZc3J55o8zPFaGZq2dKWkPjzn2FTobf1Oedc2RJvgrgEGCMii4Ob1x4B/hBaVOmmVi044QRLEPmWkKtUCR59FJYsgbsKnRjEOefKlnhHMc1Q1U5AR6CjqnYBeocaWboZMMDuqJ6999yBRx4JF10EDz5oN14751x5UKwV5VR1Q9QMrNeEEE/6OvVU+1rAaKa//MVukxg+vMAZwp1zrkwpzZKjkrAoyoL994fu3eH112PubtgQRo2CTz6B555LcmzOOReC0iSIQqfaKJeys+GLL+CXX2LuHjIEjjoKrrsOVq9ObmjOOZdohSYIEdkoIhtiPDYCByQpxvSRHUwh9eabMXdXqgSPPWazcty41wKrzjlXthSaIFS1jqrWjfGoo6rxzuNUfhx2GLRoUegaER06wFVXwVNPwaefJi8055xLtNI0MVU8IlaLeO89+PXXAovdeSc0a2Yd1jt3Ji88V0ZdcAE8/niqo3BuL54giis7G7ZutSRRgNq14W9/g5kz4e9/T2JsruxZvhxeeMHaJp1LM54giuvYY2GffQptZgI4/XRb1vr22+0mOudiej9Y2n3GDFi2LLWxOJePJ4jiqlIF+vWzVeZycwssJmK1h5074eqrkxifK1umTLHRDVBordS5VPAEURLZ2bBypQ15LUSrVnDLLfDKKwUuJ+EquilTrKrZuDFMmpTqaJzbgyeIkujbFypXLrKZCeyeiDZtYMQI2FJxpjd08fjhB1i40Ob5OvFE+O9//TZ8l1Y8QZRE/frWFxFHgqhWzSbzW7jQpuNwbpcpU+xr797Qp491WPtkXi6NhJogRKSviMwTkQUistetYyJyiYjMEpHpIvKxiLQLtp8oItOCfdNEJP0mBszOhm+/hQULiix6/PEwaJAliPnzkxCbKxumTLGmpcMOswQB8O67qY3JuSihJQgRyQDGAP2AdsCgSAKI8pKqdlDVzsAo4IFg+yqgv6p2AAYDL4QVZ4n1729f33gjruJ//StUrw6XXbbXjOGuIlK1BNG7t41o2H9/u8vSE4RLI2HWILoDC4IV6LYD44AB0QWiZoYFW+Nag+1fq+rPwfbZQA0RqRZirMXXqpV98oujmQns7/+ee6yZefz4kGNz6e+77+Dnny1BRPTpY6sWbt6curicixJmgmgK/BT1ekmwbQ8icpmILMRqEFfEOM+ZwFequi3GscNEJEdEclauXJmgsIshO9v+oNesiav4pZdC16427HXDhqLLu3Isuv8hok8f2LYNPvooNTE5l0/KO6lVdYyqHgzcANwavU9E2gP3UcDqdar6hKpmqWpW48aNww82v+xsuxfi7bfjKp6RYTfMLltmN9C5CmzyZGjeHA4+ePe2Y46xUQ3ezOTSRJgJYinQPOp1s2BbQcYBp0VeiEgz4FXgAlVdGEqEpdWtG+y3X9zNTADdW6/l3tNzWPbweH6+/M+2FF3fvv6psSLJy7M7qI8/3vofImrUsNFxniBcmghzRtYvgdYi0hJLDAOBc6ILiEhrVY2M6zkFmB9srwe8Bdyoqp+EGGPpVKpkndUvvwzbt0PVqlajWLLExrUuWmRfo5+vW8dNkeMfAd1vP2T7dhg6FL75xnqyXfk2c6Y1S/aOMTjvpJPg2mth6VJouleLrHNJFVqCUNWdIjICmARkAM+o6mwRGQnkqOoEYISInADsANZiI5YARgCHALeLSKQxpo+qrggr3hLLzra5vXv2hFWrYPFi2LFj9/4qVWyK8FatoEcPa1Jo1Yo3vj2YQbe05K931eYPrafYp8lRo7ztqSKI9D/06rX3vujhrhdemLyYnItBtJyMuczKytKcnJzkX3jLFvtD37nTksDBB+9KAhx8sM37nZGx12GqdtjMmTB3Lux75SB47TWYPduOdeXXqafaDTHz5u29TxUOOMA+cIwdm/TQXMUjItNUNSvWvoq36E+i1agBn31W7MNE7A7rTp3g+uvh2dGjbaW6K66weyukYi35XWHs2AEffgjnnRd7v4jVIt56y/oqKqV8HImrwPy3L4XatbPm5ueeg9e+bAp33WX/GOK8+c6VQTk5sGlT7P6HiD59bFHzr79OXlzOxeAJIsVuu81qEaefDrevvBxt395qEX6zVPlUWP9DxIkn2lcfzeRSzBNEitWsaWtXDx0Kd/+lCldWftRm+fzTn1IdmgvDlCn2iaBRo4LL7LsvdOni03+7lPMEkQZq1oSnn4Znn4WnvjuW8dXPJ2/U/TYdgys/tm6FTz4pvHkpok8f++SwcWP4cTlXAE8QaWTwYFuD6O/NRrFxR3UWnnw5uTvLxygzB/zvfzaVRrwJItKh7VyKeIJIM4cdBm9/3YTXs+7h4IXvcm/Xf7Mi/e7+cCUxebINeT722KLLHnWUjZDzfgiXQp4g0lDt2nD+p8NZ1bwzQ2ddzZEdN5XvmTjKyb04RZoyxaZnqVu36LLVqtm9EJ4gXAr5fRBpSqpUptHLj8KRR3L99rvp1es+7rkHbrihnA2NnzYNzjzTppaoVatkjzp1oF8/e56uNm609sMbboj/mJNOgquuskELBx0UXmzOFcATRDo74ggYOpSLn3+AeX0Gc/PN7Zg6FV54ARo2THVwCfCf/9gNY/vua4t3//rr3o916yx5RG+LNQS4Tx945530vcFw6lSbpyue/oeI6Gk3Lr44nLicK4yqlovH4YcfruXSihWq9etrXq9eOuaRPK1aVbVZM9VPPknQ+XNzVV9+WXXBggSdMA55ear33acKqj16qC5bVrzjc3NVN21SXb5cddEi1T/9yc41dmw48SbCH/+oWrWq6ubN8R+Tl2c/7LPOCi8uV+Fhc+PF/L9anhoryqfGjeFPf0Lef59LG4zj009t0tjjjrNlTEvVfL9mDQwYAL/7HXTsCP/4R/j9Adu32xTnN9xg133/fZsyvTgqVbLmpH33hZYtba6SrCxrjlm3Lpy4S2vKFDjySOt4jldk2o333rPah3NJ5gmiLLj4YvsH+Mc/cnjrDUybZrOMX3stnHYarF1bgnN++aUtbzdpEvzlLzZqZvhwOPlkWwozDGvX2toXzzwDt94KL71UvH+YBcnIgMcfh5Ur4eabS3++RFu9GqZPtxl7i6tPH0t6qZiI0rmCqhZl7VFum5givvhCVUT16qtV1VofHnpItXJl1RYtVJ98UnXJkjjOk5en+sgj1txx4IGqn39u23NzbXuNGqoNGlizUyLNn6/apo1d9/nnE3vuiKuusvfos8/COX9JvfKKNYGVpF1w1Sr7nu66K/FxOaeFNzGl/B97oh7lPkGoqv7hD6oZGaozZ+7a9Nlnqq1b208SVDt0UL3+etUpU1S3bct3/IYNqgMHWsGTT7Z/PvnNnavavbuVGTRIdc2a0sf94YeWdBo2VP3oo9KfryAbNqg2baraqZPqjh3hXae4Lr1UtVYt1e3bS3Z8t26qRx2V2JicC3iCKC9Wr7Z/skcfbTWBQF6e5Yz77lPt1Uu1ShX7ydaurXraaar/+Ifqz+/OUs3MVK1UyTp1c3MLvs6OHaojR1r1pGlT1XffLXnMzz1nAWVmJqcj/N//tm9+9OjwrxWvtm1V+/Ur+fG33GIfDNatS1xMzgU8QZQnTz1lP7bnniuwyIYNqq+9ZhWOAw9UvYBn9Vdq6MqM/fTR376v//2v6tatcVwrJ0f10EPtepddZiOH4pWba//YQLV378TUROKRl6d66qmqNWuq/vBDcq5ZmKVL7T24//6Sn+PDD+0cr76auLicC3iCKE9yc1WPOEJ1331V164tvOzmzZo39PeqoD8e3FPPPuYXrVrVfuq1aqn27686ZoyNFC3sHHr11XZQ69bxte9v3qx69tl2zEUXlbxppaQWL7YEkZ2d3OvG8n//Z+/DV1+V/Bzbtll1cPjwxMXlXCBlCQLoC8wDFgA3xth/CTALmA58DLSL2ndTcNw84KSirlVhEoSq/bOpVEl1xIiCy3z3nbXFg+rNN+9qk9+0SfWNN0+0wOMAABx+SURBVKxC0KqV7uq76NpV9cEHC7klYcoUq45UqqR6660F/9P/5RfrwxCxT81RTWFJNWqUfWOvvZaa60dceKFq/fqFN+nFo39/+4G59DN6tH3SKqNSkiCADGAh0AqoCsyITgBBmbpRz7OBd4Ln7YLy1YCWwXkyCrtehUoQqpYcKlVSnTZt732vvKJap451DL/1VoGnyMuzPPLAA6pZWfbbkJGhesopNohpy5Z8B6xbpzp48O6MMnv2nvtnzrQkUrNm6ptDtm+3HvvmzVU3bkxNDHl59n6ccUbpz/XII/a+J/OGRle0t9+2n0vNmmW2jyhVCeIIYFLU65uAmwopPwh4O1ZZYBJwRGHXq3AJYu1a1f32szuRI59Ot22zoZ5gn+IXLy7WKWfPVr3xRuuXBtV99lG9+GIbeLRHReA//1Ft1Ei1WjXLLrm5qhMnWlI64IDYSSsVPv3UajLXXJOa6y9caG/kI4+U/lzffWfnevTR0p/LJcaaNfb7fsAB9rMpo7WIVCWIs4Cnol6fDzwSo9xlQQ3hJ6B1sO0R4LyoMk8DZxV2vQqXIFTtfgKwjusff7S+CVC9/PIYY1zjt3On6nvvqV5wgfVVgGrLlqq33263M6iqtUX17287O3e22kznzqo//ZSY7y1RIkODv/46+dd+8kl7f779tvTnysuzG15OO63053KJcc45NtJv2jTVLl1UO3ZMXZNqKaR1gojafw7wnBYjQQDDgBwg58ADDwzp7UtjeXmqxxyz+x6D2rVVx49P6CU2brQ8dOKJ9mEcVI88UvWxx1RXr8pTffppqzkMGJC6ppzCrFljHfrdu1vmS6ZBg1T33z9x/zSGDVOtWzf5nf5ub//6l/0xRG5gfOwxex258bQMKStNTJWA9bHKehNTIWbOtE8xHTqozpsX6qV++snutWjXzn5zqlZVPfNM1Tde2arr16XxJ6cXX9SkNwHk5VkT4LnnJu6ckTuyP/44ced0xbdsmX0gy8ranazXr7fq9u9/n9rYSiBVCaIysCjoZI50UrfPV6Z11PP+kUCB9vk6qRd5J3Uhvv8+Ro9yePLyrFZ95ZWqjRvrrpFQTZtaTeOKK+wD1Ycf2mS0KZeXp3rCCfbp++efk3PNb76xN+XppxN3zrVrrSnvttsSd05XPHl5Nny6WrW9B2lcdFGZ7KwuLEGI7Q+HiJwMPISNaHpGVe8VkZFBQBNE5G/ACcAOYC0wQlVnB8feAgwFdgJXqerbhV0rKytLc3xCs6TbscMmKv3qK/j2W5gzB+bOtWUbIho1gkMPtUe7dru/Nm2axOUbFiyw9VxPOw3GjQv/en//O1xxBXz/PbRokbjzHnkk5OXBZ58l7pwufs89B0OG2FTK11yz574vv4Tu3eHRR23iyzJCRKapalbMfWEmiGTyBJE+8vLgp58sWcyZsztxfPvtnjPP1qkDbdtasujeHXr1stehJY2774bbb7eFhU46KaSLBE4/HWbMgEWLEnveO++072PlSmjQILHndoX78Ufo0AE6d7Zp6vMv7ahqMySrwtdfp+/iVfl4gnBpQRVWrNg7ccyeDcuWWZkmTWwp5l697HHIIQn8O9u2DTp1smrPN98kZqrxWHJzrdp05pnw1FOJPfenn9rU7P/6F5x1VmLP7QqWl2cfKv73P5g5E1q1il3uH/+w2sPnn9unnjKgsAQRWh9Esh8Vug+ijMvLs+GzTzxhA3+aNNmzX+O886wpv9ApQeI1ZYruurs8LDk5do2XXkr8uXfssL6Uiy9O/LldwSI3Kj7+eOHl1q+3fogy1FlNqvogkslrEOWHKsybZ7X499+HDz6wFhWAgw7aXbvo1QuaNy/BBQYPhrFjbRGfdu0SGbq5/35b5e6XX6xKlGhnnAHTpsHixWWmGaNMmz/fap7HHQcTJxb9nl90kf1+/fIL1K2bnBhLobAahK8o59KOiPVFDB8O48fD8uUwaxY8/LA18U6YYP/jDzzQmqAuvhjefddaAeIyerR1gFxySTEOKobJky3xhJEcwJo6fvwRvvsunPO73XJz7ZetWjVrLownIf/hD7B5s62YWMZ5gnBpT8QGIF1+OfznP1ab+PpreOAB+z88frz9z8zMtMEla9YUccLGjWHUKJg61UalJNL27Xbe3r0Te95offrY13ffDe8azowebf0OY8bYsLt4ZGVZR/bjj4e/xnvIPEG4MqdSJfv7u/pqq02sWAEvvmgf2K+91v6OL7zQRh0W6MIL4eij4brrYNWqxAX3xRf26THMBNGypVWdPEGEa9YsG/V21lkwaFD8x4nAsGHWhFnGm709Qbgyr1o1OOcc++A+Y4YNU3/lFRtE0q0bPPOM/c/eQ6VKNuJk/XpLEokyZYr9g+jZM3HnjKVPH+ug2b493OtUVNu3wwUXQL16dl9Dcft6zjkHataEJ54IJ74k8QThypWOHeGxx2DpUmsV2LIFfv97q1Vcc02+Zvv27S05PPssTJqUmACmTLGOkvr1E3O+gvTpY3cjfvppuNepqO6+22oATz5pTZLFtc8+VusYOxY2bEh8fEniCcKVS3XrwqWXWivBhx9aH8Xf/279FH36wKuvws6dwK23Wo/4qafCn/9snZIltXmztVeH2bwU0asXVK7szUxh+OIL+10YMgSys0t+nmHDLImX4c5qTxCuXBOBY4+12TV++gnuucemAjnjDJsB4+6/1mTZq/+zm9puvhlOPNGqHyXxySfWNJGMBFG3LhxxhCeIRNuyxUYtHXAAPPRQ6c7VrZsNjy3DndWeIFyF0aQJ3HKLzX7x2mvWwnT77dC8Qz26LRjLP7o/w7aPv2BbZkfm3vc6q1YV8+96yhT7VH/00aF9D3vo08cmwYrcJOJK7+ab7RPEP/9pzUSlIWJDXqdPt/tWyiC/Uc5VaPPnw9NP2//ZBQug6uLveFEHcThfMYZLubvuaJq3qcEhh0Dr1uzxtVGjfH2XPXpA1arWW54MX3xh1xw7FgYOLP7xa9faXYgffWRtbxddZAmuovrgA2u6GzHC2iMTYf16q42cc471Z6Qhn2rDuTht3ao6b+ZWXXDaH1VBlzQ4TC8+Ypa2bGkzbUemAIksyXr44aoDB6r+4751mlepki27lyw7d9piUUOGxFf+119VJ01Svf56CzyyAlTVqva1Y0dbX7Yi2rDBVuw75BDVTZsSe+6hQ22tiPXrE3veBKGQqTa8icm5KNWqQZsO1Tj41dHwzjs0rbKSJ77uxqJrH2XLZmXuXHjzTXjwQTjvPGjY0Pql37zhIyQvj2HjenPvvTYBYeiV84wMOOEE64eIdbEdO6xfZORImyaiXj3rrX/wQRuCeccdVtvZuNHGBa9bZx02550HP/8ccvBp5ppr7O7055+HWrUSe+5IZ/XYsYk9bzIUlDnK2sNrEC4Uy5ap9utnn7Czs1VXroxZbPUFV+r2ytX1mO5bd9UwWrdWve461U8+Uc3NDSm+p56yi33zjV3k669VR49WPflkW4IWrKbQtasF8847BX9C/vVXW4yoWjU7dtSoUq1tXma89Za9TzfeGM758/JUO3Wyn0EaIhUryiX74QnChSY3V/XBB60p5oADbEbY/Dp0sFXrVHXpUltRr08fWw0WbPXRYcNU337bmrES5ocf7AKdOqk2arS7/SszU/XSS22Z0lWrinfOBQtU+/fffZ53301gwGlm9mxrpuvQIcE/mHzGjLH388svw7tGCXmCcC4RvvrK/mGKqN500+71iJcvtz+lP/1pr0PWrrUlsc8+e/cH+rp1rd9i3LgENUsfe6zNi37BBarPPWeLhyfCW29ZmzyonnGG6uLFiTlvuli82N63/fdP0FzyhVi3zqYBT8Np2j1BOJcomzbZ2sOg2r27fdp++WV7/fnnhR66ZYvqm2/a4ZG1vKtWtRashx9WnT69FE1ReXklPLAIW7da4qtZU7V6ddWRI5O6/nloVqxQbdNGtV491Zkzk3PNSGf1hg3JuV6cPEE4l2jjx9swpjp1bERQ3bq2mE+cdu5UnTpV9ZprVFu10l0tQ/XqWevO/fdbvinGKcP144+qv/2tBdmqlerrr4eXlMK2YYP9zGrUUP344+Rd97PPNK5Fh5IsZQkC6AvMAxYAN8bYfw3wLTATmAwcFLVvFDAbmAM8THDPRkEPTxAu6RYvVj3qKPsz6t+/1Kd6/nmrXbRpszth1K5tfRn33msJJcxm8rhMnqzarp0F16+f6nffpTigYtqyRbV3b9WMDKvOJVNeng0lTrPO6pQkCCADWAi0AqoCM4B2+cr0AmoGz4cDLwfPjwQ+Cc6RAfwP6FnY9TxBuJTYscPWSk1wM8XPP1vL1WWXqR522O6EUb26as+edrvFe+/ZwKOk275d9YEHrPZUtar1xyT63oEw7NxpfSmg+sILqYkhsnRpTk5qrh9DYQkitDupReQI4E5VPSl4fROAqv65gPJdgEdU9ajg2EeAowEBPgLOV9U5BV3P76R25dnq1fDxxzbx4Ecf2YJJeXl243O3bjaBbOvWux8tWkCVKiEHtWwZ3HijLbrUtatNNVLa6SnComr3Izz1lM2xdOWVqYlj/XrYf384/3yboykNFHYndZgJ4iygr6peFLw+H+ihqiMKKP8IsExV7wlejwYuwhLEI6p6S4xjhgHDAA488MDDf/jhh1C+F+fSzYYNNtN3JGF8882es0pnZFiSiE4akcdBByV4Ro0337TZD3v0sGnTa9ZM4MkT5OabbYbWW26xGRtTaehQ+Ne/7GbEOnVSGwuFJ4i0mHhFRM4DsoDjgteHAIcCzYIi/xWRY1R1j0luVPUJ4AmwGkTyInYuterWhb597QH2AXnlSptbKvJYsMC+fvwxbNq0+9gqVWxRukjCOOQQqwBkZZWw1nHqqbak38CBlihef91uSU8XDz5oyeEPf7B1HlJt2DCbDHDsWHuexsJMEEuB5lGvmwXb9iAiJwC3AMep6rZg8+nAZ6q6KSjzNnAEkKRZ0JwrW0Rg333tcdRRe+5TheXL90wekcf77+9eba9WLTv2uONsQbysLJt7MC5nn21Tdvz+93DuuTa/ejpM/Pf88zaNxlln2QpSxV0ZLgw9ekCHDrbaXJoniDA7qSsDi4CW7O6kbp+vTBesI7t1vu2/A94LzlEFG+HUv7DreSe1c8WXl2f31b3yiuqIEXt2iNesqXriiar33GOjQeOadePBB+3gIUNCnF8kThMm2Gil449Pg+Ff+aRRZzWp6KQGEJGTgYewkUjPqOq9IjIyCGiCiLwHdAB+CQ75UVWzRSQDeBQ4FlDgHVW9prBreSe1c4mxapX1a3zwgT1mzbLtNWrAkUda7aJnT+scj9mSNHKkTQR4xRXWIZyKT+1Tp9p6GYcdZp3nadDWv4d162wa8DTorE5JJ3WyeYJwLhyrVtn/20jCmDnTtteoYYva9ewJv/mNtZrstx8ICtdeCw88ALfdZgkjmWbMsHayJk2sA6ZRo+ReP14XXmiz6Ka4s9oThHMuYVav3p0wPvzQ/h9H/o00bmyJosNhyvCvh5E59Sm233s/VW++NjnBLVxoHSlVqthU5wcemJzrlsRnn1mGfeIJuPjilIXhCcI5F5q1a21VzZkzrTlq5kxbD2Pr5lxe5FwG8jK37fs4c44ZRocO0LGjJZFWraBSIlek+eUXW+513TqrORx6aAJPHgJVW7O6WjX48suUhZH2w1ydc2VX/fq2UmevXru35ebCokUZfPP1C3x36ybumn8Jf/ykDnf9Z9Cu2katWrYueMeONty2WTN7NG9uzfPFGim7bp2N+V2+HCZPTv/kANY3M2wYXH65rXnbtWuqI9qL1yCcc+HasgVOPhk+/pitL/2HWS3676ptRGocq1btfdi+++5OGrEeTZsG9+Rt3mwr5X3+ud2016dP0r/FEot0Vp90kt1LkoKbDL2JyTmXWhs3wvHHWzaYOBF6995r95IlhT/WrNnzlPVYy+Car3CxPs6hW77i6RPGsaLnb2nefHdNpFkz60xPa3fdBXfeaXcvPvKIJdMk8gThnEu91attyNP331szUI8exTr811/h50Vb2frKm+zz5oscMGMilXO381Pttjza8Dae3HQOq1fvfVyjRuyRNCKPyOumTdPgxu8PPoDhw2HuXDjzTBse3KxZkYf9+qv1/2zZYsuTl4QnCOdcevjlFzjmGKsOfPCBdUAUJTfXhku9+KINC92wwSa8GzgQzjsPunTZda/Fli1W2/jpJ3tEP4+8Xrt270vsvz8cfHDsR8OGSbqVY/t2GD3apgOpXNmGB19++a470rdssRFjOTm7H3Pm2KSNnTvbBI4l4QnCOZc+Fi+2JLFjh42Xbd167zKq9tH4xRdtzqLIvQJnnmlTefTqZTMSlsCmTbubrSKJ4/vvYdEiGyW7NN+EQHXr2oirWMmjefMSh1Gw778n99IRZLwzkVXNOvF4538w/sffMHu25Uqw/pmsLHscfrg9mjYt2eU8QTjn0svcuXDssdZBMHXq7vsVFi+Gl16yxPDtt3Y/Q79+lhT6909Kh8KWLZYwFi7c+/H995bXIqpUsVlzmzSxUVkFPWrXLnz/8uVWI5g2zb7Omqn0z32Vv3ElTVnKO80u5uvf/pn2xzQgK8uSQaJqNZ4gnHPp5+uvrSaw337WlDJunN3cBlbDOPdcm2SvYcPUxhklN9dqHgsX7q5xLFwIK1ZYf0D+x9atxTt//fp71gy6td1I86fvRB7+GzRoYE1Q55+f0DYvTxDOufT06adw4ok2VLV9e0sK55xji1aUA7m59q3FSh7Rj3r1LCm0aFHA//4ZM+CSS+zu6+OOg8ceS9i9Hp4gnHPpa84ca7fp0CE9puNOV3l58PTTcMMN1pFy7bVw662lvneisASRyBvdnXOu+A491EYzeXIoXKVKNmfT3LlWy/rzn63W9dZb4V0ytDM755xLvH33hWeftWHCNWrYin6/+53VMBLM52Jyzrmy6LjjbCjwX/9qTU4JnfnQeIJwzrmyqmpVuOmm0E7vTUzOOedi8gThnHMuJk8QzjnnYgo1QYhIXxGZJyILROTGGPuvEZFvRWSmiEwWkYOi9h0oIu+KyJygTIswY3XOOben0BKEiGQAY4B+QDtgkIi0y1fsayBLVTsCrwCjovY9D9yvqocC3YEVYcXqnHNub2HWILoDC1R1kapuB8YBA6ILqOr7qro5ePkZ0AwgSCSVVfW/QblNUeWcc84lQZgJoinwU9TrJcG2gvweeDt43gZYJyL/EZGvReT+oEayBxEZJiI5IpKzcuXKhAXunHMuTTqpReQ8IAu4P9hUGTgGuBboBrQChuQ/TlWfUNUsVc1q3LhxkqJ1zrmKIcwb5ZYCzaNeNwu27UFETgBuAY5T1W3B5iXAdFVdFJR5DfgN8HRBF5s2bdoqEfmhFPE2AmIsnZ42PL7S8fhKx+MrnXSOr8Cpc8NMEF8CrUWkJZYYBgLnRBcQkS7A40BfVV2R79h6ItJYVVcCvYFCp2pV1VJVIUQkp6AZDdOBx1c6Hl/peHylk+7xFSS0JiZV3QmMACYBc4DxqjpbREaKSHZQ7H6gNvAvEZkuIhOCY3Ox5qXJIjILEODJsGJ1zjm3t1DnYlLVicDEfNtuj3p+QiHH/heIY0Vz55xzYUiLTuo08USqAyiCx1c6Hl/peHylk+7xxVRuVpRzzjmXWF6DcM45F5MnCOecczFVqAQRx+SB1UTk5WD/58mcIFBEmovI+8HEhLNF5MoYZXqKyPpgxNd0Ebk91rlCjnOxiMwKrr/X0GMxDwfv4UwR6ZrE2DKj3pvpIrJBRK7KVyap76GIPCMiK0Tkm6htDUTkvyIyP/hav4BjBwdl5ovI4CTGd7+IzA1+fq+KSL0Cji30dyHE+O4UkaVRP8OTCzi20L/3EON7OSq2xSIyvYBjQ3//Sk1VK8QDyAAWYndlVwVmAO3ylbkU+EfwfCDwchLj2x/oGjyvA3wXI76ewJspfh8XA40K2X8yNmWKYDc3fp7Cn/cy4KBUvofAsUBX4JuobaOAG4PnNwL3xTiuAbAo+Fo/eF4/SfH1weZCA7gvVnzx/C6EGN+dwLVx/PwL/XsPK758+/8K3J6q96+0j4pUgyhy8sDg9XPB81eA40VEkhGcqv6iql8Fzzdi944UNndVuhoAPK/mM+yGx/1TEMfxwEJVLc3d9aWmqh8Ba/Jtjv49ew44LcahJwH/VdU1qroW+C/QNxnxqeq7avcxQdQkmqlQwPsXj3j+3kutsPiC/x2/BcYm+rrJUpESRDyTB+4qE/yBrAcaJiW6KEHTVhfg8xi7jxCRGSLytoi0T2pgRoF3RWSaiAyLsb+4kzSGZSAF/2Gm+j3cT1V/CZ4vA/aLUSZd3seh7J5EM7+ifhfCNCJoAnumgCa6dHj/jgGWq+r8Avan8v2LS0VKEGWCiNQG/g1cpaob8u3+Cmsy6QT8HXgt2fEBR6tqV2ydj8tE5NgUxFAoEakKZAP/irE7Hd7DXdTaGtJyrLmI3ALsBF4soEiqfhceAw4GOgO/YM046WgQhdce0v5vqSIliHgmD9xVRkQqA/sAq5MSnV2zCpYcXlTV/+Tfr6obVHVT8HwiUEVEGiUrvuC6S4OvK4BXsap8tLgmaQxZP+ArVV2ef0c6vIfA8kizW/A11mJYKX0fRWQIcCpwbpDE9hLH70IoVHW5quaqah42BU+s66b6/asMnAG8XFCZVL1/xVGREsSuyQODT5gDgQn5ykwAIqNFzgKmFPTHkWhBe+XTwBxVfaCAMk0ifSIi0h37+SUzgdUSkTqR51hn5jf5ik0ALghGM/0GWB/VnJIsBX5yS/V7GIj+PRsMvB6jzCSgj4jUD5pQ+gTbQicifYHrgWwtYKGuOH8Xwoovuk/r9AKuG8/fe5hOAOaq6pJYO1P5/hVLqnvJk/nARth8h41uuCXYNhL7QwCojjVLLAC+AFolMbajsaaGmcD04HEycAlwSVBmBDAbG5HxGXBkkt+/VsG1ZwRxRN7D6BgFW2p2ITALW1I2mTHWwv7h7xO1LWXvIZaofgF2YO3gv8f6tSYD84H3gAZB2Szgqahjhwa/iwuAC5MY3wKs/T7yexgZ2XcAMLGw34UkxfdC8Ls1E/unv3/++ILXe/29JyO+YPuzkd+5qLJJf/9K+/CpNpxzzsVUkZqYnHPOFYMnCOecczF5gnDOOReTJwjnnHMxeYJwzjkXkycI54pBRHLzzRibsFlCRaRF9KygzqVaqGtSO1cObVHVzqkOwrlk8BqEcwkQzO0/Kpjf/wsROSTY3kJEpgQTy00WkQOD7fsFay3MCB5HBqfKEJEnxdYEeVdEaqTsm3IVnicI54qnRr4mpt9F7Vuvqh2AR4CHgm1/B55T1Y7YpHcPB9sfBj5UmzSwK3Y3LUBrYIyqtgfWAWeG/P04VyC/k9q5YhCRTapaO8b2xUBvVV0UTLq4TFUbisgqbCqIHcH2X1S1kYisBJqp6raoc7TA1oBoHby+AaiiqveE/505tzevQTiXOFrA8+LYFvU8F+8ndCnkCcK5xPld1Nf/Bc8/xWYSBTgXmBo8nwwMBxCRDBHZJ1lBOhcv/3TiXPHUyLcI/TuqGhnqWl9EZmK1gEHBtsuBf4rIdcBK4MJg+5XAEyLye6ymMBybFdS5tOF9EM4lQNAHkaWqq1Idi3OJ4k1MzjnnYvIahHPOuZi8BuGccy4mTxDOOedi8gThnHMuJk8QzjnnYvIE4ZxzLqb/B9LtC94ncVD/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "latest-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save(\"base_model_nn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "possible-poison",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_012_01__x.csv\n",
      "subject_009_01__x.csv\n",
      "subject_010_01__x.csv\n",
      "subject_011_01__x.csv\n"
     ]
    }
   ],
   "source": [
    "# Create Features for Test Data\n",
    "data_folder = \"TestData/x/\"\n",
    "dest_folder = \"TestFeatures/\" \n",
    "create_features(data_folder, dest_folder, \"\", False, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "crucial-pattern",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Label\n",
    "dest_folder = \"TestFeatures/\"\n",
    "test_session_ids = extract_session_ids(dest_folder)\n",
    "for i in range(0,len(test_session_ids)):\n",
    "    sid = test_session_ids[i]\n",
    "    x_path =  dest_folder + \"subject_\" + sid + \"__x.csv\"\n",
    "    y_path = \"TestLabel/\" + \"subject_\" + sid + \"__y.csv\"\n",
    "\n",
    "    x = pd.read_csv(x_path, header = None)\n",
    "    predY = base_model.predict(x)\n",
    "    y = np.argmax(predY, axis=1)\n",
    "    \n",
    "    pd.DataFrame(y).to_csv(y_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aging-curve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Test Performance:\n",
      "Accuracy Score : 0.8999122080470661\n",
      "Precision Score : 0.8999122080470661\n",
      "Recall Score : 0.8999122080470661\n",
      "F1 Score : 0.8999122080470661\n",
      "classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Ground(0)       0.85      0.78      0.81    251901\n",
      "  Upstairs(1)       0.96      0.98      0.97    251857\n",
      "Downstairs(2)       0.97      0.98      0.97    251462\n",
      "     Grass(3)       0.82      0.87      0.85    251706\n",
      "\n",
      "     accuracy                           0.90   1006926\n",
      "    macro avg       0.90      0.90      0.90   1006926\n",
      " weighted avg       0.90      0.90      0.90   1006926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,classification_report\n",
    "# prediction result for validation set\n",
    "valPredY = base_model.predict(X_val)\n",
    "valPredY = np.argmax(valPredY, axis=1)\n",
    "# New Model Evaluation metrics\n",
    "print(\"Overall Test Performance:\")\n",
    "print('Accuracy Score : ' + str(accuracy_score(y_val,valPredY)))\n",
    "print('Precision Score : ' + str(precision_score(y_val,valPredY,average='micro')))\n",
    "print('Recall Score : ' + str(recall_score(y_val,valPredY,average='micro')))\n",
    "print('F1 Score : ' + str(f1_score(y_val,valPredY,average='micro')))\n",
    "print(\"classification Report:\")\n",
    "print(classification_report(y_val, valPredY, target_names=[\"Ground(0)\", \"Upstairs(1)\", \"Downstairs(2)\", \"Grass(3)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-amendment",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
