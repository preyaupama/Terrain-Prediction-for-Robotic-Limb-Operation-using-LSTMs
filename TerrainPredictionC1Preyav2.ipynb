{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "qualified-yukon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, InputLayer, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import SGD, Adam\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "super-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_path = 'TrainingData/' # raw training data\n",
    "raw_test_path = 'TestData/' # raw testing data\n",
    "\n",
    "pp_tr_path = 'PreppedTrainingData/' # preprocessed training data folder\n",
    "pp_tr_session = 'tr_session_' # prefix for preprocessed training data for each session\n",
    "pp_tr_final = 'pp_tr.csv' # prefix for final training data\n",
    "\n",
    "pp_te_path = 'PreppedTestData/' # preprocessed test data folder\n",
    "pp_te_session = 'session_' # prefix for preprocessed test data for each session\n",
    "pp_te_final = 'pp_te.csv' # prefix for final test data\n",
    "\n",
    "tr_features_folder = \"TrainingFeatures/\" # Here training data with features calculated are stored\n",
    "\n",
    "acc_gyro_freq = 40\n",
    "label_freq = 10\n",
    "\n",
    "sample_p_label = acc_gyro_freq//label_freq # sample per label\n",
    "\n",
    "time_window = 1 # time frame we will look at - unit (seconds)\n",
    "\n",
    "row_p_window = time_window*acc_gyro_freq # how many samples fit in the time window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "independent-motor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_session_ids(path):\n",
    "    files = os.listdir(path)\n",
    "#     print(\"File Count: \" + str(len(files)))\n",
    "    session_ids = []\n",
    "    for f in files:\n",
    "        if f != '.DS_Store':\n",
    "#             print(f)\n",
    "            temp = f.split('subject_')[1].split('__')[0]\n",
    "            if temp not in session_ids:\n",
    "                session_ids.append(temp)\n",
    "#     print(\"Session Count: \" + str(len(session_ids)))\n",
    "#     print(session_ids)\n",
    "    return session_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "instrumental-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training or testing - for each session there are 4 required files\n",
    "def prepData(raw_data_folder, prepped_data_folder, session_prefix, final_prefix):\n",
    "    session_ids = extract_session_ids(raw_data_folder)\n",
    "    final_data = pd.DataFrame()\n",
    "    for i in range(0,len(session_ids)):\n",
    "        sid = session_ids[i]\n",
    "        x_time_path = raw_data_folder + \"subject_\" + sid + \"__x_time.csv\"\n",
    "        x_path =  raw_data_folder + \"subject_\" + sid + \"__x.csv\"\n",
    "        y_time_path = raw_data_folder + \"subject_\" + sid + \"__y_time.csv\"\n",
    "        y_path = raw_data_folder + \"subject_\" + sid + \"__y.csv\"\n",
    "\n",
    "        # read the files \n",
    "        x = pd.read_csv(x_path, header = None, names=[\"xa\",\"ya\",\"za\",\"xg\",\"yg\",\"zg\"])\n",
    "        x[\"x_time\"] = pd.read_csv(x_time_path, header = None, names=[\"x_time\"])\n",
    "        y = pd.read_csv(y_time_path, header = None, names = [\"y_time\"])\n",
    "        y[\"label\"] = pd.read_csv(y_path, header = None, names = [\"label\"])\n",
    "        \n",
    "#         print(len(x)/len(y))\n",
    "#         print(x.head())\n",
    "#         print(y.head())\n",
    "        \n",
    "        # merge labels with accelarometer and gyroscope observation - for each sample_p_label rows of features 1 label is used\n",
    "        y_time = []\n",
    "        y_label = []\n",
    "        session = [i]*len(x)           \n",
    "        ind = 0\n",
    "        counter = 1\n",
    "        for row in x.itertuples():\n",
    "            y_row = y.iloc[ind]\n",
    "            y_time.append(y_row.y_time)\n",
    "            y_label.append(y_row.label)\n",
    "            \n",
    "            if counter < sample_p_label:\n",
    "                counter = counter + 1\n",
    "            else:\n",
    "                if ind + 1 < len(y):\n",
    "                    ind = ind + 1\n",
    "                counter = 1\n",
    "        x[\"y_time\"] = y_time\n",
    "        x[\"label\"] = y_label\n",
    "        x[\"label\"] = x[\"label\"].astype('category')\n",
    "        x[\"session\"] = session\n",
    "        final_data = pd.concat([final_data,x])\n",
    "        # create files for preprocessed data for each session\n",
    "        file_path = prepped_data_folder + session_prefix + sid + '.csv'\n",
    "        print(file_path)\n",
    "        x.to_csv(file_path,index=False)\n",
    "    final_data.to_csv(prepped_data_folder + final_prefix, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "after-reputation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreppedTrainingData/tr_session_006_01.csv\n",
      "PreppedTrainingData/tr_session_001_08.csv\n",
      "PreppedTrainingData/tr_session_001_07.csv\n",
      "PreppedTrainingData/tr_session_006_03.csv\n",
      "PreppedTrainingData/tr_session_007_04.csv\n",
      "PreppedTrainingData/tr_session_004_02.csv\n",
      "PreppedTrainingData/tr_session_001_01.csv\n",
      "PreppedTrainingData/tr_session_003_02.csv\n",
      "PreppedTrainingData/tr_session_001_04.csv\n",
      "PreppedTrainingData/tr_session_008_01.csv\n",
      "PreppedTrainingData/tr_session_002_02.csv\n",
      "PreppedTrainingData/tr_session_001_05.csv\n",
      "PreppedTrainingData/tr_session_005_02.csv\n",
      "PreppedTrainingData/tr_session_007_01.csv\n",
      "PreppedTrainingData/tr_session_007_02.csv\n",
      "PreppedTrainingData/tr_session_007_03.csv\n",
      "PreppedTrainingData/tr_session_005_01.csv\n",
      "PreppedTrainingData/tr_session_002_01.csv\n",
      "PreppedTrainingData/tr_session_003_01.csv\n",
      "PreppedTrainingData/tr_session_002_04.csv\n",
      "PreppedTrainingData/tr_session_001_03.csv\n",
      "PreppedTrainingData/tr_session_001_02.csv\n",
      "PreppedTrainingData/tr_session_004_01.csv\n",
      "PreppedTrainingData/tr_session_006_02.csv\n",
      "PreppedTrainingData/tr_session_005_03.csv\n",
      "PreppedTrainingData/tr_session_001_06.csv\n",
      "PreppedTrainingData/tr_session_002_03.csv\n",
      "PreppedTrainingData/tr_session_003_03.csv\n",
      "PreppedTrainingData/tr_session_002_05.csv\n"
     ]
    }
   ],
   "source": [
    "# prepare train data + visualize\n",
    "prepData(raw_train_path, pp_tr_path, pp_tr_session,pp_tr_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "portuguese-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to create features\n",
    "def dist_vector(x1,y1,z1,x2,y2,z2): # distance between two vectors\n",
    "    return math.sqrt((x1-x2)*(x1-x2)+(y1-y2)*(y1-y2)+(z1-z2)*(z1-z2))\n",
    "def len_vec(x,y,z): # length of vector\n",
    "    return math.sqrt(x*x+y*y+z*z)\n",
    "def angle_vector(x1,y1,z1,x2,y2,z2): # cosine of angle\n",
    "    len1 = len_vec(x1,y1,z1)\n",
    "    len2 = len_vec(x2,y2,z2)\n",
    "    if len1 == 0 or len2 == 0:\n",
    "        return 0 # need to decide on this later\n",
    "    return (x1*x2+y1*y2+z1*z2)/(len1*len2) # what will happen if the denominator is zero\n",
    "def create_features(data_folder, dest_folder, all_session_file, use_prev_label=False, file_type=\"Train\"):\n",
    "    files = os.listdir(data_folder)\n",
    "    all_sessions_feat = pd.DataFrame() # features for all sessions\n",
    "    ts_feat_count = (row_p_window-1)*4 # time sensitive feature count - 4 features for each consecutive row pairs\n",
    "\n",
    "    for f in files:\n",
    "        print(f)\n",
    "        df = pd.read_csv(data_folder + f)\n",
    "        if file_type == \"Test\":\n",
    "            df = pd.read_csv(data_folder + f, names=[\"xa\",\"ya\",\"za\",\"xg\",\"yg\",\"zg\"]) \n",
    "        \n",
    "        # each row contains session number + features + label for previous row + target label\n",
    "        prev_row = None\n",
    "        prev_features = None\n",
    "        for row in df.itertuples():\n",
    "            features = []\n",
    "            if file_type == \"Train\":\n",
    "                features.append(row.session)\n",
    "            # time sensitive features---------------------------------\n",
    "            # step 1: initialize the time sensitive feature list\n",
    "            ts_features = [0]*ts_feat_count\n",
    "            if prev_features is not None:\n",
    "                ts_features = prev_features\n",
    "            # step 2: remove the first four features\n",
    "            ts_features = ts_features[4:]\n",
    "            # step 3: add the new four features at the end\n",
    "            if prev_row is None:\n",
    "                # 0,0,0 is the reference point - don't know if it's the right thing to do\n",
    "                dacc = dist_vector(0,0,0,row.xa,row.ya,row.za) # distance between accelarator vectors\n",
    "                aacc = angle_vector(0,0,0,row.xa,row.ya,row.za) # angle between accelerator vectors\n",
    "                dgyro = dist_vector(0,0,0,row.xg,row.yg,row.zg) # distance between gyroscope vectors\n",
    "                agyro = angle_vector(0,0,0,row.xg,row.yg,row.zg) # angle between gyroscope vectors\n",
    "                ts_features.extend([dacc, aacc, dgyro, agyro])\n",
    "            else:\n",
    "                dacc = dist_vector(prev_row.xa,prev_row.ya,prev_row.za,row.xa,row.ya,row.za) # distance between accelarator vectors\n",
    "                aacc = angle_vector(prev_row.xa,prev_row.ya,prev_row.za,row.xa,row.ya,row.za) # angle between accelerator vectors\n",
    "                dgyro = dist_vector(prev_row.xg,prev_row.yg,prev_row.zg,row.xg,row.yg,row.zg) # distance between gyroscope vectors\n",
    "                agyro = angle_vector(prev_row.xg,prev_row.yg,prev_row.zg,row.xg,row.yg,row.zg) # angle between gyroscope vectors\n",
    "                ts_features.extend([dacc, aacc, dgyro, agyro])\n",
    "            # step 4: add all the time sensitive features\n",
    "            features.extend(ts_features)\n",
    "            #---------------------------------------------------------\n",
    "            \n",
    "            if use_prev_label: # using previous row label as a feature could be problematic for test data\n",
    "                if prev_row is None:\n",
    "                    features.append(row.label) # if it is the first row use the label of this row\n",
    "                else:\n",
    "                    features.append(prev_row.label)\n",
    "                    \n",
    "            # Finally add the label\n",
    "            if file_type == \"Train\":\n",
    "                features.append(row.label)\n",
    "            # add row of features to the features file for a session\n",
    "            features_df = pd.DataFrame([features])\n",
    "            features_df.to_csv(dest_folder + f,mode='a',header=None,index=False) # features for a session\n",
    "            \n",
    "            # add row of features to the features file for all sessions\n",
    "            if file_type == \"Train\":\n",
    "                features_df.to_csv(dest_folder + all_session_file,mode='a',header=None,index=False)\n",
    "            \n",
    "            prev_row = row\n",
    "            prev_features = ts_features        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "breathing-mercury",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_session_002_01.csv\n",
      "tr_session_002_03.csv\n",
      "tr_session_002_02.csv\n",
      ".DS_Store\n",
      "tr_session_002_05.csv\n",
      "tr_session_002_04.csv\n",
      "tr_session_004_02.csv\n",
      "tr_session_004_01.csv\n",
      "tr_session_008_01.csv\n",
      "tr_session_006_01.csv\n",
      "tr_session_001_08.csv\n",
      "tr_session_006_02.csv\n",
      "tr_session_006_03.csv\n",
      "tr_session_003_02.csv\n",
      "tr_session_001_07.csv\n",
      "tr_session_001_06.csv\n",
      "tr_session_003_03.csv\n",
      "tr_session_003_01.csv\n",
      "tr_session_001_04.csv\n",
      "tr_session_001_05.csv\n",
      "tr_session_001_01.csv\n",
      "tr_session_001_02.csv\n",
      "tr_session_001_03.csv\n",
      "tr_session_005_01.csv\n",
      "tr_session_007_04.csv\n",
      "tr_session_005_03.csv\n",
      "tr_session_005_02.csv\n",
      "tr_session_007_03.csv\n",
      "tr_session_007_02.csv\n",
      "tr_session_007_01.csv\n"
     ]
    }
   ],
   "source": [
    "# create features\n",
    "create_features(pp_tr_path, tr_features_folder, pp_tr_final, False, \"Train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "national-recognition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling\n",
      "Counter({0.0: 1006926, 3.0: 206436, 2.0: 73068, 1.0: 55216})\n",
      "1341646\n",
      "1341646\n",
      "After oversampling\n",
      "4027704\n",
      "4027704\n",
      "Counter({0.0: 1006926, 1.0: 1006926, 2.0: 1006926, 3.0: 1006926})\n"
     ]
    }
   ],
   "source": [
    "# sampling\n",
    "train_data_w_feature = pd.read_csv(tr_features_folder + pp_tr_final, header = None)\n",
    "columns = len(train_data_w_feature.columns)\n",
    "X_train = train_data_w_feature.iloc[:,1:columns-1]\n",
    "y_train = train_data_w_feature.iloc[:,columns-1]\n",
    "counter = Counter(y_train)\n",
    "print(\"Before oversampling\")\n",
    "print(counter)\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "oversample = SMOTE()\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)\n",
    "counter = Counter(y_train)\n",
    "print(\"After oversampling\")\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "basic-basin",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = columns-2\n",
    "output_dim = 4\n",
    "lr=0.001\n",
    "def simpleNN(num_hidden_layers, num_neurons):\n",
    "    # input layer -> hidden layer -> hidden layer -> output layer\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_neurons, input_dim=input_dim, activation='relu')) # input layer\n",
    "    for i in range(0,num_hidden_layers):\n",
    "        model.add(Dense(num_neurons, activation='relu')) # hidden layers\n",
    "        \n",
    "    model.add(Dense(output_dim, activation='softmax')) # output layer\n",
    "    # compile the keras model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "engaged-invite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function for plotting training and validation learning curves\n",
    "def plot_history(history):\n",
    "    # plot loss\n",
    "    plt.title('Loss')\n",
    "    plt.plot(history.history['loss'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_loss'], color='red', label='test')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "\n",
    "    # plot accuracy\n",
    "    plt.title('Accuracy')\n",
    "    plt.plot(history.history['accuracy'], color='blue', label='train')\n",
    "    plt.plot(history.history['val_accuracy'], color='red', label='test')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "innocent-savage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 84s 31us/step - loss: 0.1294 - accuracy: 0.9546\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0763 - accuracy: 0.9736\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.0643 - accuracy: 0.9780\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0582 - accuracy: 0.9801\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0538 - accuracy: 0.9818\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0511 - accuracy: 0.9828\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.0487 - accuracy: 0.9836\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0474 - accuracy: 0.9842\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.0461 - accuracy: 0.98471s - loss:\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.0450 - accuracy: 0.9851\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.0441 - accuracy: 0.9854\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0435 - accuracy: 0.9857\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.0430 - accuracy: 0.9860\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0423 - accuracy: 0.9863\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0421 - accuracy: 0.9864\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 78s 29us/step - loss: 0.0416 - accuracy: 0.9867\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0414 - accuracy: 0.9867\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.0413 - accuracy: 0.9868\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 77s 29us/step - loss: 0.0415 - accuracy: 0.9869\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 77s 29us/step - loss: 0.0411 - accuracy: 0.9870\n",
      "1342568/1342568 [==============================] - 14s 11us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.4811 - accuracy: 0.7966\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.4053 - accuracy: 0.8357\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3839 - accuracy: 0.8463\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 77s 29us/step - loss: 0.3710 - accuracy: 0.8526\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3623 - accuracy: 0.8570\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.3555 - accuracy: 0.8601\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.3506 - accuracy: 0.8626\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.3468 - accuracy: 0.8646\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 74s 27us/step - loss: 0.3432 - accuracy: 0.8661\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 73s 27us/step - loss: 0.3403 - accuracy: 0.8677\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3374 - accuracy: 0.8691\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3352 - accuracy: 0.8703\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3336 - accuracy: 0.8710\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3318 - accuracy: 0.8720\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.3301 - accuracy: 0.8727\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.3285 - accuracy: 0.8735\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.3271 - accuracy: 0.8740\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3262 - accuracy: 0.8744\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3252 - accuracy: 0.8750\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.3244 - accuracy: 0.8754\n",
      "1342568/1342568 [==============================] - 12s 9us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.3544 - accuracy: 0.8699\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.2884 - accuracy: 0.8942\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2723 - accuracy: 0.9000\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 72s 27us/step - loss: 0.2632 - accuracy: 0.9036\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.2572 - accuracy: 0.9060\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 77s 29us/step - loss: 0.2525 - accuracy: 0.9077\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2488 - accuracy: 0.9091\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2458 - accuracy: 0.9102\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 74s 28us/step - loss: 0.2436 - accuracy: 0.9111\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2417 - accuracy: 0.9120\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 73s 27us/step - loss: 0.2398 - accuracy: 0.9125\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 73s 27us/step - loss: 0.2384 - accuracy: 0.9132\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 72s 27us/step - loss: 0.2374 - accuracy: 0.9138\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2365 - accuracy: 0.9141\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.2351 - accuracy: 0.9146\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 76s 28us/step - loss: 0.2343 - accuracy: 0.9149\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 75s 28us/step - loss: 0.2340 - accuracy: 0.9150\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 71s 27us/step - loss: 0.2333 - accuracy: 0.9152\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 72s 27us/step - loss: 0.2324 - accuracy: 0.9156\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 73s 27us/step - loss: 0.2320 - accuracy: 0.9157\n",
      "1342568/1342568 [==============================] - 20s 15us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 93s 35us/step - loss: 0.1206 - accuracy: 0.9579\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 95s 35us/step - loss: 0.0707 - accuracy: 0.9760\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.0601 - accuracy: 0.9800\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 90s 33us/step - loss: 0.0557 - accuracy: 0.9817\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.0534 - accuracy: 0.9829\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 90s 33us/step - loss: 0.0524 - accuracy: 0.9834\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.0514 - accuracy: 0.9840\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.0512 - accuracy: 0.9841\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 85s 32us/step - loss: 0.0510 - accuracy: 0.9844\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 85s 32us/step - loss: 0.0508 - accuracy: 0.9846\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 86s 32us/step - loss: 0.0505 - accuracy: 0.9847\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 86s 32us/step - loss: 0.0511 - accuracy: 0.9847\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 86s 32us/step - loss: 0.0515 - accuracy: 0.9846\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 85s 32us/step - loss: 0.0513 - accuracy: 0.9845\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 84s 31us/step - loss: 0.0524 - accuracy: 0.9845\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 84s 31us/step - loss: 0.0522 - accuracy: 0.9846\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.0532 - accuracy: 0.9845\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.0532 - accuracy: 0.9846\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 86s 32us/step - loss: 0.0535 - accuracy: 0.9844\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.0543 - accuracy: 0.9842\n",
      "1342568/1342568 [==============================] - 14s 11us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.4631 - accuracy: 0.8063\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.3852 - accuracy: 0.8463\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.3628 - accuracy: 0.8576\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.3515 - accuracy: 0.8633\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.3445 - accuracy: 0.8667\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.3414 - accuracy: 0.8683\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 87s 33us/step - loss: 0.3379 - accuracy: 0.8706\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.3360 - accuracy: 0.8717\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 93s 35us/step - loss: 0.3339 - accuracy: 0.8726\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 93s 35us/step - loss: 0.3326 - accuracy: 0.8736\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 87s 32us/step - loss: 0.3316 - accuracy: 0.8744\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.3306 - accuracy: 0.8748\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.3298 - accuracy: 0.8752\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.3289 - accuracy: 0.8758\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 92s 34us/step - loss: 0.3299 - accuracy: 0.8755\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.3306 - accuracy: 0.8756\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.3297 - accuracy: 0.8758\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.3301 - accuracy: 0.8760\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 88s 33us/step - loss: 0.3322 - accuracy: 0.8753\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.3314 - accuracy: 0.8755\n",
      "1342568/1342568 [==============================] - 21s 15us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.3389 - accuracy: 0.8762\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2748 - accuracy: 0.8994\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2585 - accuracy: 0.9058\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2497 - accuracy: 0.9094\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2444 - accuracy: 0.9113\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 90s 33us/step - loss: 0.2412 - accuracy: 0.9127\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2387 - accuracy: 0.9138\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 90s 34us/step - loss: 0.2371 - accuracy: 0.9146\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2359 - accuracy: 0.9153\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2348 - accuracy: 0.9157\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 91s 34us/step - loss: 0.2344 - accuracy: 0.9160\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2344 - accuracy: 0.9162\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 90s 34us/step - loss: 0.2356 - accuracy: 0.9159\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 90s 34us/step - loss: 0.2361 - accuracy: 0.9158\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 90s 34us/step - loss: 0.2358 - accuracy: 0.9162\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2357 - accuracy: 0.9162\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2361 - accuracy: 0.9163\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2369 - accuracy: 0.9162\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2367 - accuracy: 0.9162\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 89s 33us/step - loss: 0.2377 - accuracy: 0.9160\n",
      "1342568/1342568 [==============================] - 15s 11us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 223s 83us/step - loss: 0.1187 - accuracy: 0.9588\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 222s 82us/step - loss: 0.0716 - accuracy: 0.9761\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 220s 82us/step - loss: 0.0621 - accuracy: 0.9797\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 218s 81us/step - loss: 0.0580 - accuracy: 0.9813\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 219s 82us/step - loss: 0.0561 - accuracy: 0.9821\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 226s 84us/step - loss: 0.0547 - accuracy: 0.9829\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 220s 82us/step - loss: 0.0533 - accuracy: 0.9835\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 222s 83us/step - loss: 0.0534 - accuracy: 0.9836\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 228s 85us/step - loss: 0.0528 - accuracy: 0.9838\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 222s 83us/step - loss: 0.0531 - accuracy: 0.9839\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 221s 82us/step - loss: 0.0533 - accuracy: 0.9839\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 220s 82us/step - loss: 0.0538 - accuracy: 0.9841\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 224s 83us/step - loss: 0.0537 - accuracy: 0.9841\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 224s 83us/step - loss: 0.0540 - accuracy: 0.9841\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 223s 83us/step - loss: 0.0546 - accuracy: 0.9840\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 223s 83us/step - loss: 0.0551 - accuracy: 0.9842\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 222s 83us/step - loss: 0.0548 - accuracy: 0.9841\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 222s 83us/step - loss: 0.0565 - accuracy: 0.9837\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 226s 84us/step - loss: 0.0562 - accuracy: 0.9837\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 223s 83us/step - loss: 0.0562 - accuracy: 0.9837\n",
      "1342568/1342568 [==============================] - 20s 15us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 222s 83us/step - loss: 0.4591 - accuracy: 0.8085\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 227s 84us/step - loss: 0.3840 - accuracy: 0.8479\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 230s 86us/step - loss: 0.3643 - accuracy: 0.8578\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 212s 79us/step - loss: 0.3540 - accuracy: 0.8630\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 187s 70us/step - loss: 0.3479 - accuracy: 0.8661\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 183s 68us/step - loss: 0.3424 - accuracy: 0.8688\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.3390 - accuracy: 0.8706\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.3365 - accuracy: 0.8720\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.3338 - accuracy: 0.8733\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.3323 - accuracy: 0.8744\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.3313 - accuracy: 0.8749\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 182s 68us/step - loss: 0.3299 - accuracy: 0.8755\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 181s 68us/step - loss: 0.3294 - accuracy: 0.8759\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.3277 - accuracy: 0.8765\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.3280 - accuracy: 0.8768\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 182s 68us/step - loss: 0.3275 - accuracy: 0.8769\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.3274 - accuracy: 0.8771\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 181s 68us/step - loss: 0.3269 - accuracy: 0.8774\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 181s 68us/step - loss: 0.3267 - accuracy: 0.8774\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.3255 - accuracy: 0.8781\n",
      "1342568/1342568 [==============================] - 19s 14us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.3357 - accuracy: 0.8772\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 178s 66us/step - loss: 0.2742 - accuracy: 0.8999\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2596 - accuracy: 0.9057\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2527 - accuracy: 0.9085\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2481 - accuracy: 0.9101\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2439 - accuracy: 0.9120\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2420 - accuracy: 0.9130\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2404 - accuracy: 0.9137s - loss: 0.2404 - accuracy: 0.91\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2392 - accuracy: 0.9143\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2383 - accuracy: 0.9149\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2380 - accuracy: 0.9148\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2379 - accuracy: 0.9152\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.2378 - accuracy: 0.9152\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2371 - accuracy: 0.9155\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2361 - accuracy: 0.9160\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2361 - accuracy: 0.9160\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2355 - accuracy: 0.9161\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 180s 67us/step - loss: 0.2357 - accuracy: 0.9164\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 179s 67us/step - loss: 0.2361 - accuracy: 0.9162\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 181s 67us/step - loss: 0.2361 - accuracy: 0.9160\n",
      "1342568/1342568 [==============================] - 19s 14us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.1267 - accuracy: 0.9554: 1:24 - loss: - ETA: 1:22 - loss: 0.3331 - accuracy: 0.87 - ETA: 1:22 - loss: 0.3319 - accura - E - ETA: 1:18 - los - ETA: 1:15 - loss: 0.2645 - ETA - ETA - ETA: 55s - loss: 0.18 - ETA: 54s - loss: 0.1802 - ETA: 46s - loss: 0.1664 - accuracy: 0.9 - ETA: 46s - loss - ET - ETA: 31s - loss: 0.1486 -   - ETA: 24s - lo - ETA: 22s - loss: 0.1407 - accu - ETA: 22s - loss: 0.1403 - accu - ETA: 2 - ETA: 19s - loss: 0.1385 - accuracy:  - ETA: 19s - loss: 0.1383 - accur - ETA: 18s - loss: 0.1379 - accurac - ETA: 18s - loss: 0.1376 - accuracy: 0.951 - ETA: 18s - loss: 0.137 - ETA: 17s - loss: 0.1368 - a - ETA: 16s - loss: 0.1363  - ETA: 13s - loss: 0.1342 - a - ETA: 12s - loss: 0.1338 - accuracy: 0.9 - ETA: 12s - ETA: 4s - l - ETA: 3s - los - ETA: 3s - ETA: 2s - loss: - ETA: 1s - ETA: 0s - loss: 0.1272 - accuracy - ETA: 0s - loss: 0.1271 - accuracy - ETA: 0s - loss: 0.1269 \n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.0760 - accuracy: 0.9739 - ETA: 1:00 - loss: 0.081 - ETA: 1:00 - loss - ETA: 38s - loss: 0.0793 - accu - ETA: 37s - loss: 0.0792 - accuracy: 0. - ETA: 32s - loss: 0.0787 - ac - ETA: 32s - loss: 0.0787 - accurac - ETA: 3 - ETA: 27s -\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 84s 31us/step - loss: 0.0663 - accuracy: 0.9774: 58s - loss: 0 - ETA: 57s - loss: 0.0674 - accuracy: 0 - ETA: 57s - loss: 0.0674 - accuracy: 0.97 - ETA: 57s - loss: 0.0674 - accuracy:  - ETA: 57s - loss: 0.0674 - accuracy: 0.97 - ETA: 56s - loss: 0.0674 - accu - ETA - ETA: 3s - loss: 0.0663 - accura\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 85s 32us/step - loss: 0.0621 - accuracy: 0.9791: 58s - loss: 0.0627 - a - ETA:  - ETA: 56s - loss: 0.0625 - accuracy: 0.97 - ETA: 55s - loss - ETA: 54s - loss: 0.0623 - ETA: 53s - loss: 0.0623 - accuracy: 0. - ETA: 53s - loss: 0.0623 - accuracy: 0.979 - ETA: 53s - loss: 0.0623 -  - ETA: 52s - loss: 0 - ETA: 51s - loss: 0.0624 - accuracy: 0.978 - ETA: 51s -  - ETA: 49s - loss: 0.0625 - accuracy: 0. - ETA: 49s - lo  - ETA: 43s - loss: 0.0 - ETA: 42s - loss: 0.0624 - accur - ETA: 41s - loss - ETA: 40s - loss: 0.0624 - accu - ETA: 39s - loss: 0.0624 - accur - ETA: 13s - loss: 0.0621 - accuracy: 0.97 - ETA: 13s - loss: 0.0621 - accuracy:  - ETA: 13s - los - ETA: 11s - - E\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.0591 - accuracy: 0.9802: 59s - loss: 0 - ETA: 58s - loss: 0.059 - ETA: 55s - loss: 0.0593 - accuracy: 0.9 - ETA: 55s - loss: - ETA: 53s - loss: - ETA: - ETA: 50s - loss: 0.0592 - acc - ETA: 49s - loss: 0.0592  - ETA: 48s - loss: 0.0591 - accuracy:  - ETA: 48s - loss: 0.0592 - accuracy:  - ETA: 48s - loss: 0.0591 - accur - ETA: 47s - loss: 0.0592 - ac - ETA: 46s - loss: 0.0592 - accuracy: 0.9 - ETA: 46s - loss: 0.0592 - accuracy: 0. - ETA: 46s - loss: 0.0592 - accuracy: 0. - ETA: 46s - loss: 0.05 - ETA: 45s - loss: 0.0593 - accurac - ETA: 42s - loss: 0.0594 - accura - ETA: 39s - loss: 0.0593 - accuracy - ETA: 39s - loss: 0.0593  -  - ETA - ETA: 29s - loss: 0.0593 - accuracy: 0.98 - ETA: 29s - ETA: 27s - loss: 0.0592 - accuracy: 0 - ETA: 27s - loss: 0.0 - ETA: 25s - loss: 0.0592 - - ETA: 25s - loss: 0.0591 - accur - ETA: 22s - loss: 0.05 - ETA: 21s - loss: 0.0593 - - ETA: 20s - loss: 0.0592 - accuracy:  - ETA: 19s - loss: 0.0592 - accura - ETA: 19s - loss: 0.0593 - accura - ETA: 18s - loss: 0.0593 - ac - ETA: 18s - loss: 0.0593 - accurac - ETA: 17s -  - ETA: 16s - loss: 0. - ETA: 1 - ETA: 2s - l - ETA: 0s - loss: 0.0\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0573 - accuracy: 0.9809: 1:31 - loss: 0.056 - ETA: 1:23 - loss: 0.0 - ETA: 1:22 - loss: 0.0576 - accuracy: 0. - ETA: 1:22 - loss: - ETA: 1:17 - loss: 0.0575 - accuracy: 0.98 - ETA: 1:17 - l - ETA - ETA: 1:15 - loss: 0.057 - ETA: 1:14 - loss: 0 - ETA: 1:12 - ETA: 1:11 - loss: 0.0583  - ETA: 1:11 - loss: 0.0581 - accuracy:  - ETA: 1:11 - loss: 0.0579 - accuracy: 0. - ETA: 1:11 - loss: 0.0580 - accuracy: 0.98 - ETA: 1:11 - loss: 0.0580 - accuracy:  - ETA: - ETA: 58s - loss: 0.0581 - accura - ETA: 57s - loss: 0.0581 - accuracy: 0.980 - ETA: 57s - loss: 0.0582 - accurac - ETA: 57s - loss: 0.0581 - accur - ETA: 56s - loss: 0.0582 - accuracy: - ETA: 56s - loss: 0.0581 - accur - ETA: 55s - loss: 0.0583  - ETA: 54s - loss: 0.0582 - a - ETA: 54s - loss: 0.0582 - accurac - ET - ETA: 51s - loss: 0.0581 - ac - ETA: 50s - loss: 0.0580 - accuracy: 0.98 - ETA: 50s - loss: 0.0580 - a - ETA: 50s - loss: 0.0579 - accuracy - ETA: 49s - loss:  - ETA: 48s - loss: 0.0579 - accuracy - ETA: 47s - loss: 0.0578 - ac - ETA: 47s - los - ETA: 45s - loss: 0.0579 - ETA: 42s - loss: 0.0577 - accuracy:  - ETA: 39s - loss: 0.0578 - ETA: 39s - loss: 0.0578 - accuracy - ETA: 38s - loss: 0.0578 - accu - ETA: 37s - lo - ET - ETA: 34s - loss: 0.0578 - - ETA: 33s - loss: 0.05 - ETA: 32s - loss: 0.0578 - ac - ETA: 31s - los - ETA: 30s - loss: 0.0576 - accuracy: - ETA: 30s - loss: 0.0 - ETA: 28s - loss: 0.05 - ETA: 27s - loss: 0.0575  - ETA: 2s - los - ETA: 1s - -\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0559 - accuracy: 0.9815: 1:12 - loss: 0.0561  - ETA: 1:11 - loss: 0.0560 - accuracy - ETA: 1:11 - loss: 0.0559 - - ETA: 59s - loss: 0.0551 - accuracy: 0 - ETA: 59s - loss: 0.0550 - accuracy:  -  - ETA: 54s - loss - ETA: 53s - loss: 0.0551 - accur - - ETA: 50s - l - ETA: 49s - loss: 0.0551 - accuracy: 0 - ETA: 49s - loss: 0.0552 - ETA: 48s - loss - ETA: 46s - loss: 0.0551 - accuracy: 0.9 - ETA: 46s - loss: 0.0552 - accuracy: 0.9 - ETA: 46s - loss: 0.0551 -  - ETA: 43s - loss: 0.0553 - accuracy: 0.98 - ETA: 43s - los - ETA: 41s - loss: 0.0552 - accuracy: 0. - ETA: 41s - loss: 0.0553 - acc  - ETA: 38s - loss: 0.0552 - accuracy: 0.98 - ETA: 38 - ETA: 36s - loss: 0.0553 - accuracy - ETA: 36s - loss: 0.0555 - accuracy: 0.98 - ETA: 36s - l - ETA: 30s - loss: 0.0556 - accuracy: 0.98 - ETA: 30s  - ETA: 26s - loss: 0.0559 -  - ETA: 25s - loss: 0.0559 - accuracy: 0 - ETA: 25s - loss: 0.055 - ETA: 24s - loss: 0.0560 -  - ETA: 23s - loss: 0.0558 - accurac - ETA: 22s - loss: 0.0558 - accu - ETA: 22s - loss: 0.0558 - ETA: 21s - loss:  - ETA: 20s - loss: 0.0558 - accuracy: - ETA: 19s - lo - ETA: 18s - loss: 0.0559 - ac - ETA: 17s - loss: 0.0560  - E - ETA: 1 - ETA: 4s - loss: 0.0560  - ETA - ETA: \n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.0540 - accuracy: 0.9822TA: 59s - loss: 0.0542 - a - ETA: 58s - loss: 0.0543 - accuracy:  - ETA: 57s - ETA: 56s  - ETA: 54s - loss: 0 - ETA: 53s - loss: 0.0540 - accuracy: - ETA: 53s - loss: 0.0542 - accuracy - ETA: 52s - ETA: 50s - loss: 0.0540 - accuracy: 0 - ETA: 50s - loss: 0.0540 - accuracy - ETA: 50s - loss: 0.05 - ETA: 44s - loss: 0.053 - ETA: 43s - los - ETA: 42s - loss: 0.0538 - accura - ETA: 39s - loss: 0.0539 - - - ETA: 36s - lo - ETA: 34s - loss: 0.0 - ETA: 33s - loss: 0.05 - ETA: 32s - loss: 0.0539 - accuracy: 0.9 - ETA: 32s - loss: 0.0539 - accur - ETA: 31s - loss: 0 - ETA: 30s - loss: 0. - ETA: 27s - loss: 0.0541 - accuracy: 0 - ETA: 26s - loss: 0.0541 - accuracy: 0.982 - ETA: 26s - loss: 0.0541 - accurac - ETA: 26s - loss: 0.0541 - accuracy:  - ETA: 26s - loss: 0.0541 - accuracy:  - ETA: 23s - loss: 0.0541 - accuracy: - ETA: 23s - loss:  - ETA: 21s - loss: 0.0541 - accuracy: 0 - ETA: 21s - loss: 0.0540 - accuracy:  - ETA: 21s - loss: 0.0540 - a - ETA: 16s - loss: 0.0541  - ETA: 15s - loss: 0.0542 - accurac - ETA: 12s -  - ETA: 0s - loss: 0.0540 - ac\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0535 - accuracy: 0.9824: 59s - loss: 0.0534 - accuracy: 0.98 - ETA: 59s - loss: 0.0534 - ETA: 58s - loss: - ETA: 57s - loss: 0.0536 - accuracy: 0.982 - ETA: 57s - loss: 0.0535 - accuracy: 0.9 - ETA: 56s - loss: 0.0535 - accura - ETA: - ETA: 54s  - ETA: 52s - loss: 0.0533 - - ETA: 52s - loss: 0.0 - ETA: 50s - loss: 0.0535 - accuracy: 0.98 - ETA: 50s - los - ETA: 42s - loss: 0.0538 - accura - ETA: 42s - loss: 0.0537 - accuracy: 0.98 - ETA - ETA: 39s  - ETA - ETA: 36s - loss: 0.0538 -  - ET - ETA: 33s - loss: 0.0537 - accuracy: 0 - ETA: 33 - ETA: 31s - loss: 0.0538 - accuracy: 0.982 - ETA: 31s - loss: 0.0538 - acc - ETA: 30s - loss: 0.0537 - ETA: 29s - loss: 0.0537 - accuracy: 0. - ETA: 29s - loss: 0.0536 - acc - ETA: 28s - loss: 0.0536 - accu - ETA: 28s - loss: 0.0536 - accur - ETA: 27s - loss: 0.0536 - accuracy: 0. - ETA: 27s - loss: 0.0537 - accur - ETA: 27s - loss: 0.0536  - ETA: 26  - ETA: 5s - loss: 0.0534  - ETA\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0522 - accuracy: 0.9829: 1:21 - loss: 0.0536 - accuracy - ETA: 1:20 - loss: 0.0548 - accuracy: 0. - ETA - ETA: 1:10 - loss: 0.0529 - accuracy:  - ETA:  - ETA: 1:08 - loss: 0.0530 - accu - ETA: 1:08 - l - ETA: 1:03 - loss: 0.0527 - ac - ETA: 1:02 - loss: 0.0526 - accura - ETA: 1:02 - loss: 0.0527 - accu - ETA: 1:02 - loss: 0.0526 - accuracy: 0. - ETA: 1:01 - loss: 0.0527  - E - ETA: 1:0 - ETA: 59s - loss: 0.0526 - accuracy: 0.98 - ETA: 58s  - ETA: 57s - loss: 0.0526 - acc - ETA: 56s - loss: 0.05 - ETA: 55s - loss: 0.0525 - - ETA: 54s - l - ETA: 53s - loss: 0.0525 - accuracy: 0. - ETA: 52s - loss: 0.0525 - accuracy: - ETA: 52s - loss: 0.0525 - accuracy: 0.982 - ETA: 52s - loss: 0.0525 - accur - ETA: 51s - loss: 0.0524 - accuracy: 0.982 - ETA: 51s - loss: 0.052 - ETA: 50s - los - ETA: 49s - - ETA: 47s - loss: 0.0523 - accuracy: - ETA: 47s - loss: 0.0524 - accur - ETA: 46s - loss: 0.0524 - accuracy: - ETA: 46s - loss: 0.0524 - accuracy: 0.982 - ETA: 46s - loss: 0.0523 - accuracy: 0.982 - ETA: 46s - loss: 0.052 - ETA: 45s - loss: 0.0524 - a - ETA: 42s - loss: 0.0526 - accuracy:  - ETA: 42s - loss: 0.0526 - accuracy: 0 - ETA: 41s - loss: 0.0526 - acc - ETA: 41s - loss: 0.0526 - accuracy: 0 - ETA: 40s - ETA: 39s - loss: 0.0525  - ETA: 38s - loss: 0.0526 - accur - ETA: 3 - ETA - ETA: 33s - loss: 0.0523 - ETA: 33 - ETA: 31s - loss: 0.0524 - ETA: 27s - loss: 0.0524 - accuracy: 0 - ETA: 27s - loss: 0.0524 - ac - ETA: - - ETA: 13s - l -\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0515 - accuracy: 0.9832ETA: 27s - loss: 0.0519 - accura - ETA: 27s - loss: - ETA: 25s - loss: 0 - ETA: 24s - loss - ETA: 23s - loss:  - ETA: 22s - loss: 0.0516 - accuracy: 0.9 - ETA: 21s - loss: 0.051 - ETA: 20s - los - ETA: 19s - loss: 0.0515 - - ETA: 16s - loss: 0.0514 - accuracy:  - ETA: 16s - loss: 0.0514 - a - ETA: 15s - loss: 0.0515  - ETA: 12s - loss: 0.0516 - accuracy: 0 - ETA: 11s - loss: 0.0518 - accuracy: - ETA: 7s - loss: - ETA: 3s - loss: 0.0515 - accu - ETA:  - ETA: 2s - loss: 0.0515 - ac - ETA: 1s - ETA\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0503 - accuracy: 0.9838: 1:02 - - ETA: 54s - loss: 0.0503 - accura - ETA: 53s - loss: 0.0503 - ETA: 50s - loss: 0.0501 - accuracy: 0. - ETA: 50s - loss: - ETA: 49s - loss: 0.0502 - accuracy: 0.983 - ETA: 49s - loss: 0.0502 - accuracy: 0. - ETA: 48s - ETA: 44s - loss: 0.0502 - accuracy: - ETA: 44s - - ETA: 4 - ETA: 41s - loss: 0.0505 - accuracy: - ETA: 40s - loss: 0.0 - ETA: 39s - loss: 0.05 - ETA: 36s - loss: 0.0502 - acc - ETA: 35s - loss: 0.0503 - accuracy: 0.983 - ETA: 35s - loss: 0.0503 - accur - ETA: 35s - loss: 0.0502 - - ETA: 34s - loss: 0.0504 - accurac - ETA: 33s - loss - ETA: 16s -  - ETA: 14s - loss: 0.0504 - accuracy: 0.983 - ETA: 14s - loss: 0.0504 - accuracy: - ETA: 14s - loss: 0.0504 - accurac - E - ETA: 3s - loss: 0.0500  - ETA: 2s - los - ETA: 1s - loss: 0.0501 - accu - ETA: 1s - - ETA: 0s\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0495 - accuracy: 0.9843:  - ETA: 1:21 - E - ETA: 1:13 - l - ETA: 1:12 - loss: 0.0487 - accura - ETA: 1:12 - loss: 0.0487 - accuracy:  - ETA: 1:12 - lo - ETA: 47s - loss: 0.0488 - accuracy: 0. - ETA: 46s - loss: 0.0489 - accur - ETA: 46s - loss: 0.0488 - accura - ETA: 45s - loss: 0.0488 - accuracy: 0.98 - ETA: 45s - loss: 0.0488 - acc - ETA: 44s - loss: 0.0489 - accuracy: 0 - ETA: 44s - loss: 0.0489 - accuracy: 0.98 - ETA: 44 - ETA: 40s - loss: 0.0488 - accuracy:  - ETA: 40s - loss: 0.0488 - accura - ETA: 39s - loss: 0.0487 - acc - ETA: 39 - ETA: 37 - ETA: 35s - loss - ETA: 27s - loss: 0.0490 -  - ETA: 26s - loss: 0.0490 - accuracy: 0. - ETA: 26s - loss: 0.0491  - ETA: 25s - loss: 0.0490 - accuracy - ETA: 25s - loss: 0.0490 - accuracy: - ETA: 24s - loss: 0.0491 - accuracy - ETA: 24s - loss: 0.0490 - accuracy: 0 - ETA: - ETA: 22s - loss:  - ETA: 20s - loss: 0.0495 -  - ETA: 20s - loss: 0. - ETA: 1 - ETA: 10s - loss: 0.0 - ETA: 9s - loss: 0.0494 - accuracy - - ETA: 7s - loss: 0.0495 - accuracy:  - ETA: 7s - loss: 0.0495  - ETA: 6s - loss: 0.0 - ETA: 6s - loss: - ETA: 4s - loss: 0.0494  - ETA - ETA: 1s - loss: - ETA: 1s - los - ETA: 0s - loss: 0.0495 - \n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.0477 - accuracy: 0.9849: 1:05 - loss: 0.0483 - accu - E - ETA: 1 - ETA: 50s - loss: 0.0474 - accuracy - ETA: 49s - loss: 0.0475 -  - ETA: 48s - loss: 0.0476 - accuracy: 0 - ETA: 48s - loss: 0.0476 - accuracy: 0 - ETA: 48s  - ETA: 44s - loss: 0.0475 - accuracy: 0.9 - E - ETA: 42s - loss: 0.0472 - accurac - ETA: 41s - loss: 0.0475 - accu - ETA: 41s - loss - ETA: 39s - loss: 0.0473 - accuracy: - ET - ETA: 37s - loss: 0. -  - ETA: 31s - loss: 0 - ET - ETA: 23s - loss: 0.0474 - ac - ETA: 23s - loss: 0.0474 - accuracy: 0.985 - ETA: 23s - loss: 0.0474 - accurac - E - ETA: 18s - loss: 0.0472 - accuracy: 0 - ETA: - ETA: 0s - loss:\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0474 - accuracy: 0.9853TA: 1:18 - loss: 0.0452  - ETA: 1:08 - loss: 0.046 - ETA: 1:08 - loss: 0.046 - ETA: 1:04 - ETA: 1:03 - loss: 0.0474 - accuracy:  - ETA: 1:03 - loss: 0.0 - ETA: 1:02 - loss: - ETA: 1:02 - loss: 0.0 - ETA - ETA: 1:00 - loss: 0.0486 - accuracy: 0.98 - ETA: 1:00 - loss: 0.048 - ETA: - ETA: 55s - loss: 0.0481 - a - ETA: 54 - ETA: 46s - loss: 0.0483 -  - ETA: 45s - loss: 0.0483 - accuracy: 0.985 - ETA: 45s - loss - ET - ETA: 41s - loss - ETA: 40s - loss: 0.0479  - ETA: 39s - lo - ETA: 35s - loss: - ETA: 32s - loss: 0.0478 - accur - ETA: 31s - loss: 0.0478 -  - ETA: 28s - loss: 0.0477 - accurac - ETA: 28s - loss: 0.0477 - accuracy: 0 - ETA: 27s - loss: 0.0477 - acc - ETA: 27s - loss: 0.0476 - ETA: 26s - loss: 0. - ETA: 25s - loss: 0.0477 - a - ETA: 24s -  - ETA: 22s - loss: 0.0477 - accuracy: 0.98 - ETA: 22s - loss: 0.0477 - accuracy: 0.985 - ETA: 22s - loss: 0.0477 - accuracy: 0 - ETA: 22s - loss: 0.0478 - accuracy: - ETA: 22s - loss: 0.0478 - ETA: 21s - loss: 0.0477 - accuracy: 0 - ETA: 20s - loss:  - E - ETA: 17s - loss: 0.0479 - accuracy: 0.98 - ETA: 17s - loss: 0.0478 - accurac - ETA: 16s - loss: 0.0478 - accuracy: 0.9 - ETA: 16s - loss: 0.0478 - - ETA: 15s - loss: 0.0478 - accuracy: 0 - ETA: 15s - loss: 0.0478 - accurac - ETA: 15s - loss: 0.0478 - accuracy: 0.98 - ETA: 15s - loss:  - ETA: 11s - loss: 0.0476 - accura - ETA: 11s - loss: 0.0476 - accuracy: 0.98 - ETA: 11s - loss: 0.0476 - acc - ETA: 10s - loss: 0.0477 - accuracy: 0.985 - ETA: 10s - loss: 0.0477 - - ETA: 9s - loss: 0 - ETA:  - ETA: 0s - loss: 0.0474 \n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0461 - accuracy: 0.9856: 1:31 - l - ETA: 1:22 - - ETA: 58 - ETA: 54s - loss: 0.0467 - accurac - ETA - ETA: 52s - loss: 0.0467 - accuracy: 0 - ETA: 52s - loss:  - ETA: 50 - ETA: 48s - loss: 0.0465 - accurac - ETA: 48s - loss: 0.0464 - ETA: 47s - loss: 0.0462 - accuracy - ETA: 47s - l - ETA: 43s - loss: 0.0462 - accuracy:  - ETA: 43s - loss: 0.0462 - accuracy: - ETA: 42s - loss: 0.0462 - ET - ETA: 28s - loss:  - E - ETA: 24s - loss: 0.0462 - accur - ETA: 24s - loss: 0.0462  - ETA: 23s - \n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 83s 31us/step - loss: 0.0464 - accuracy: 0.9857: 58s - loss: 0.0451 - accuracy: 0.98 - ETA: 58s - loss: - ETA: 57s - loss: 0.0450 -  - ETA: 56s - loss:  - ETA: 55s - ETA: 49s - loss: 0. - ETA: 47s - loss: 0.0453 - accu - ETA: 47s - loss: 0.0453  - ETA: 46s - loss: - ETA: 42s - l - ET  - ETA: 36s - loss: 0.0463 - accuracy - ETA: 36s - loss: 0.0463 \n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0465 - accuracy: 0.9860: 1:23 - los - ETA: 1:21 - loss: 0.041 - ETA: 1:21 - loss: 0.0416 - accu - ETA - ETA: 1:19 - loss: 0.043 - ETA: 1: - ETA: 1:08 - loss: 0.0442 - accura - ETA: 1:08 - loss: 0.0 - ETA: 1:07 - loss: 0.0448 - accuracy: 0.98 - ETA: 1:07 - loss: 0.0450 - ac - ETA: 1:07 - loss: 0.0451  - E - ETA: 1:04 - loss: 0.0 - ETA: 1:03 - loss: 0 -  - ET - ETA: 50s - loss: 0 - ETA: 47s - loss: 0.0462 - ac - ETA: 46s - loss: 0.0463 - ETA: 45s - loss:  - ETA: 44s - loss: 0.0464 - accur - ETA: 43s - loss: 0.0465 - accuracy: 0. - ETA: 43s - loss: 0.0465 - ac - ETA: 40s - loss: 0 - ETA: 39s - loss: 0.0464 - accu - ETA: 38s - ETA: 37s - loss: 0.0463 - acc - ETA: 36  - ETA: 16s - l - ETA: 14s - loss: 0.0464 - acc - ETA: 11s - loss: 0.046 - ETA: 10s - loss: 0.0464 - accuracy: 0.9 - ETA: 10s - loss: 0.0464 - ac\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0466 - accuracy: 0.9861: 1:16 - loss: 0.0426 - accuracy - ETA - ETA: 1:15 - loss: 0.0425 - accuracy - ETA: 1:14 - - ETA:  - ETA: 1:13 - loss: - ETA: 1:10 - loss: 0.0446 - accuracy: 0.98 - ETA: 1:10 - loss: 0.0447 - accu - ETA: 1:09 - loss: 0.0448 - accu - ETA: 1:04 - loss: 0.045 - E -  - ETA: 50s - loss: 0.0457 - acc - ETA: 49 - ETA: 47s - loss: 0.0455 - accuracy: 0.9 - ETA: 47s - loss: 0.0456 - accuracy: 0.9 - ETA: 47s - loss: 0.0456 - accuracy: - ETA: 47s - loss: 0.0456 -  - ETA: 44s - loss: 0.0456 - accuracy: 0 - ETA: 43s - loss:  - ETA: 42s - loss: 0.0 - ETA: 41s - loss: 0.0456 - acc - ETA: 40s - loss: 0.0456 - accurac - ETA:  - ETA: 3 - ETA: 34s - loss: 0.0456 - accuracy: 0. - ETA: 34 - ET - ETA: 19s - loss: 0.0461 - a - ETA:  - ETA: 3s - loss: 0.0466 - accuracy: 0. - ETA: 2s - loss: 0.0467 - accura - ETA: 2s - los - ETA: 1s - - ETA: 1s - loss: 0.0467 -  - ETA: 0s - l\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 82s 31us/step - loss: 0.0481 - accuracy: 0.9864 ETA: 1:18 - loss: 0.0470 - ac - ETA: 1:13 - ETA: 1:04 - los - ETA: 1:03 - loss: 0. - ETA: 54s - loss: 0.0479 - accuracy - ETA: 53s - loss: 0.0479 - accuracy: 0.986 - ETA: 53s - loss: 0.0479 - accuracy: - ETA: 53s - loss: 0.0478 - accura - ETA: 52s - loss: 0.0477 - accuracy:  - ETA: 52s - ETA: 44s - loss: 0.0491 - accuracy: 0.98 - ETA: 43s - loss: 0.0491 - accuracy:  - ETA: 43s - loss: 0.0490 - ac - ETA: 42s -  - ETA: 41s - l - ETA: 35s  - ETA: 33s - los - ETA: 32s - loss: 0.0486 - ETA: 31s - loss: 0 - ETA: 29s - loss: 0.0483 - accuracy: 0.98 - ETA: 29s - loss: 0.0486 - accuracy: - ETA: 29s - loss: 0.0487 - accuracy: 0 - ETA: 29s - loss: 0.0487 - accu - ETA: 28s - loss: 0.0486 - ac - ETA: 27s - loss: 0.048 - ETA: 26s - loss: 0.0483 - accura - ETA: 26s - loss: 0.0483 - accuracy: 0.986 - ETA: 26s - loss: 0.0483 - a - ETA: 25s - loss: 0.0482 - accuracy:  - ETA: 25s - lo  - ETA: 19s - loss: 0.0490 - accuracy:  - ETA: 16s - loss: 0.0488 - accuracy: 0.9 - ETA: 1\n",
      "1342568/1342568 [==============================] - 11s 8us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.4786 - accuracy: 0.7970: 51s - loss: 0.5426 - - ETA: 48s - loss: 0.5355 - acc - ETA: 47s - loss: 0. - ETA: 46s - loss: 0.531 - ETA: 45s - loss: 0.5299 - accu - ETA: 44s - loss: 0.5287 - ac - ETA: 43s - loss: 0.5272 - accuracy:  - ETA: 43s - loss: 0.5267 - accuracy: - ETA: 43s - loss - ETA: 41s - loss: - ETA: 40s - loss: 0.5214 - accuracy - ETA: 39s - loss: 0.5207 -  - ETA: 39s - loss: 0.5195 - accuracy: 0.775 - ETA: 38s - lo - E - ETA: 35s  - ETA: 33s - loss: 0.5118 - accuracy: - ETA: 33s - loss: 0.5113 - a - ETA: 32s - loss:  - ETA: 24s - loss: 0.5001 - ac - ETA: 18s - loss: 0.4944 -  - ETA: 18s  - ETA: 16s - l - ETA: 14s - loss: 0.4904  - ETA - ETA: 0s - l\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.4022 - accuracy: 0.8371: 1:06 - - ETA: 1: - ETA  - ETA: 46s - loss: 0.4105 - ac - ETA: 46s - loss: 0.4104 - accuracy: - ETA: 45s - loss: 0.4102 - accuracy: 0.833 - ETA: 45s - loss: 0.4102 - accu - ETA: 45s - loss: 0.4101 - ac - ETA: 44s - loss: 0.4 - ETA: 40s - loss: 0.4093 - ac - ETA: 40s - loss: 0.4091  - ETA: 37s - loss: 0.4083 - - ETA: 36s - ETA: 34s - loss: 0.4079 - acc - ETA: 33s - lo - ETA: 32s - loss: 0.4075 - ETA: 13s - loss: 0.4041 - accuracy: 0.8 - ETA: 12s - loss: 0.4 - ETA: 11s - loss: 0.4039 - accuracy: 0 - ETA: 11s - loss: 0.4038 -  - - - ETA: 0s - loss: 0.4023 \n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3814 - accuracy: 0.8475: 59s - loss: 0.3854 - accuracy: - ETA: 58s - ETA: 56s - loss: 0.3854 - - ETA: 56s - loss: 0.3853 - accu - ETA: - ETA: 53s - loss: 0.3853 - accuracy: 0.845 - ETA: 53s - loss: 0.3853 - accuracy: 0. - ETA: 53s - loss: 0.3852 - accuracy: 0.8 - ETA: 53s -  - ETA: 49s - loss: 0.3849 - accuracy: 0.8 - ETA: 49s - loss: 0.3849 - accuracy: 0.845 - ETA: 49s - loss: 0.3 - ETA: 47s - loss: 0.3848 - accuracy: 0.84 - ETA: 47s - loss: 0.3848 - ac - ETA: 47s - loss: 0.3849 - - ETA: 46s - loss: 0.3848 - accuracy: 0.84 - ETA: 46s - loss: 0.3848 - accuracy: 0.845 - ETA: 46s - loss: 0 - ETA: 42s - loss - ETA: 41s - loss: 0.3 - ETA - ETA: 38s - loss: 0.3839 - acc - ETA: 37s - loss:  - ETA: 36s - loss: 0.3839 - accuracy: 0. - ETA: 35s - loss: 0.383 - ETA: 34s - loss: 0.3838 - accurac - ET - ETA: 32s - loss: 0.3836 - accuracy: - ETA: 32s - loss: 0.3836 - acc - ETA: 31s - loss: 0.3836 - accuracy: 0.84 - ETA: 31s - loss: 0.3 - ETA: 30s - loss: 0.3834 - accuracy: 0. - - ETA: 25s - loss: 0 - ETA: 24s - loss:  - ETA: 23s - loss: 0.3830 - accur - ETA: 22s - loss: 0 - ETA: 21s - loss: 0.3828 - accuracy: 0.84 - ET\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3713 - accuracy: 0.8525: 28s - loss: 0.3727 - accuracy: 0 - ETA: 28s - loss: 0.3726 - accuracy: 0. - ETA: 28s - loss: 0 - ETA: 26s - loss: 0.3725 - acc - ETA: 26s - loss: 0.3725  - ETA: 25s - loss: 0.3726 - accu - ETA: 24s - loss: 0.3726 - ETA: 21s - loss: 0.3723 -  - ETA: 20s - loss: 0.37 - E - E - E - ETA: 1s - l - ETA: 0s\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3644 - accuracy: 0.8563: 49s - loss: 0.3653 - ac - ETA: 49s -  - - ETA: 36s - loss: 0.3649 - accuracy: 0 - ETA: 36s - loss: 0.3649 - accuracy:  - ETA: 35s - loss: 0.3649 -  - ETA: 34s - loss: 0.3649 - accuracy:  - ETA: 34s - loss: 0.3652 - a - ETA: 33s - loss: 0.3652 - ac - ETA: 33s - loss: 0.3651 - accur - ETA: 32s -  - ETA: 31s - loss: 0.3653 - accuracy: 0.8 - ETA: 30s - loss: 0.3653 - accuracy: 0.856 - ETA: 30s - loss: 0.3653 - accuracy: 0. - ETA: 30s - loss: 0.3653 - accuracy - ETA: 2 -  - ETA: 17s - loss: 0.3648 -  - ETA: 16 - ETA: 14s - loss: 0.3 - ETA: 13 - ETA: 0s - loss: 0.3645 - ac - ETA: 0s - loss: 0.3644 - accuracy: 0. - ETA: 0s - loss: 0.3644 - ac\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3591 - accuracy: 0.8592:  - ETA: 1:02 - loss: 0.3595 - accuracy - ETA: 1: - ETA - ETA: 59s - loss: 0.3593 - accuracy - ETA: 45s - loss: 0.3612 - accuracy:  - ETA: 45s - loss: 0.3611 - accuracy: 0 - ETA: 45s - loss: 0.3611 - accuracy: 0.858 - ETA: 45s - loss: 0.3 - ETA: 4\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3534 - accuracy: 0.8618: 57s - loss: 0.3541 - ac - ET - ETA: 55s - loss: 0.3540 - accuracy: 0. - ETA: 55s - loss: 0.3540 - accuracy: 0.861 - ETA: 54s - loss: 0.3540 - accuracy: 0. - ETA: 54s - ETA: 53s - loss: 0.3540 - accuracy: 0.861 - ETA - ETA: 51s - loss: 0.3540 - a - ETA: 50s - loss: 0.3538 - a - ETA: 49s - loss: 0.3542 - accu - ETA: 48s - loss: 0.3542 -  - ETA: 48s - loss: 0.3540 - - ETA: 47s - loss: 0. - ETA: 39s - loss: 0.3541 - ac - ETA: 38s - loss: 0.3540 - accuracy: 0.8 - ETA: 38s - loss: 0.3539 - accuracy:   - ETA: 35s - ETA: 34s - loss: 0.3545 - accuracy:  - ETA: - ETA: 20s - loss: 0.  - ETA: 12s - loss: 0.3535  - - ETA: 9s - loss: 0.3534 - accuracy:  - - ETA: 8s - loss: 0.3 - ETA: 4s - l - ETA: 0s - loss: 0\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3491 - accuracy: 0.8639: 1:20 - loss: - E - ETA: 1:17 - loss - ETA: 58s - loss: 0.3 - ETA: 57s - loss: 0.3496 - accuracy: 0.8 - ETA: 57s - loss: 0.3497  - ETA: 56s -  - ETA: 54s - loss: 0.3495 - accuracy: - ETA: 54s - loss: 0.3495 - accuracy: 0. - ETA: 54s - loss: 0.3494  - ETA: 53s - loss: 0.3495 - a - ETA: 5 - E - ETA: 48s - loss: 0.3495 - accurac - ETA: 47s  - ETA: 39s - loss: 0.3498 - accuracy: 0.863 - ETA: 39s - loss: 0.3498 - accuracy: 0.86 - ETA: 39s - loss: 0.3497 - accuracy: 0.863 - ETA: - ETA: 37s - loss: 0. - ETA: 36s - loss: 0.3498 - a - ETA: 35s - loss: 0.3496 - acc - ETA: 30s - loss: 0.3493  - ETA: 0s - loss: 0.3491 - accura - ETA: 0s - loss: 0.3491 - accuracy: 0.\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3455 - accuracy: 0.8660: 58s - loss: 0.3464 - accuracy: 0.86 - ETA: 58s - loss: 0.3465  - ETA: 57s - loss: 0.3467 - accura - ETA: 57s - loss:  - ETA: 55s - loss: 0.34 - ETA: 45s - loss: 0.3468 - ac - ETA: 44s - loss: - ETA: 43s - loss: 0.3468 - accuracy: 0. - ETA: 43s -  - ETA: 41s - loss: 0.3467 - accurac - ETA: 41s - loss: 0.3468 - accura - ETA: 40s - loss: 0.3469 - accurac - ETA: 40s - - ETA: 38s - loss: 0.3466 - accuracy: 0.865 - ETA: - ETA: 36s - loss: 0.3467 - acc - ETA: 36s - loss: 0.3468 - accuracy: - ETA: 35s - loss: 0.3466 -  - ETA: 34s - loss: 0.3465 - ETA: 34s - loss: 0.346 - ETA: 32s - loss: - ETA: 31s - loss: 0.3463 - ac - ETA: 30s -  - ETA: 27s - loss: 0.3461 - accur - ETA: 26s - loss: 0.3461 - accuracy: 0.865 - ETA: 26s - loss: 0.3461 -  - ETA: 25s - loss: 0.3461  - ETA: 24s - loss: 0.3461 - accura - ETA: 24s - loss: 0.3461 - accuracy - ETA: 23s - loss: 0.3461 - accurac - ETA: 16s - loss: 0.3460 - accuracy:  - ETA: 16s - lo - ETA: 14s - loss: 0.3459 -  - ETA: 13s - loss: 0.3458 - accuracy: 0 - ETA: 13s - loss: 0.3458 - accuracy - ET - ETA: 11s - loss: 0.3457 - acc - ETA: 10s - loss: 0.3457 - accuracy: 0.86 - ETA: 10s - lo - ETA: 8s - loss:\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3431 - accuracy: 0.8673: 1:11 - loss: 0.342 - ETA: 55s - los - ETA: 54s - loss: 0.34 - ETA: 53s - loss - ETA: 49 - ETA: - ETA: 46s - loss: 0.3446 - accur - ETA: 45s - loss: 0.34 - ETA: 42s -  - ETA: 40s - loss: 0.3 - ETA: 39s - loss: 0.3441 - acc - ETA: 38s - loss: 0.3442 - accuracy: 0. - ETA: 38s - loss: 0.3442 - a - ETA: 37s  - ETA: 36s - loss - ETA: 30s - loss: 0.3443 - accuracy:  - ETA: 30s - loss: 0.3443 - accuracy: 0.86 - ETA: 29s - loss: 0.3443 - a - ETA: 29s  - ETA: 27s - loss:  - ETA: 26s - loss: 0 - ETA: - ETA: 0s - l\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3421 - accuracy: 0.8689: 1: - ETA: 1:09 - loss: 0.3466 - accuracy: 0.86  - ETA: 55s - loss:  - ETA: 53s - loss: 0. - ETA: 43s - loss: 0.3413 - accuracy:  - ETA: 43s - loss: 0.3412 - accuracy: 0. - ETA: 42s - loss: 0.3413 - - ETA: 42s - loss: 0.3411 - accuracy: 0.868 - E  - ETA:  - ETA - ETA: 22s - loss: 0.3435 - accuracy: 0.86 - ETA: 22s - loss: 0.3435  - ETA: 21s - loss: 0.3 - ETA: 2 - ETA: 6s - loss: 0.3426 - accuracy: 0.86 - ETA: 6s - loss: 0.3426 - accuracy: 0. - ETA: 6s - loss: 0.3 - ETA - ETA: 3s - loss: 0.3424  - ETA: \n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3377 - accuracy: 0.8696: 1:21 - loss: 0.3377  - ETA: 1:20 - loss: 0.3356  - ETA: 1:19 - los - ETA: 1:11 - - ETA: 59s - loss: 0.3363 - acc - ETA: 59s - los - ETA: 57s - loss: 0.3391 - a - ETA: 56s - loss - ETA: 55s - loss: 0.3390 -  - ETA: 54s - loss: 0.3388 - accuracy: 0. - ETA: 54s - loss: 0.3387 - a - ETA:  - ETA: 37s - loss: 0.3 - ETA: 36s - loss: 0.3381 - accuracy: 0 - ETA: 36s - loss: 0.3381 -  - ETA: 35s - lo - ETA: 34s - loss: 0.33 - ETA: 33s - loss: 0.3380 - accuracy: 0. - ETA: 33s - loss: 0.3380 - accuracy: 0. - ETA: 32s - loss: 0.3380 - accuracy:  - ETA: 32s - loss: 0.3380 - accuracy - ETA: 32s - loss: 0.3380 - accuracy: - ETA: 31s - loss: 0.3379 - a - ETA: 31s - loss: 0.3378 -  - ETA: 30s - loss: 0.3 - ETA: 29s - loss: 0.3375 - acc - ETA: 28s - loss: 0.3375 - accuracy: 0.869 - ETA: 28s - loss: 0.3375 - accura - ETA: 27s - loss: 0.3377 - ETA: 26s - loss: 0.3375 - accuracy: 0.8 - ETA: 26s - loss: 0.337 - ETA: 25s - loss: 0.3375 - accuracy: 0.86 - ETA: 25s - loss: 0.3376 - accura - ETA: 25s - loss: 0 - ETA: 23s - loss: 0.3374 -  - ETA: - ETA: 21s - loss: 0.3376 - ETA: 20s - loss: 0.3376 - acc - ETA:  - ETA: 17s - loss: 0.3377 - acc - ETA: 17s - loss: 0.3376 - accuracy: 0. - ETA: 16s - loss: 0.3375 - accuracy: 0.8 - ETA: 16s - loss: 0.3375 - accuracy: - ETA: 16s - loss: 0.3376 - ETA: 15s - loss: 0.3374 - accuracy: 0.869 - ETA: 15s - loss: 0.3374 - accuracy:  - ETA: 15s - loss: 0. - ETA: 13s - - ETA: 10s - lo - ETA: 3s - loss: 0 - ETA: 3s - loss: 0.3375 - ac - E - ETA: 0s - loss: 0\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3362 - accuracy: 0.8710: 54s - loss: 0.3334 - accuracy: 0 - ETA: 54s - loss: 0.3334 - accuracy: 0.8 - ETA: 53s - loss: 0.3334 - accuracy: 0.871 - ETA: 53s - loss: 0.3335 - ac - ETA: 53s - loss: 0.3335 - ac - ETA: 52s - loss: 0.3336 - accu - ETA: 51s - loss: 0.3337 - accurac - ET - ETA: 49s - loss: 0. - ETA: 48s - loss: 0.3336 - accuracy: 0. - ETA: 47s - loss: 0.3337 - accuracy: 0.871 - ETA: 47s - l - ETA: 46s - loss: 0 - ETA: 45s - loss: 0.33 - ETA: 44s - loss: 0.3338 - accuracy: 0 - ETA: 43s - loss: 0.3337 - accuracy: - ETA: 43s - loss: 0.3338 - accuracy: 0.8 - ETA: 43s - loss: 0.3338 - accuracy: - ETA: 42s - loss:  - ETA: 41s - loss - ETA: 40s - loss: 0 - ETA: 39s - l - ETA: 37s - loss: 0.3341 - accuracy:  - ETA: 37s - loss: 0.3342 - accuracy: 0.87 - ETA: 37s - loss: 0.3342 - accuracy: 0.871 - ETA: 37s - loss: 0.3342 - accuracy - ETA: - ETA: 34s - lo - ETA: 33s - loss: 0.3363 - - ETA - ETA: - ETA: 28s - loss:  - ETA: 27s - loss: 0.3359 - acc - ETA: 26s - loss: 0.3359 - accuracy: 0.8 - ETA: 26s  - ETA: 2 - ETA: 23s - loss: 0.3360 - acc - ETA: 22s - loss: 0.3359 - accuracy - ETA: 21s - loss: 0.3359 - accuracy: 0.8 - ETA: 21s - loss: 0.3360 - accuracy - ETA: 21s - loss: - ETA: 20s - loss: 0.3362 -\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3336 - accuracy: 0.8718: 1:23 - los - ETA: 1:21 - l - ETA: 1:20 - loss: 0.3359 - ac - ETA: 1:16 - loss: 0.3346 - accu - ETA: 1: - ETA: 1:14 - loss: - - E - ETA: 1:11 - loss: 0.3338 - accuracy: 0.87 - ETA: 1: - ETA: 1:07 - - ETA: 1:07 - loss: 0.3 - ETA: 1:06 - loss: 0.3339 - ac - ETA: 1:00 - loss: 0.3342 - a - ETA: 59s - loss: 0.3339 - accuracy: 0.87 - ETA: 59s - lo - ETA: 57s - loss: 0.3337 - accuracy: 0. - ETA: 57s - lo - ETA: 56s - loss: 0.33 - ETA: 55s - loss: 0.3340 - accuracy - ETA: 54s - los - ETA: 48s - loss: 0.3340 - ETA: 47s - loss: 0.3339 - accura - ETA: 47s - loss - ETA: 45s - loss: 0.3337 - acc - ET - ETA: 29s - loss:  - ETA: 5s - loss: 0.3337 -  - - ETA: 3s - loss: 0.3336 - accuracy:  - ETA: 3s - loss: 0.3336  - ETA: 0s - loss: 0.3336 - accuracy: 0.87\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.3313 - accuracy: 0.8731: 1: - ETA: 1:00 - loss:  - ETA: 58s - loss: 0.3305 - accurac - ETA: 55s - loss: 0 - ETA: 54s - loss: 0.3308 - accuracy: 0.8 - ETA: 53s - loss: - ETA: 52s - loss: 0.3305 - accuracy:  - ETA: 45s - loss: 0.3309 - accurac - ET - ETA: 38s - loss: 0.3315 -  - ETA: 35s - loss: 0.3313 - accur - ETA: 34s - loss: 0.3314 - ETA: 33s - loss: - ETA: 25s - - ETA: 23s - loss: 0.33 - ETA: 22s - loss: - ETA: 21s - loss: 0.3314 - a - ETA: 20s - loss: 0.3313 - accuracy: - ETA: 18s  - ETA: 14s - loss: 0.3317 - ETA: 13s - loss: 0.3317 - accuracy: 0.873 - ETA: 13s - loss: 0.3317 - accuracy - ETA: 12s - loss: 0.3317 - accuracy: 0.873 - ETA: 12s - loss: 0.3317 - - ETA: 11s - loss: 0.3316 - accuracy: 0.8 - - ETA: 9s - loss: 0.3318 - accura - ETA: 8s - loss: 0.3318 - accuracy: 0. - ETA: 8s - los - ETA: 5s - loss: 0.3317 -  - ETA: 2s - - ETA:  - ETA: 1s - loss: 0.3314 - accuracy: 0. - ETA: 0s - ETA: 0s - loss: 0.3313 - accuracy: 0.\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3331 - accuracy: 0.8741: 1:10 - loss: - ETA: 1:09 - loss: 0.332 - ETA: 1:08 - - ETA: 1:06 - l - ETA: 1:02 - loss: 0.3300 - accuracy:  - ETA: 1 - - ETA: 55s - ETA: 54s - loss: 0.3297 - a - ETA: 53s - loss: 0.3399 - a - ETA: 52s - loss: 0.3398 - a - ETA: 49s - loss: 0.3382 - - ETA: 48s - loss: 0.3378 -  - ETA: 47s - loss: 0.3372 - a - ETA: 47s - loss: 0.3372 - accuracy: 0.87 - ETA: 46s - loss: 0.3372 - acc - - ETA: 37s - loss: - ETA: 33s - loss: 0.3 - ETA: 32s - loss: 0.3362 - ac - ETA: 31s - loss: 0.3360 - accura - ETA - ETA: 27s - loss: 0.3355 - accuracy: 0.87 - ETA: 24s - loss: 0.3352 - ETA: 23s - loss: 0. - ETA: 22s - loss: 0 - ETA: 21s - loss: 0.3346 - accuracy - ETA: 20s - loss: 0.3345 -  - ETA: 20s - loss: 0.3343 -\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3338 - accuracy: 0.8747: 59s - los - ETA: 58s - loss: 0. - ETA: 57s - loss: 0.3268 - - ETA: 56s - loss: 0.3270 - acc - ETA: 55s - loss: 0.3271 - accurac - ETA: 55s - loss: 0.3270 - ac - - ETA: 38s - loss: 0.3353 - accuracy: 0.874 - ETA: 38s - loss: 0.33 - - ETA: 30s - loss: 0.3355  - ETA: 29s - loss: 0.3353 - accuracy: 0.8 - ETA: 29s - ETA: 27s - loss: 0.3350 - accurac - ETA: 27s - los - ETA: 26s - loss: 0.3346 - accuracy: - ETA: 25s - los - ETA: 24s - loss: 0. - ETA: 23s - l - ETA: 8s - loss: 0.333 - ETA: 8s - loss: 0.3330 - accu - ETA: 8s - loss: 0.3330 - accuracy: 0.87 - ETA: 8s - loss: 0.3330 - accuracy: 0.87 - ETA: 8s - loss: - ETA: 0s - loss: 0.3339 -  - ETA: 0s - loss: 0.3338 - \n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3268 - accuracy: 0.8755\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.3259 - accuracy: 0.8764\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.3243 - accuracy: 0.8770\n",
      "1342568/1342568 [==============================] - 12s 9us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.3490 - accuracy: 0.8712\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2843 - accuracy: 0.8950\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2697 - accuracy: 0.9004\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 82s 30us/step - loss: 0.2624 - accuracy: 0.9033\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2577 - accuracy: 0.9054\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 82s 30us/step - loss: 0.2541 - accuracy: 0.9070\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2506 - accuracy: 0.90872s - l - ETA: 0s - loss:\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2485 - accuracy: 0.9093\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2455 - accuracy: 0.9104\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2494 - accuracy: 0.91130s - loss: 0.2495 - accuracy\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2437 - accuracy: 0.9116\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2415 - accuracy: 0.9123\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 81s 30us/step - loss: 0.2402 - accuracy: 0.9129\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 82s 30us/step - loss: 0.2402 - accuracy: 0.9136: 22s - loss: 0.2399 - accuracy: 0 - ETA: 22 - ETA: 20s - loss: 0.2399 - accu - ETA: 19s - loss: 0.\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2384 - accuracy: 0.9142:  - ETA: 1:28 - loss: 0.2371  - ETA: 1:26 - - ETA: 1:11 - loss: 0 - ETA: 1:05 - loss: 0.2390  - - ETA: 59s - lo - ETA: 58s - loss: 0.2382 - accuracy: 0. - ETA: 58s - loss: 0. - ETA: 56s - loss: 0.2384 - accuracy: 0. - ETA: 56s - loss: 0.2 - ETA - ETA: 48s - loss: 0.2382 - accuracy: 0.9 - ETA: 48s - loss:   - ETA: - ETA: 34s - loss: 0.2380 - - ETA: 33s - loss: 0.2381 - accuracy:  - ETA: 32s - lo - ETA: 22s - loss: 0.23 - ETA: 21s - loss: 0.2382 - accur - ETA: 20s - loss: 0.2 - ETA: 19s - loss: 0.2382 - accuracy: 0 - ETA: 19s - loss: 0.2381 - accura - ETA: 18s - loss: 0.238 - ETA: - E - ETA - ETA: 0s - l\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2365 - accuracy: 0.9150: 1:07 - ETA: 1:05 - loss: 0.2383 - accuracy - ETA: 1:05 - loss: 0.2380 - accuracy:  - ETA: 1:04 - loss: 0.2379 - accuracy - ETA:  - ETA: 1:01 - l - - ETA: 56s - loss: 0.2374 - accuracy: 0.91 - ETA: 56s - los - ETA: 55s - loss: 0.2370 - accuracy - ETA: 54s - loss: 0.2371 - a - ETA: 53s - loss: 0.2369 - accur - ETA: 53s - loss: 0.2367 - E - ETA: 43s - loss: 0.2367 - accurac - ETA: 43s - los - ETA: 41s - loss: 0.2366 - accuracy: 0.915 - ETA: - ETA: 32s - ETA: 1s - loss: 0.236 - ETA: 0s - l\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2353 - accuracy: 0.9156:  - ETA: 1:09 - loss: - ETA: 1:08 - ETA: 46s - loss: 0.2332 -  - ETA: 46s - loss: 0.2331 - accuracy: 0 - ETA: 45s - loss: 0.2331 - ETA: 44s - l - ETA: 43s - los - ETA: 39 - ETA: 37s - loss: 0.2339 - accura - ETA: 37s - loss: 0.23 - ETA: 36s - loss: 0.2338  - ETA: 3 - ETA: 31s - loss: 0.2351  - ETA: 30s - loss: 0.2350 - accur - ETA: 29s - loss: 0.2350 - accuracy:  - ETA: 29s - loss: 0.2350 -  - ETA: 28s - loss - ET - ETA: 22s - loss: 0.2355 - accuracy: 0.91 - ETA: 22s - loss: 0.2 - ETA: 21s - loss: 0.2355 - accuracy: 0 - ETA: 21s - loss: 0.2355 - a - ETA: 20s - loss: 0.2 - ETA: 19s - loss - ETA: 18s - loss: 0.2354 - accuracy - ETA: 17 - ETA: 16s - loss: 0.235 - ETA: 15s - loss: 0.2353 - accuracy: 0 - ETA: 14s - loss: 0.2353 - accuracy: - ETA: 14s - loss: 0.2354 - ac - ETA: 13s - loss: 0.2353 - ETA: 12s - loss: 0.2 - ETA: 11s - loss: 0.2353 - accuracy: 0.91 - ETA: 11s - loss: 0.2353 - ac - ETA: 10s - loss: 0.2352 - accuracy: 0. - ETA: 1s - loss: 0.2353 - ac - ETA: 0s - loss: - ETA: 0s - loss: 0.2353 - ac\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2366 - accuracy: 0.9155TA: 56s -  - ETA: 54s - loss: 0.2363 - accura - ETA: 53s - l - ETA: 52s - loss: 0.2370 - acc - ETA: 51s - - ETA: 50s - loss: 0.2370 - accuracy: 0.915 - ETA: 49s - loss: 0.2370 - accuracy: 0 - ETA: 49s - los - ETA: 48s - loss: 0.2366 - accuracy - ETA: 47s - loss: 0.2365 - acc - ETA: 47s - loss: 0.2365 - accu - ETA: 46s  - - ETA: 42s -  - ETA: 41s - loss: 0.2 - ETA: 37s - loss: 0.2360 - accuracy: 0. - ETA: 37s - loss: 0.2360 - accuracy: 0.9 - ETA: 37s - - ETA: 35s - loss: 0.2 - ETA: 34s - loss: 0.2356 - accur - ETA: 34s - loss: 0.2356 - accuracy:  - ETA: 3 - ETA: 29s - loss: 0.2365 - accuracy: 0.91 - ETA: 29s - loss: 0.2365 - - ETA: 28s - loss: 0.2368  - ETA: 27s - loss: 0.2368 - accuracy: 0.91 - ETA: 27s - loss: 0.2368 - - ETA: 15 - ETA: 0s - los\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2374 - accuracy: 0.9161: 57s  - ETA: 55s - loss: 0.2349 - accuracy: 0.91 - ETA: 55s - loss: 0.2350 - - ETA: 49s - loss: 0.2350 - accuracy: 0.916 - ETA: 49s - loss: 0.2351 - accurac - ETA: 49s - loss: 0.2352 - accuracy: 0.9 - ETA: 49s - loss: 0.2353 - accuracy: 0. - ETA: 46s - loss: 0.2356 - accuracy: 0.9 - ETA: 46s - loss: 0.2355 - accuracy: 0. - ETA: 46s - loss: 0 - ETA: 45s - loss: 0. - ETA: 41s - loss: 0.2355 - accur - ETA: 41s - loss: 0.2356 - accuracy: 0. - ETA: 40s - loss: 0.2356 - accuracy: 0. - ETA: 40 - ETA: 38s - loss: 0.2357 - accuracy: 0.91 - ETA: 38s - loss: 0.2 - ETA: 35s - loss: 0.2354 - - ETA: 27s - loss: 0.23 - ETA: 26s - ETA: 25s - - ETA: 23s - loss: 0.2 - ETA: 22s - loss: 0.2350 - accuracy: 0.9 - ETA: 2 - ETA: 20s - los - ETA: 16s - loss: 0.2365 - accurac - ETA: 16s - loss: 0.2364 - accuracy: 0.916 - ETA: 16s - loss:  - ET - ETA: 3s - loss: 0.2375 - accura\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 80s 30us/step - loss: 0.2336 - accuracy: 0.9169: 59s - loss: 0.2320 - accuracy: 0. - ETA: 58s - loss: 0.2320 - accura - ETA: 58s - loss: 0.2321 - accuracy: 0. - ETA: 58s - loss: 0.2322 - accuracy: 0. - ETA: 58s -  - ETA: 54s - loss: 0.2323  - ETA: 0s - loss: 0.2336 - accuracy: 0.91 - ETA: 0s - loss: 0.2336 - accuracy - ETA: 0s - loss:\n",
      "1342568/1342568 [==============================] - 11s 8us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.1185 - accuracy: 0.9586\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.0703 - accuracy: 0.9761\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0591 - accuracy: 0.9802\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0525 - accuracy: 0.9826\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0490 - accuracy: 0.9839\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0463 - accuracy: 0.9853\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0429 - accuracy: 0.9864\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0406 - accuracy: 0.9872\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0401 - accuracy: 0.9878\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 101s 37us/step - loss: 0.0389 - accuracy: 0.9883\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 101s 37us/step - loss: 0.0380 - accuracy: 0.9886\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0390 - accuracy: 0.9886\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0369 - accuracy: 0.9891\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0379 - accuracy: 0.9892\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0376 - accuracy: 0.9894\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0364 - accuracy: 0.9896\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0382 - accuracy: 0.9896\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0391 - accuracy: 0.9896\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0372 - accuracy: 0.9896\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 100s 37us/step - loss: 0.0391 - accuracy: 0.9898\n",
      "1342568/1342568 [==============================] - 16s 12us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.4577 - accuracy: 0.8085\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 104s 39us/step - loss: 0.3724 - accuracy: 0.8518\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.3458 - accuracy: 0.8648\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.3314 - accuracy: 0.8722\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 104s 39us/step - loss: 0.3207 - accuracy: 0.8767\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.3125 - accuracy: 0.8812\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.3051 - accuracy: 0.8843\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 104s 39us/step - loss: 0.3002 - accuracy: 0.8867\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.2960 - accuracy: 0.8886\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.2930 - accuracy: 0.8905\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.2886 - accuracy: 0.8921\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.2869 - accuracy: 0.8933\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.2851 - accuracy: 0.8939\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.2823 - accuracy: 0.8958\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.2790 - accuracy: 0.8968\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 104s 39us/step - loss: 0.2774 - accuracy: 0.8977\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.2760 - accuracy: 0.8985\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 104s 39us/step - loss: 0.2756 - accuracy: 0.8989\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 103s 39us/step - loss: 0.2724 - accuracy: 0.8999\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 103s 38us/step - loss: 0.2718 - accuracy: 0.9005\n",
      "1342568/1342568 [==============================] - 16s 12us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.3378 - accuracy: 0.8759\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2702 - accuracy: 0.9005\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2512 - accuracy: 0.9080\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2405 - accuracy: 0.9121\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2342 - accuracy: 0.9151\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2289 - accuracy: 0.9171\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2235 - accuracy: 0.9191\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2220 - accuracy: 0.9202\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2203 - accuracy: 0.9214\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2180 - accuracy: 0.9226\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2175 - accuracy: 0.9229\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2155 - accuracy: 0.9241\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2230 - accuracy: 0.9247\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2116 - accuracy: 0.9255\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2110 - accuracy: 0.9261\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2110 - accuracy: 0.9263\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2085 - accuracy: 0.9268\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2102 - accuracy: 0.9275\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2049 - accuracy: 0.9280\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 101s 38us/step - loss: 0.2056 - accuracy: 0.9286\n",
      "1342568/1342568 [==============================] - 16s 12us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 345s 129us/step - loss: 0.1176 - accuracy: 0.9591\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 349s 130us/step - loss: 0.0684 - accuracy: 0.9769\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 349s 130us/step - loss: 0.0563 - accuracy: 0.9812\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 348s 129us/step - loss: 0.0501 - accuracy: 0.9836\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 349s 130us/step - loss: 0.0452 - accuracy: 0.9853\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0425 - accuracy: 0.9866\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0414 - accuracy: 0.9874\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 353s 131us/step - loss: 0.0401 - accuracy: 0.9881 - loss: 0.0401 - accuracy: 0.98 - ETA: 1s - loss: 0.0401 \n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0384 - accuracy: 0.9888\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 353s 131us/step - loss: 0.0371 - accuracy: 0.9894\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0362 - accuracy: 0.9896\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 353s 131us/step - loss: 0.0358 - accuracy: 0.9899\n",
      "Epoch 13/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0365 - accuracy: 0.9901\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0366 - accuracy: 0.9902\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 353s 131us/step - loss: 0.0365 - accuracy: 0.9904\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 352s 131us/step - loss: 0.0360 - accuracy: 0.9906\n",
      "Epoch 17/20\n",
      "2685136/2685136 [==============================] - 26187s 10ms/step - loss: 0.0384 - accuracy: 0.9906\n",
      "Epoch 18/20\n",
      "2685136/2685136 [==============================] - 393s 146us/step - loss: 0.0366 - accuracy: 0.9911\n",
      "Epoch 19/20\n",
      "2685136/2685136 [==============================] - 393s 146us/step - loss: 0.0367 - accuracy: 0.9915\n",
      "Epoch 20/20\n",
      "2685136/2685136 [==============================] - 393s 146us/step - loss: 0.0399 - accuracy: 0.9915\n",
      "1342568/1342568 [==============================] - 23s 17us/step\n",
      "Epoch 1/20\n",
      "2685136/2685136 [==============================] - 363s 135us/step - loss: 0.4522 - accuracy: 0.8113\n",
      "Epoch 2/20\n",
      "2685136/2685136 [==============================] - 356s 132us/step - loss: 0.3655 - accuracy: 0.8553\n",
      "Epoch 3/20\n",
      "2685136/2685136 [==============================] - 596s 222us/step - loss: 0.3377 - accuracy: 0.8685\n",
      "Epoch 4/20\n",
      "2685136/2685136 [==============================] - 417s 155us/step - loss: 0.3206 - accuracy: 0.8761\n",
      "Epoch 5/20\n",
      "2685136/2685136 [==============================] - 443s 165us/step - loss: 0.3101 - accuracy: 0.8809\n",
      "Epoch 6/20\n",
      "2685136/2685136 [==============================] - 384s 143us/step - loss: 0.3024 - accuracy: 0.8847\n",
      "Epoch 7/20\n",
      "2685136/2685136 [==============================] - 377s 140us/step - loss: 0.2949 - accuracy: 0.8883\n",
      "Epoch 8/20\n",
      "2685136/2685136 [==============================] - 366s 136us/step - loss: 0.2906 - accuracy: 0.8907\n",
      "Epoch 9/20\n",
      "2685136/2685136 [==============================] - 379s 141us/step - loss: 0.2843 - accuracy: 0.8934\n",
      "Epoch 10/20\n",
      "2685136/2685136 [==============================] - 371s 138us/step - loss: 0.2811 - accuracy: 0.8952\n",
      "Epoch 11/20\n",
      "2685136/2685136 [==============================] - 371s 138us/step - loss: 0.2792 - accuracy: 0.8962\n",
      "Epoch 12/20\n",
      "2685136/2685136 [==============================] - 377s 141us/step - loss: 0.2740 - accuracy: 0.8982\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2685136/2685136 [==============================] - 382s 142us/step - loss: 0.2727 - accuracy: 0.8991\n",
      "Epoch 14/20\n",
      "2685136/2685136 [==============================] - 365s 136us/step - loss: 0.2709 - accuracy: 0.9005\n",
      "Epoch 15/20\n",
      "2685136/2685136 [==============================] - 372s 139us/step - loss: 0.2665 - accuracy: 0.9018\n",
      "Epoch 16/20\n",
      "2685136/2685136 [==============================] - 371s 138us/step - loss: 0.2653 - accuracy: 0.9026\n",
      "Epoch 17/20\n",
      "2460064/2685136 [==========================>...] - ETA: 32s - loss: 0.2618 - accuracy: 0.9039"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-94bfd8b5f534>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# fit the grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mhot_y_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgrid_search_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhot_y_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m# best params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best Score \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"with \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    839\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1286\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1288\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    807\u001b[0m                                    (split_idx, (train, test)) in product(\n\u001b[1;32m    808\u001b[0m                                    \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m                                    enumerate(cv.split(X, y, groups))))\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    833\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    752\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 256\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, sample_weight, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sample_weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKerasClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/keras/wrappers/scikit_learn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mfit_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \"\"\"\n\u001b[0;32m-> 1551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1591\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/usr/local/anaconda3/envs/ECE542/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# hyperparameter tuning\n",
    "tuned_model = KerasClassifier(build_fn = simpleNN, epochs=20, verbose = 1)\n",
    "# define hyperparameter options\n",
    "num_hidden_layers = [1, 2, 3, 4]\n",
    "num_neurons = [100, 250, 500]\n",
    "param_grid = dict(num_hidden_layers = num_hidden_layers, num_neurons = num_neurons)\n",
    "# prepare the grid\n",
    "grid = GridSearchCV(estimator = tuned_model, param_grid = param_grid, cv=3)\n",
    "# fit the grid\n",
    "hot_y_train = np_utils.to_categorical(y_train)\n",
    "grid_search_result = grid.fit(X_train, hot_y_train)\n",
    "# best params\n",
    "print(\"Best Score \" + str(grid_search_result.best_score_) + \"with \" + str(grid_search_result.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "hundred-circus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4027704\n",
      "4027704\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "earned-schema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3020778 samples, validate on 1006926 samples\n",
      "Epoch 1/20\n",
      "3020778/3020778 [==============================] - 107s 35us/step - loss: 0.4084 - accuracy: 0.8340 - val_loss: 0.3540 - val_accuracy: 0.8596\n",
      "Epoch 2/20\n",
      "3020778/3020778 [==============================] - 106s 35us/step - loss: 0.3329 - accuracy: 0.8684 - val_loss: 0.3161 - val_accuracy: 0.8772\n",
      "Epoch 3/20\n",
      "3020778/3020778 [==============================] - 105s 35us/step - loss: 0.3140 - accuracy: 0.8771 - val_loss: 0.3177 - val_accuracy: 0.8779\n",
      "Epoch 4/20\n",
      "3020778/3020778 [==============================] - 109s 36us/step - loss: 0.3053 - accuracy: 0.8814 - val_loss: 0.2986 - val_accuracy: 0.8839\n",
      "Epoch 5/20\n",
      "3020778/3020778 [==============================] - 105s 35us/step - loss: 0.3020 - accuracy: 0.8832 - val_loss: 0.3074 - val_accuracy: 0.8834\n",
      "Epoch 6/20\n",
      "3020778/3020778 [==============================] - 106s 35us/step - loss: 0.2978 - accuracy: 0.8854 - val_loss: 0.2994 - val_accuracy: 0.8853\n",
      "Epoch 7/20\n",
      "3020778/3020778 [==============================] - 107s 35us/step - loss: 0.2959 - accuracy: 0.8869 - val_loss: 0.3092 - val_accuracy: 0.8819\n",
      "Epoch 8/20\n",
      "3020778/3020778 [==============================] - 108s 36us/step - loss: 0.2930 - accuracy: 0.8885 - val_loss: 0.3087 - val_accuracy: 0.8835\n",
      "Epoch 9/20\n",
      "3020778/3020778 [==============================] - 106s 35us/step - loss: 0.2882 - accuracy: 0.8909 - val_loss: 0.2901 - val_accuracy: 0.8916\n",
      "Epoch 10/20\n",
      "3020778/3020778 [==============================] - 113s 37us/step - loss: 0.2865 - accuracy: 0.8918 - val_loss: 0.2817 - val_accuracy: 0.8954\n",
      "Epoch 11/20\n",
      "3020778/3020778 [==============================] - 102s 34us/step - loss: 0.2829 - accuracy: 0.8936 - val_loss: 0.3264 - val_accuracy: 0.8783\n",
      "Epoch 12/20\n",
      "3020778/3020778 [==============================] - 105s 35us/step - loss: 0.2801 - accuracy: 0.8948 - val_loss: 0.2843 - val_accuracy: 0.8919\n",
      "Epoch 13/20\n",
      "3020778/3020778 [==============================] - 106s 35us/step - loss: 0.2784 - accuracy: 0.8961 - val_loss: 0.2873 - val_accuracy: 0.8961\n",
      "Epoch 14/20\n",
      "3020778/3020778 [==============================] - 107s 36us/step - loss: 0.2756 - accuracy: 0.8970 - val_loss: 0.2805 - val_accuracy: 0.8974\n",
      "Epoch 15/20\n",
      "3020778/3020778 [==============================] - 111s 37us/step - loss: 0.2735 - accuracy: 0.8982 - val_loss: 0.2682 - val_accuracy: 0.8996\n",
      "Epoch 16/20\n",
      "3020778/3020778 [==============================] - 107s 35us/step - loss: 0.2722 - accuracy: 0.8988 - val_loss: 0.2723 - val_accuracy: 0.8991\n",
      "Epoch 17/20\n",
      "3020778/3020778 [==============================] - 107s 35us/step - loss: 0.2709 - accuracy: 0.8992 - val_loss: 0.2849 - val_accuracy: 0.8917\n",
      "Epoch 18/20\n",
      "3020778/3020778 [==============================] - 108s 36us/step - loss: 0.2694 - accuracy: 0.8998 - val_loss: 0.3018 - val_accuracy: 0.8846\n",
      "Epoch 19/20\n",
      "3020778/3020778 [==============================] - 107s 35us/step - loss: 0.2689 - accuracy: 0.9004 - val_loss: 0.2729 - val_accuracy: 0.9015\n",
      "Epoch 20/20\n",
      "3020778/3020778 [==============================] - 105s 35us/step - loss: 0.2712 - accuracy: 0.9010 - val_loss: 0.2658 - val_accuracy: 0.8999\n"
     ]
    }
   ],
   "source": [
    "# Training and Testing after Oversampling\n",
    "# split into train and validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.25)\n",
    "hot_y_train = np_utils.to_categorical(y_tr)\n",
    "hot_y_val = np_utils.to_categorical(y_val)\n",
    "# create the model\n",
    "# base_model = simpleNN(grid_search_result.best_params_['num_hidden_layers'],grid_search_result.best_params_['num_neurons']) # columns-2 features and 4 possible classes\n",
    "base_model = simpleNN(2, 100)\n",
    "# fit the keras model on the dataset\n",
    "hist = base_model.fit(X_tr, hot_y_train, batch_size=32, epochs=20, validation_data=(X_val, hot_y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "checked-dollar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "accredited-enforcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.save(\"base_model_nn.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "diverse-regression",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_012_01__x.csv\n",
      "subject_009_01__x.csv\n",
      "subject_010_01__x.csv\n",
      "subject_011_01__x.csv\n"
     ]
    }
   ],
   "source": [
    "# Create Features for Test Data\n",
    "data_folder = \"TestData/x/\"\n",
    "dest_folder = \"TestFeatures/\" \n",
    "create_features(data_folder, dest_folder, \"\", False, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "charming-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Label\n",
    "dest_folder = \"TestFeatures/\"\n",
    "test_session_ids = extract_session_ids(dest_folder)\n",
    "for i in range(0,len(test_session_ids)):\n",
    "    sid = test_session_ids[i]\n",
    "    x_path =  dest_folder + \"subject_\" + sid + \"__x.csv\"\n",
    "    y_path = \"TestLabel/\" + \"subject_\" + sid + \"__y.csv\"\n",
    "\n",
    "    x = pd.read_csv(x_path, header = None)\n",
    "    predY = base_model.predict(x)\n",
    "    y = np.argmax(predY, axis=1)\n",
    "    \n",
    "    pd.DataFrame(y).to_csv(y_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-athletics",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,classification_report\n",
    "# prediction result for validation set\n",
    "valPredY = base_model.predict(X_val)\n",
    "valPredY = np.argmax(valPredY, axis=1)\n",
    "# New Model Evaluation metrics\n",
    "print(\"Overall Test Performance:\")\n",
    "print('Accuracy Score : ' + str(accuracy_score(y_val,valPredY)))\n",
    "print('Precision Score : ' + str(precision_score(y_val,valPredY,average='micro')))\n",
    "print('Recall Score : ' + str(recall_score(y_val,valPredY,average='micro')))\n",
    "print('F1 Score : ' + str(f1_score(y_val,valPredY,average='micro')))\n",
    "print(\"classification Report:\")\n",
    "print(classification_report(y_val, valPredY, target_names=[\"Ground(0)\", \"Upstairs(1)\", \"Downstairs(2)\", \"Grass(3)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-uniform",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "documented-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample to match label frequency - time warping (downsample)\n",
    "warped_data_folder = \"warped_data/\"\n",
    "session_files = os.listdir(pp_tr_path)\n",
    "for file in session_files:\n",
    "    df = pd.read_csv(pp_tr_path + file)\n",
    "    warped_data = pd.DataFrame()\n",
    "    rows = len(df)\n",
    "    start = 1\n",
    "    while start < rows:\n",
    "        warped_data = warped_data.append(df.iloc[start], ignore_index=True, sort = False)\n",
    "        \n",
    "        start = start + 4\n",
    "        \n",
    "    warped_data.to_csv(warped_data_folder+file, index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "miniature-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warped_data/tr_session_002_01.csv\n",
      "warped_data/tr_session_002_03.csv\n",
      "warped_data/tr_session_002_02.csv\n",
      "warped_data/tr_session_002_05.csv\n",
      "warped_data/tr_session_002_04.csv\n",
      "warped_data/tr_session_004_02.csv\n",
      "warped_data/tr_session_004_01.csv\n",
      "warped_data/tr_session_008_01.csv\n",
      "warped_data/tr_session_006_01.csv\n",
      "warped_data/tr_session_001_08.csv\n",
      "warped_data/tr_session_006_02.csv\n",
      "warped_data/tr_session_006_03.csv\n",
      "warped_data/tr_session_003_02.csv\n",
      "warped_data/tr_session_001_07.csv\n",
      "warped_data/tr_session_001_06.csv\n",
      "warped_data/tr_session_003_03.csv\n",
      "warped_data/tr_session_003_01.csv\n",
      "warped_data/tr_session_001_04.csv\n",
      "warped_data/tr_session_001_05.csv\n",
      "warped_data/tr_session_001_01.csv\n",
      "warped_data/tr_session_001_02.csv\n",
      "warped_data/tr_session_001_03.csv\n",
      "warped_data/tr_session_005_01.csv\n",
      "warped_data/tr_session_007_04.csv\n",
      "warped_data/tr_session_005_03.csv\n",
      "warped_data/tr_session_005_02.csv\n",
      "warped_data/tr_session_007_03.csv\n",
      "warped_data/tr_session_007_02.csv\n",
      "warped_data/tr_session_007_01.csv\n"
     ]
    }
   ],
   "source": [
    "# fix data\n",
    "target_folder = 'warped_noheader_sorted/'\n",
    "warped_data_files = os.listdir(warped_data_folder)\n",
    "for file in warped_data_files:\n",
    "    if file != '.DS_Store':\n",
    "        print(warped_data_folder+file)\n",
    "        df_old = pd.read_csv(warped_data_folder+file)\n",
    "        df_new = df_old[['xa','ya','za','xg','yg','zg','label']]\n",
    "        df_new.to_csv(target_folder+file, index = False, header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "pending-little",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each data point -> time series\n",
    "def gen_time_series(window, data_folder, label_type = 'mid'): # label type can be middle or at the end\n",
    "    \n",
    "    x_arr = list()\n",
    "    y_arr = list()\n",
    "    \n",
    "    files = os.listdir(data_folder)\n",
    "    for file in files:\n",
    "        if file != '.DS_Store': \n",
    "            print(file)\n",
    "            df = pd.read_csv(data_folder+file)\n",
    "\n",
    "            df_len = len(df)\n",
    "            start = 0\n",
    "            while start + window - 1 < df_len:\n",
    "                data_window = df.iloc[start:start + window,0:6]\n",
    "                data_window = np.asarray(data_window)\n",
    "                x_arr.append(data_window)\n",
    "\n",
    "                ind = start + (window//2)\n",
    "                if label_type != 'mid':\n",
    "                    ind = start + window - 1\n",
    "\n",
    "                y_arr.append(int(df.iat[ind,6]))\n",
    "\n",
    "                start = start + 1\n",
    "            \n",
    "    X = np.asarray(x_arr)\n",
    "    Y = np.asarray(y_arr)\n",
    "    return X,Y # X and Y are numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "tender-neutral",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr_session_002_01.csv\n",
      "tr_session_002_03.csv\n",
      "tr_session_002_02.csv\n",
      "tr_session_002_05.csv\n",
      "tr_session_002_04.csv\n",
      "tr_session_004_02.csv\n",
      "tr_session_004_01.csv\n",
      "tr_session_008_01.csv\n",
      "tr_session_006_01.csv\n",
      "tr_session_001_08.csv\n",
      "tr_session_006_02.csv\n",
      "tr_session_006_03.csv\n",
      "tr_session_003_02.csv\n",
      "tr_session_001_07.csv\n",
      "tr_session_001_06.csv\n",
      "tr_session_003_03.csv\n",
      "tr_session_003_01.csv\n",
      "tr_session_001_04.csv\n",
      "tr_session_001_05.csv\n",
      "tr_session_001_01.csv\n",
      "tr_session_001_02.csv\n",
      "tr_session_001_03.csv\n",
      "tr_session_005_01.csv\n",
      "tr_session_007_04.csv\n",
      "tr_session_005_03.csv\n",
      "tr_session_005_02.csv\n",
      "tr_session_007_03.csv\n",
      "tr_session_007_02.csv\n",
      "tr_session_007_01.csv\n",
      "(334543, 30, 6)\n",
      "(334543,)\n"
     ]
    }
   ],
   "source": [
    "target_folder = 'warped_noheader_sorted/'\n",
    "X,Y = gen_time_series(30,target_folder)\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "sorted-boost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before oversampling\n",
      "Counter({0: 250863, 3: 51609, 2: 18267, 1: 13804})\n",
      "After Undersampling\n",
      "Counter({0: 13804, 1: 13804, 2: 13804, 3: 13804})\n",
      "(55216, 30, 6)\n",
      "(55216,)\n"
     ]
    }
   ],
   "source": [
    "# under sample\n",
    "counter = Counter(Y)\n",
    "print(\"Before oversampling\")\n",
    "print(counter)\n",
    "# create the undersampler\n",
    "undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "# transform the dataset\n",
    "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # this method takes only 2d array\n",
    "X, Y = undersample.fit_resample(X, Y)\n",
    "X, Y = undersample.fit_resample(X, Y)\n",
    "X, Y = undersample.fit_resample(X, Y)\n",
    "counter = Counter(Y)\n",
    "print(\"After Undersampling\")\n",
    "print(counter)\n",
    "\n",
    "X = X.reshape(X.shape[0], X.shape[1]//6, 6) # reshape to retain the original shape\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "refined-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "def simpleLSTM(timestep, features, lstm_neurons, rate_dropout, output_dim, lr):\n",
    "    # input layer -> LSTM -> output layer\n",
    "    \n",
    "    model_LSTM = Sequential()\n",
    "    # add input layer\n",
    "    model_LSTM.add(InputLayer(input_shape=(timestep, features)))\n",
    "    # add LSTM layer\n",
    "    model_LSTM.add(LSTM(units=lstm_neurons, activation='relu'))\n",
    "    # add dropout layer\n",
    "    model_LSTM.add(Dropout(rate_dropout))\n",
    "    # add dense layer\n",
    "    model_LSTM.add(Dense(output_dim, activation='softmax')) # output layer\n",
    "    # compile the keras model\n",
    "    model_LSTM.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    return model_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "extra-italy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               42800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4)                 404       \n",
      "=================================================================\n",
      "Total params: 43,204\n",
      "Trainable params: 43,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 44172 samples, validate on 11044 samples\n",
      "Epoch 1/50\n",
      "44172/44172 [==============================] - 12s 280us/step - loss: 0.4446 - accuracy: 0.8189 - val_loss: 0.2446 - val_accuracy: 0.9073\n",
      "Epoch 2/50\n",
      "44172/44172 [==============================] - 11s 252us/step - loss: 0.2260 - accuracy: 0.9163 - val_loss: 0.1757 - val_accuracy: 0.9361\n",
      "Epoch 3/50\n",
      "44172/44172 [==============================] - 11s 256us/step - loss: 0.1764 - accuracy: 0.9341 - val_loss: 0.1569 - val_accuracy: 0.9485\n",
      "Epoch 4/50\n",
      "44172/44172 [==============================] - 11s 255us/step - loss: 0.1485 - accuracy: 0.9446 - val_loss: 0.1322 - val_accuracy: 0.9530\n",
      "Epoch 5/50\n",
      "44172/44172 [==============================] - 12s 262us/step - loss: 0.1297 - accuracy: 0.9529 - val_loss: 0.1405 - val_accuracy: 0.9465\n",
      "Epoch 6/50\n",
      "44172/44172 [==============================] - 11s 245us/step - loss: 0.1103 - accuracy: 0.9605 - val_loss: 0.1097 - val_accuracy: 0.9627\n",
      "Epoch 7/50\n",
      "44172/44172 [==============================] - 11s 254us/step - loss: 0.0979 - accuracy: 0.9660 - val_loss: 0.1251 - val_accuracy: 0.9564\n",
      "Epoch 8/50\n",
      "44172/44172 [==============================] - 11s 250us/step - loss: 0.0877 - accuracy: 0.9704 - val_loss: 0.1083 - val_accuracy: 0.9665\n",
      "Epoch 9/50\n",
      "44172/44172 [==============================] - 11s 251us/step - loss: 0.0776 - accuracy: 0.9728 - val_loss: 0.0883 - val_accuracy: 0.9696\n",
      "Epoch 10/50\n",
      "44172/44172 [==============================] - 11s 247us/step - loss: 0.0741 - accuracy: 0.9744 - val_loss: 0.1223 - val_accuracy: 0.9550\n",
      "Epoch 11/50\n",
      "44172/44172 [==============================] - 11s 246us/step - loss: 0.0620 - accuracy: 0.9787 - val_loss: 0.1110 - val_accuracy: 0.9627\n",
      "Epoch 12/50\n",
      "44172/44172 [==============================] - 11s 246us/step - loss: 0.0592 - accuracy: 0.9796 - val_loss: 0.0959 - val_accuracy: 0.9673\n",
      "Epoch 13/50\n",
      "44172/44172 [==============================] - 11s 253us/step - loss: 0.0523 - accuracy: 0.9818 - val_loss: 0.0913 - val_accuracy: 0.9697\n",
      "Epoch 14/50\n",
      "44172/44172 [==============================] - 11s 256us/step - loss: 0.0496 - accuracy: 0.9824 - val_loss: 0.0855 - val_accuracy: 0.9727\n",
      "Epoch 15/50\n",
      "44172/44172 [==============================] - 11s 256us/step - loss: 0.0426 - accuracy: 0.9854 - val_loss: 0.0839 - val_accuracy: 0.9718\n",
      "Epoch 16/50\n",
      "44172/44172 [==============================] - 11s 258us/step - loss: 0.0431 - accuracy: 0.9849 - val_loss: 0.0762 - val_accuracy: 0.9756\n",
      "Epoch 17/50\n",
      "44172/44172 [==============================] - 11s 257us/step - loss: 0.0369 - accuracy: 0.9868 - val_loss: 0.0758 - val_accuracy: 0.9794\n",
      "Epoch 18/50\n",
      "44172/44172 [==============================] - 11s 257us/step - loss: 0.0377 - accuracy: 0.9875 - val_loss: 0.1009 - val_accuracy: 0.9680\n",
      "Epoch 19/50\n",
      "44172/44172 [==============================] - 12s 263us/step - loss: 0.0339 - accuracy: 0.9883 - val_loss: 0.0840 - val_accuracy: 0.9758\n",
      "Epoch 20/50\n",
      "44172/44172 [==============================] - 12s 262us/step - loss: 0.0363 - accuracy: 0.9880 - val_loss: 0.0712 - val_accuracy: 0.9799\n",
      "Epoch 21/50\n",
      "44172/44172 [==============================] - 12s 261us/step - loss: 0.0293 - accuracy: 0.9900 - val_loss: 0.0860 - val_accuracy: 0.9772\n",
      "Epoch 22/50\n",
      "44172/44172 [==============================] - 12s 261us/step - loss: 0.0273 - accuracy: 0.9905 - val_loss: 0.0978 - val_accuracy: 0.9773\n",
      "Epoch 23/50\n",
      "44172/44172 [==============================] - 12s 270us/step - loss: 0.0269 - accuracy: 0.9907 - val_loss: 0.0887 - val_accuracy: 0.9805\n",
      "Epoch 24/50\n",
      "44172/44172 [==============================] - 12s 271us/step - loss: 0.0259 - accuracy: 0.9912 - val_loss: 0.1069 - val_accuracy: 0.9709\n",
      "Epoch 25/50\n",
      "44172/44172 [==============================] - 12s 268us/step - loss: 0.0304 - accuracy: 0.9895 - val_loss: 0.0804 - val_accuracy: 0.9784\n",
      "Epoch 26/50\n",
      "44172/44172 [==============================] - 12s 273us/step - loss: 0.0234 - accuracy: 0.9915 - val_loss: 0.0774 - val_accuracy: 0.9784\n",
      "Epoch 27/50\n",
      "44172/44172 [==============================] - 12s 271us/step - loss: 0.0279 - accuracy: 0.9908 - val_loss: 0.0955 - val_accuracy: 0.9759\n",
      "Epoch 28/50\n",
      "44172/44172 [==============================] - 12s 267us/step - loss: 0.0242 - accuracy: 0.9919 - val_loss: 0.0933 - val_accuracy: 0.9760\n",
      "Epoch 29/50\n",
      "44172/44172 [==============================] - 12s 272us/step - loss: 0.0226 - accuracy: 0.9925 - val_loss: 0.0950 - val_accuracy: 0.9773\n",
      "Epoch 30/50\n",
      "44172/44172 [==============================] - 12s 276us/step - loss: 0.0206 - accuracy: 0.9928 - val_loss: 0.0906 - val_accuracy: 0.9770\n",
      "Epoch 31/50\n",
      "44172/44172 [==============================] - 12s 279us/step - loss: 0.0197 - accuracy: 0.9935 - val_loss: 0.0817 - val_accuracy: 0.9797\n",
      "Epoch 32/50\n",
      "44172/44172 [==============================] - 12s 276us/step - loss: 0.0247 - accuracy: 0.9916 - val_loss: 0.1407 - val_accuracy: 0.9683\n",
      "Epoch 33/50\n",
      "44172/44172 [==============================] - 12s 274us/step - loss: 0.0199 - accuracy: 0.9930 - val_loss: 0.1104 - val_accuracy: 0.9739\n",
      "Epoch 34/50\n",
      "44172/44172 [==============================] - 12s 271us/step - loss: 0.0163 - accuracy: 0.9939 - val_loss: 0.1379 - val_accuracy: 0.9660\n",
      "Epoch 35/50\n",
      "44172/44172 [==============================] - 12s 269us/step - loss: 0.0235 - accuracy: 0.9919 - val_loss: 0.0797 - val_accuracy: 0.9792\n",
      "Epoch 36/50\n",
      "44172/44172 [==============================] - 12s 273us/step - loss: 0.0179 - accuracy: 0.9937 - val_loss: 0.0952 - val_accuracy: 0.9788\n",
      "Epoch 37/50\n",
      "44172/44172 [==============================] - 12s 266us/step - loss: 0.0299 - accuracy: 0.9902 - val_loss: 0.0993 - val_accuracy: 0.9747\n",
      "Epoch 38/50\n",
      "44172/44172 [==============================] - 12s 273us/step - loss: 0.0211 - accuracy: 0.9925 - val_loss: 0.0881 - val_accuracy: 0.9804\n",
      "Epoch 39/50\n",
      "44172/44172 [==============================] - 12s 269us/step - loss: 0.0169 - accuracy: 0.9946 - val_loss: 0.1078 - val_accuracy: 0.9784\n",
      "Epoch 40/50\n",
      "44172/44172 [==============================] - 12s 272us/step - loss: 0.0174 - accuracy: 0.9938 - val_loss: 0.0874 - val_accuracy: 0.9803\n",
      "Epoch 41/50\n",
      "44172/44172 [==============================] - 13s 286us/step - loss: 0.0109 - accuracy: 0.9961 - val_loss: 0.0971 - val_accuracy: 0.9786\n",
      "Epoch 42/50\n",
      "44172/44172 [==============================] - 12s 282us/step - loss: 0.0192 - accuracy: 0.9934 - val_loss: 0.0902 - val_accuracy: 0.9775\n",
      "Epoch 43/50\n",
      "44172/44172 [==============================] - 13s 299us/step - loss: 0.0130 - accuracy: 0.9955 - val_loss: 0.1074 - val_accuracy: 0.9779\n",
      "Epoch 44/50\n",
      "44172/44172 [==============================] - 13s 293us/step - loss: 0.0180 - accuracy: 0.9937 - val_loss: 0.1099 - val_accuracy: 0.9785\n",
      "Epoch 45/50\n",
      "44172/44172 [==============================] - 12s 270us/step - loss: 0.0199 - accuracy: 0.9930 - val_loss: 0.0990 - val_accuracy: 0.9744\n",
      "Epoch 46/50\n",
      "44172/44172 [==============================] - 12s 269us/step - loss: 0.0172 - accuracy: 0.9945 - val_loss: 0.0872 - val_accuracy: 0.9828\n",
      "Epoch 47/50\n",
      "44172/44172 [==============================] - 12s 273us/step - loss: 0.0185 - accuracy: 0.9940 - val_loss: 0.0911 - val_accuracy: 0.9774\n",
      "Epoch 48/50\n",
      "44172/44172 [==============================] - 12s 273us/step - loss: 0.0166 - accuracy: 0.9942 - val_loss: 0.0784 - val_accuracy: 0.9816\n",
      "Epoch 49/50\n",
      "44172/44172 [==============================] - 12s 283us/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0985 - val_accuracy: 0.9798\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44172/44172 [==============================] - 12s 282us/step - loss: 0.0176 - accuracy: 0.9942 - val_loss: 0.0839 - val_accuracy: 0.9814\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "# split into train and validation\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(X, Y, test_size=0.20)\n",
    "hot_Y_train = np_utils.to_categorical(Y_tr)\n",
    "hot_Y_val = np_utils.to_categorical(Y_val)\n",
    "# create the model\n",
    "LSTM_model = simpleLSTM(30, 6, 100, 0.1, 4, 0.001)\n",
    "LSTM_model.summary()\n",
    "# fit the keras model on the dataset\n",
    "lstm_hist = LSTM_model.fit(X_tr, hot_Y_train, batch_size=64, epochs=50, validation_data=(X_val, hot_Y_val),verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "swedish-impossible",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(lstm_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "embedded-minute",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.save(\"LSTM_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "loving-notice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Test Performance:\n",
      "Accuracy Score : 0.981437884824339\n",
      "Precision Score : 0.981437884824339\n",
      "Recall Score : 0.981437884824339\n",
      "F1 Score : 0.981437884824339\n",
      "classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    Ground(0)       0.95      0.97      0.96      2762\n",
      "  Upstairs(1)       0.99      0.99      0.99      2738\n",
      "Downstairs(2)       0.99      0.99      0.99      2768\n",
      "     Grass(3)       0.99      0.96      0.98      2776\n",
      "\n",
      "     accuracy                           0.98     11044\n",
      "    macro avg       0.98      0.98      0.98     11044\n",
      " weighted avg       0.98      0.98      0.98     11044\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,classification_report\n",
    "# prediction result for validation set\n",
    "valPredY = LSTM_model.predict(X_val)\n",
    "valPredY = np.argmax(valPredY, axis=1)\n",
    "# New Model Evaluation metrics\n",
    "print(\"Overall Test Performance:\")\n",
    "print('Accuracy Score : ' + str(accuracy_score(Y_val,valPredY)))\n",
    "print('Precision Score : ' + str(precision_score(Y_val,valPredY,average='micro')))\n",
    "print('Recall Score : ' + str(recall_score(Y_val,valPredY,average='micro')))\n",
    "print('F1 Score : ' + str(f1_score(Y_val,valPredY,average='micro')))\n",
    "print(\"classification Report:\")\n",
    "print(classification_report(Y_val, valPredY, target_names=[\"Ground(0)\", \"Upstairs(1)\", \"Downstairs(2)\", \"Grass(3)\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-dylan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fitted-matthew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject_012_01__x.csv\n",
      "11329\n",
      "subject_009_01__x.csv\n",
      "9497\n",
      "subject_010_01__x.csv\n",
      "12269\n",
      "subject_011_01__x.csv\n",
      "12939\n",
      "subject_009_01__y_time.csv\n",
      "9497\n",
      "subject_010_01__y_time.csv\n",
      "12269\n",
      "subject_011_01__y_time.csv\n",
      "12939\n",
      "subject_012_01__y_time.csv\n",
      "11329\n"
     ]
    }
   ],
   "source": [
    "# undersampling to match label frequency\n",
    "warped_test_folder = \"warped_test_data/\"\n",
    "test_session_files = os.listdir(raw_test_path + 'x/')\n",
    "for file in test_session_files:\n",
    "    if file != '.DS_Store':\n",
    "        print(file)\n",
    "        df = pd.read_csv(raw_test_path + 'x/' + file, header = None, names=[\"xa\",\"ya\",\"za\",\"xg\",\"yg\",\"zg\"])\n",
    "        warped_test_data = pd.DataFrame()\n",
    "        rows = len(df)\n",
    "        start = 1\n",
    "        while start < rows:\n",
    "            warped_test_data = warped_test_data.append(df.iloc[start], ignore_index=True, sort = False)\n",
    "\n",
    "            start = start + 4\n",
    "\n",
    "        warped_test_data = warped_test_data[['xa','ya','za','xg','yg','zg']]\n",
    "        # manually deleted last row to exactly match label frequency\n",
    "        warped_test_data = warped_test_data[:-1]\n",
    "        print(len(warped_test_data))\n",
    "        warped_test_data.to_csv(warped_test_folder+file, index = False, header=None)\n",
    "\n",
    "y_time_files = ['subject_009_01__y_time.csv','subject_010_01__y_time.csv','subject_011_01__y_time.csv','subject_012_01__y_time.csv']\n",
    "for file in y_time_files:\n",
    "    if file != '.DS_Store':\n",
    "        print(file)\n",
    "        df = pd.read_csv(raw_test_path + file)\n",
    "        print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "selected-equation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# funstion to prepare test time series for prediction\n",
    "def gen_test_time_series(window, file_path): # label type can be middle or at the end \n",
    "    df = pd.read_csv(file_path)\n",
    "    df_len = len(df)\n",
    "    print(df_len)\n",
    "    print(df.shape)\n",
    "    x_arr = list()\n",
    "    y_arr = list()\n",
    "    \n",
    "    current = 0\n",
    "    \n",
    "    required_past = window//2\n",
    "    required_future = required_past-1\n",
    "    \n",
    "    while current < df_len:\n",
    "        # define where the window starts\n",
    "        start = current - required_past\n",
    "        \n",
    "        # extra zero rows to fill up the past\n",
    "        past_padding = None\n",
    "        if current - required_past < 0:\n",
    "            past_padding = np.zeros([abs(current - required_past),df.shape[1]])\n",
    "            start = 0\n",
    "    \n",
    "        # define where the window ends\n",
    "        end = current + required_future\n",
    "        \n",
    "        # extra zero rows to fill up the future\n",
    "        future_padding = None\n",
    "        if end > df_len - 1:\n",
    "            future_padding = np.zeros([end - (df_len-1),df.shape[1]])\n",
    "            end = df_len - 1\n",
    "            \n",
    "        frames = list()\n",
    "        if past_padding is not None:\n",
    "            frames.append(past_padding)\n",
    "        frames.append(np.asarray(df.iloc[start:end+1,0:df.shape[1]]))\n",
    "        if future_padding is not None:\n",
    "            frames.append(future_padding)\n",
    "            \n",
    "        data_window = np.concatenate(frames,axis=0)\n",
    "        x_arr.append(data_window)\n",
    "\n",
    "        current = current + 1\n",
    "            \n",
    "    X = np.asarray(x_arr)\n",
    "\n",
    "    return X # X is numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cardiovascular-tactics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warped_test_data/subject_012_01__x.csv\n",
      "11328\n",
      "(11328, 6)\n",
      "x:(11328, 30, 6)\n",
      "(11328,)\n",
      "warped_test_data/subject_009_01__x.csv\n",
      "9496\n",
      "(9496, 6)\n",
      "x:(9496, 30, 6)\n",
      "(9496,)\n",
      "warped_test_data/subject_010_01__x.csv\n",
      "12268\n",
      "(12268, 6)\n",
      "x:(12268, 30, 6)\n",
      "(12268,)\n",
      "warped_test_data/subject_011_01__x.csv\n",
      "12938\n",
      "(12938, 6)\n",
      "x:(12938, 30, 6)\n",
      "(12938,)\n"
     ]
    }
   ],
   "source": [
    "# prepare test data and predict\n",
    "test_session_ids = extract_session_ids(warped_test_folder)\n",
    "for i in range(0,len(test_session_ids)):\n",
    "    sid = test_session_ids[i]\n",
    "    x_path =  warped_test_folder + \"subject_\" + sid + \"__x.csv\"\n",
    "    print(x_path)\n",
    "    y_path = \"TestLabel/\" + \"subject_\" + sid + \"__y_prediction.csv\"\n",
    "    \n",
    "    X_test = gen_test_time_series(30,x_path)\n",
    "    print(\"x:\"+str(X_test.shape))\n",
    "    \n",
    "    # prediction\n",
    "    predY = LSTM_model.predict(X_test)\n",
    "    Y_test = np.argmax(predY, axis=1)\n",
    "    print(Y_test.shape)\n",
    "    pd.DataFrame(Y_test).to_csv(y_path,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-banner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
